{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a href=\"https://colab.research.google.com/github/kytk/AI-MAILs/blob/main/python_7_scikit-learn-2.ipynb?hl=ja\" target=\"_blank\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHVIs_ypyjg0"
      },
      "source": [
        "## 医療従事者のためのPython: 機械学習 (2)\n",
        "\n",
        "根本清貴 (筑波大学医学医療系精神医学)\n",
        "\n",
        "Ver.20240810\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 目次\n",
        "1. 機械学習とは (復習)\n",
        "2. 機械学習の適切なモデルの選び方\n",
        "3. 教師なし学習の概要\n",
        "4. 主成分分析\n",
        "5. 乳がんデータセットでの主成分分析\n",
        "6. k-meansクラスタリング\n",
        "7. 乳がんデータセットでのk-meansクラスタリング"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. 機械学習とは (復習)\n",
        "- 機械学習の定義\n",
        "    - 「データからルールやパターンを導き出し、予測や意思決定を行う技術」\n",
        "    - (従来: 人間がルールを決める)\n",
        "- 機械学習の種類\n",
        "    - 教師あり学習\n",
        "        - 入力データとその対応する正解(ラベル)がペアになったデータセットを用いてモデルを訓練\n",
        "        - 新しいデータが入ってきた時にそのモデルから正しいラベルを予測\n",
        "        - 例: 画像の分類(犬と猫の画像を分類)、スパムメールの分類、価格予測\n",
        "    - 教師なし学習\n",
        "        - ラベルのないデータを用いてモデルを訓練\n",
        "        - データの内部構造やパターンを見つけ出す\n",
        "        - 例: クラスタリング(似たデータをグループに分ける)、次元削減(データの特徴を少数の重要な特徴に圧縮する)\n",
        "    - 強化学習\n",
        "        - エージェント(学習者)が環境と相互作用しながら学習する。エージェントは行動を選択し、その結果として得られる報酬を基に次の行動を改善する\n",
        "        - 長期的な累積報酬を最大化することが目標\n",
        "        - 例: ゲーム、ロボット制御、自動運転\n",
        "\n",
        "- 本日は、教師なし学習 (unsupervised learning) について学ぶ\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. 機械学習の適切なモデルの選び方\n",
        "\n",
        "- scikit-learn のホームページにわかりやすい図が示されている\n",
        "    - https://scikit-learn.org/1.3/tutorial/machine_learning_map/index.html\n",
        "<img src=\"https://scikit-learn.org/1.3/_static/ml_map.png\">\n",
        "- これを見ると、機械学習を行うには、データは最低50例必要であることがわかる\n",
        "    - 根拠を探したが見つけられなかった"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. 教師なし学習の概要\n",
        "\n",
        "- データに対する正解ラベルを必要とせず、データの内部構造やパターンを学習する方法\n",
        "- ラベルのないデータを用いて、データの特性を理解し、データを構造化することが目的\n",
        "\n",
        "#### 3.1. 教師なし学習の主要な手法\n",
        "\n",
        "1. **次元削減 (Dimensionality Reduction)**:\n",
        "   - データの特徴量の次元を減らし、データを簡潔に表現する手法\n",
        "   - 主なアルゴリズム: 主成分分析 (PCA)、独立成分分析 (ICA)、t-SNEなど\n",
        "   - **例**: 遺伝子発現解析において、数千の遺伝子データを主成分分析(PCA)で数十の主要パターンに圧縮することで、疾患の特徴や患者群の違いを効率的に可視化する\n",
        "\n",
        "2. **クラスタリング (Clustering)**:\n",
        "   - データを似た特徴を持つグループ（クラスター）に分ける手法\n",
        "   - 主なアルゴリズム: k-means、階層型クラスタリング、DBSCANなど\n",
        "   - **例**: 患者の血液検査結果を用いて、似た特徴を持つ患者群を自動的に分類する。k-meansアルゴリズムを使用して、例えば3つのクラスターに分けると、「健康な患者群」「代謝異常リスク群」「炎症性疾患リスク群」といった意味のあるグループが形成され、それぞれに適した予防策や治療方針の策定に役立つ\n",
        "      \n",
        "3. **異常検知 (Anomaly Detection)**:\n",
        "   - 正常なデータから外れる異常なデータポイントを検出する手法。\n",
        "   - 主なアルゴリズム: 一クラスSVM、孤立森林、ガウス混合モデルなど。\n",
        "   - **例**: 心電図データに孤立森林アルゴリズムを適用することで、通常の心拍パターンから逸脱した不整脈や心臓異常を自動的に検出し、早期診断や緊急対応につなげることができる\n",
        "\n",
        "- 今回は主成分分析とk-meansクラスタリングを実装する\n",
        "   \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4. 主成分分析\n",
        "- 次元削減のうち、よく用いられている主成分分析をscikit-learnで実装する\n",
        "- 医療データに応用する前に、わかりやすいように学生の5教科 (5次元) の試験結果を主成分分析で2次元にしてみる\n",
        "\n",
        "#### 4.1. パッケージのインポート\n",
        "- NumPy\n",
        "- Pandas\n",
        "- Matplotlib\n",
        "- Seaborn\n",
        "- Scikit-learn の preprocessingモジュール から 標準化関数 StandardScaler\n",
        "- Scikit-learn の decompositionモジュール から PCA\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 4.2. データの読み込み\n",
        "\n",
        "- student_scores.csv をダウンロード\n",
        "- Pandas で df として読み込む\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# コマンドの前に ! をつけると、Linuxコマンドが動作できる\n",
        "![[ -f student_scores.csv ]] || wget https://raw.githubusercontent.com/kytk/AI-MAILs/main/data/student_scores.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# student_scores.csv を df という名前の Pandasデータフレームとして読み込み\n",
        "df = pd.read_csv('student_scores.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# データの先頭および最後の5行を表示\n",
        "# group は science が理系、humanities が文系\n",
        "print(\"データセットの先頭5行:\")\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"データセットの最後5行:\")\n",
        "df.tail()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 4.3. PandasのDataFrameをNumpyのndarrayに変換\n",
        "\n",
        "- scikit-learn で扱うデータは基本、NumPy配列\n",
        "- 前回はサンプルデータを使っているのであまり意識しなかったが、PandasのDataFrameをNumPyに変換する\n",
        "- Pandas→NumPyは、`to_numpy` メソッドを使うだけなのでとても簡単\n",
        "- group以外の5教科の点数をNumPy配列に変換する"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# dfから 'group' の列だけ drop して、df_scoresとして代入\n",
        "# df.drop('group', axis=1) で削除できる\n",
        "df_scores = df.drop('group', axis=1)\n",
        "\n",
        "# df_scores を確認\n",
        "df_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# PandasのDataFrame df_scores を NumPy配列に変換\n",
        "# to_numpy メソッドを使用\n",
        "scores = df_scores.to_numpy()\n",
        "\n",
        "# scores を確認\n",
        "scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 4.3. データの前処理: 標準化\n",
        "- 5教科を 平均値0, 標準偏差1 となるように標準化する\n",
        "- `StandardScaler()` 関数を使用する"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# データの標準化\n",
        "\n",
        "# StandardScaler() 関数を使用して標準化を行うオブジェクト scaler を生成\n",
        "scaler = StandardScaler()  \n",
        "# 標準化するためのパラメータを計算(fit)し、適用(transform)する\n",
        "scores_standardized = scaler.fit_transform(scores)\n",
        "\n",
        "# 最初の数行を確認\n",
        "# scores, scores_scaled は numpy型 なので、scores_scaled[:3] で最初の3行が表示される\n",
        "\n",
        "# scores\n",
        "print('scores')\n",
        "print(scores[:3])\n",
        "\n",
        "# scores_standardized\n",
        "print('\\nscores_standardized')\n",
        "print(scores_standardized[:3])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 平均と標準偏差を確認\n",
        "# axis=0 とすることで、各列における行の平均や標準偏差を求められる\n",
        "print('\\nscoresの平均:', np.round(scores.mean(axis=0),1))\n",
        "print('scoresの標準偏差:', np.round(scores.std(axis=0),1))\n",
        "\n",
        "print('\\nscores_standardizedの平均:', np.round(scores_standardized.mean(axis=0),1))\n",
        "print('scores_standardizedの標準偏差:', np.round(scores_standardized.std(axis=0),1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 4.4. PCAの適用\n",
        "- Scikit-learn の PCAは非常に簡単\n",
        "- `PCA()` 関数を使用して PCAを行うオブジェクトを生成する\n",
        "    - その際、`n_components=2` のように次元を指定する\n",
        "    - ここでは 2 に設定しているので、データを2次元に圧縮する\n",
        "    - つまり、元のデータの次元数に関わらず、もっとも重要な2つの主成分だけを保持するようにPCAに指示する\n",
        "- その後、準備したオブジェクトのメソッド `fit_transform()` にデータを投入するだけ\n",
        "    - `fit()`: 入力データに基づいてPCAモデルを学習\n",
        "        - これにより、主成分（固有ベクトル）と、各主成分の重要度（固有値）が計算される\n",
        "    - `transform()`: 学習したモデルを使って、入力データを新しい主成分空間に変換する\n",
        "- `scores_pca` は、元のデータを2次元の主成分空間に投影した結果\n",
        "    - 各行は元のデータに対応し、2つの列はそれぞれ第1主成分と第2主成分の値を意味する\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# PCAの適用\n",
        "\n",
        "# PCAを行うオブジェクト pca を生成\n",
        "pca = PCA(n_components=2)\n",
        "\n",
        "# 前処理が終わったデータを投入し、fitで学習、transformで変換\n",
        "scores_pca = pca.fit_transform(scores_standardized) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- PCAの結果を確認\n",
        "    - scores_pcaの形状: `scores_pca.shape`\n",
        "    - scores_pca の実際の値 `scores_pca`\n",
        "    - 主成分の方向 (固有ベクトル): `pca.components_`\n",
        "    - 主成分の分散 (固有値): `pca.explained_variance_`\n",
        "    - 固有値の寄与率: `pca.explained_variance_ratio_`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 大きさは150行2列\n",
        "# 150人の5教科のデータが2つの情報に集約された\n",
        "print('scoresの形状: ', scores_pca.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# scores_pca そのものを見る\n",
        "# 各例の第1軸に対する寄与度、第2軸に対する寄与度が示されている\n",
        "print('scores_pca の実際の値: 5人分\\n', scores_pca[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- `pca.components_`:\n",
        "   - 固有ベクトル (主成分の方向)を表す行列\n",
        "   - 各行が主成分(第1主成分、第2主成分)に対応し、列は元の特徴量(教科)に対応\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 固有ベクトルは、各教科の第1軸、第2軸に対するベクトル (あとで図示)\n",
        "\n",
        "print('固有ベクトル(主成分の方向): \\n', pca.components_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- `pca.components_.T`:\n",
        "   - `.T`は転置を意味\n",
        "   - 転置することで、各行が元の特徴量(教科)に、各列が主成分(第1主成分、第2主成分)に対応"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('固有ベクトル(主成分の方向)の転置: \\n', pca.components_.T)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- `pca.explained_variance_`:\n",
        "   - 固有値 (各主成分の分散)\n",
        "   - 各主成分がデータの全体の分散のうちどれだけを説明しているかを意味\n",
        "\n",
        "- `np.sqrt(pca.explained_variance_)`:\n",
        "   - 分散の平方根を取ることで、固有値を標準偏差のスケールに変換\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('固有値(主成分の分散): ', pca.explained_variance_)\n",
        "print('固有値の標準偏差: ', np.sqrt(pca.explained_variance_))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- `pca.explained_variance_ratio_`:\n",
        "  - 固有値の寄与率"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('固有値の寄与率: ', pca.explained_variance_ratio_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- 主成分負荷量\n",
        "    - 主成分の固有ベクトルに主成分の固有値の標準偏差をかけ合わせたもの\n",
        "    - 元の特徴量が各主成分にどの程度寄与しているかを示す\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 各特徴量（教科）の主成分への寄与度 (主成分負荷量)\n",
        "\n",
        "# pca.components_ を列で示したいので、.T で転置する\n",
        "# それに固有値の標準偏差をかけあわせる\n",
        "loadings = pca.components_.T * np.sqrt(pca.explained_variance_)\n",
        "\n",
        "# 結果を表にしたいので、Pandasを使って表にする\n",
        "# NumPy配列をPandasにするときには、pd.DataFrame(numpy配列, columns=[列名], index=[行名])とする\n",
        "loadings_df = pd.DataFrame(loadings, columns=['PC1', 'PC2'], index=df.columns[:-1])\n",
        "\n",
        "\n",
        "print(\"\\n各特徴量の主成分負荷量:\")\n",
        "print(loadings_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- 主成分負荷量からの考察\n",
        "  - 英語は PC2 の負の方向に大きく寄与している\n",
        "  - 数学と理科は PC1 の負の方向に寄与している\n",
        "  - 国語は PC1 と PC2 に同程度寄与している\n",
        "  - 社会は PC1 の正の方向に寄与している\n",
        "\n",
        "- 第1主成分は、プラスなら文系、マイナスなら理系ということで、「文理軸」と言えるか\n",
        "- 第2主成分は、プラスなら国語、マイナスなら英語ということで、「言語軸」と言えるか"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 4.5. PCAの結果の可視化\n",
        "\n",
        "##### 4.5.1. 散布図\n",
        "- scores_pca を横軸 第1主成分 PC1, 縦軸 第2主成分 PC2 の座標にプロット\n",
        "- 個々の点は個人を示す\n",
        "- 色は、その人が理系か文系を示す"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 散布図にプロット\n",
        "# 横軸に scores_pcaの第1列、縦軸に scores_pcaの第2列をプロット\n",
        "\n",
        "# 理系の人を0(青色), 文系の人を1(緑色)で表示\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "scatter = plt.scatter(scores_pca[:, 0], scores_pca[:, 1], \n",
        "                      c=df['group'].map({'science': 0, 'humanities': 1}), cmap='winter')\n",
        "plt.colorbar()\n",
        "plt.xlabel('PC1')\n",
        "plt.ylabel('PC2')\n",
        "plt.title('PCA results')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- 以下のコードで、どの点がどの個人かを同定することもできる"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 8))\n",
        "scatter = plt.scatter(scores_pca[:, 0], scores_pca[:, 1], \n",
        "                      c=df['group'].map({'science': 0, 'humanities': 1}), cmap='winter')\n",
        "plt.colorbar()\n",
        "plt.xlabel('PC1')\n",
        "plt.ylabel('PC2')\n",
        "plt.title('Student Scores in PC Space')\n",
        "for i, txt in enumerate(df.index):\n",
        "    plt.annotate(txt, (scores_pca[i, 0], scores_pca[i, 1]))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 4.5.2. 固有ベクトルの可視化"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 固有ベクトルを改めて表示\n",
        "print(\"固有ベクトル:\")\n",
        "print(pd.DataFrame(pca.components_.T, columns=['PC1', 'PC2'], index=df.columns[:-1]))\n",
        "\n",
        "# 固有ベクトルをプロット\n",
        "plt.figure(figsize=(8, 8))\n",
        "for i, (x, y) in enumerate(zip(pca.components_[0], pca.components_[1])):\n",
        "    plt.arrow(0, 0, x, y, head_width=0.05, head_length=0.05, fc='r', ec='r')\n",
        "    plt.text(x*1.2, y*1.1, df.columns[i], fontsize=12)\n",
        "plt.xlim(-1, 1)\n",
        "plt.ylim(-1, 1)\n",
        "plt.xlabel('PC1')\n",
        "plt.ylabel('PC2')\n",
        "plt.title('Variables factor map (PCA)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- これから改めて以下が考察される\n",
        "    - PC1 は文系か理系かを示す。値が正ならば文系、負ならば理系\n",
        "    - PC2 は言語を示す 値が正ならば国語がより得意、負ならば英語が得意"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5. 乳がんデータセットを用いた主成分分析\n",
        "- 乳がんデータセットの30の特徴量を2次元まで削減する\n",
        "- その2次元がもしかしたら良性・悪性の特徴に近いかもしれないので、その一致度も見てみる"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 5.0. 乳がんデータセット (復習)\n",
        "- データセットの名称: Breast Cancer Wisconsin (Diagnostic) dataset\n",
        "- サンプル数: 569\n",
        "- 特徴量の数: 30\n",
        "- ターゲットの種類: 2クラス（良性と悪性）\n",
        "  - 0: 悪性; 1: 良性\n",
        "- 特徴量の種類: 実数値\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- 細胞診における細胞核の30の特徴量\n",
        "\n",
        "| 英語 | 日本語 | 英語 | 日本語 |\n",
        "| --- | --- | --- | --- |\n",
        "| mean radius | 平均半径 | mean texture | 平均テクスチャ |\n",
        "| mean perimeter | 平均周囲長 | mean area | 平均面積 |\n",
        "| mean smoothness | 平均平滑度 | mean compactness | 平均コンパクト度 |\n",
        "| mean concavity | 平均陥凹度 | mean concave points | 平均陥凹点数 |\n",
        "| mean symmetry | 平均対称性 | mean fractal dimension | 平均フラクタル次元 |\n",
        "| radius error | 半径誤差 | texture error | テクスチャ誤差 |\n",
        "| perimeter error | 周囲長誤差 | area error | 面積誤差 |\n",
        "| smoothness error | 平滑度誤差 | compactness error | コンパクト度誤差 |\n",
        "| concavity error | 陥凹度誤差 | concave points error | 陥凹点数誤差 |\n",
        "| symmetry error | 対称性誤差 | fractal dimension error | フラクタル次元誤差 |\n",
        "| worst radius | 最悪の半径 | worst texture | 最悪のテクスチャ |\n",
        "| worst perimeter | 最悪の周囲長 | worst area | 最悪の面積 |\n",
        "| worst smoothness | 最悪の平滑度 | worst compactness | 最悪のコンパクト度 |\n",
        "| worst concavity | 最悪の陥凹度 | worst concave points | 最悪の陥凹点数 |\n",
        "| worst symmetry | 最悪の対称性 | worst fractal dimension | 最悪のフラクタル次元 |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 5.1. パッケージのインポート\n",
        "- NumPy\n",
        "- Pandas\n",
        "- Matplotlib\n",
        "- Seaborn\n",
        "- Scikit-learn の preprocessingモジュール から 正規化関数 MinMaxScaler\n",
        "- Scikit-learn の decompositionモジュール から PCA\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.decomposition import PCA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 5.2. データの読み込み\n",
        "\n",
        "- 先程と同じように、サンプルデータセットを読み込むのではなく、Excelファイルを Pandas に読み込む\n",
        "- target は不要のために外す"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# コマンドの前に ! をつけると、Linuxコマンドが動作できる\n",
        "![[ -f breast_cancer_data.xlsx ]] || wget https://raw.githubusercontent.com/kytk/AI-MAILs/main/data/breast_cancer_data.xlsx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Pandas の DataFrame への読み込み"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_excel('breast_cancer_data.xlsx')\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# target 列は削除\n",
        "df_data = df.drop('target', axis=1)\n",
        "\n",
        "df_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# df_data の列名を feature_names とする\n",
        "feature_names = df_data.columns\n",
        "\n",
        "feature_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# NumPy配列に変換\n",
        "data = df_data.to_numpy()\n",
        "\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# target は target としてnumpyに変換しておく\n",
        "target = df['target'].to_numpy()\n",
        "\n",
        "target"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 5.3. データの前処理: 正規化\n",
        "- 30の特徴量に対して正規化 (0-1 に変換)を行う\n",
        "- 教師あり学習の時と同じ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# データの正規化\n",
        "\n",
        "# MinMaxScaler() 関数を使用して標準化を行うオブジェクト scaler を生成\n",
        "scaler = MinMaxScaler()  \n",
        "# 正規化するためのパラメータを計算(fit)し、適用(transform)する\n",
        "data_normalized = scaler.fit_transform(data)\n",
        "\n",
        "# 最初の数行を確認\n",
        "# data, data_normalized は numpy型 なので、data[:3] で最初の3行が表示される\n",
        "\n",
        "# data\n",
        "print('data')\n",
        "print(np.round(data[:3],1))\n",
        "\n",
        "# data_normalized\n",
        "print('\\ndata_normalized')\n",
        "print(np.round(data_normalized[:3],1))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 5.4. PCAの適用\n",
        "\n",
        "- `PCA()` 関数を使用して PCAを行うオブジェクトを生成する\n",
        "  - `n_components` で 圧縮する次元を指定する\n",
        "- その後、準備したオブジェクトのメソッド `fit_transform()` にデータを投入する\n",
        "    - `fit()`: 入力データに基づいてPCAモデルを学習\n",
        "        - これにより、主成分（固有ベクトル）と、各主成分の重要度（固有値）が計算される\n",
        "    - `transform()`: 学習したモデルを使って、入力データを新しい主成分空間に変換する\n",
        "- `scores_pca` は、元のデータを2次元の主成分空間に投影した結果\n",
        "    - 各行は元のデータに対応し、2つの列はそれぞれ第1主成分と第2主成分の値を意味する\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# PCAの適用\n",
        "\n",
        "# PCAを行うオブジェクト pca を生成\n",
        "pca = PCA(n_components=2)\n",
        "\n",
        "# 前処理が終わったデータを投入し、fitで学習、transformで変換\n",
        "data_pca = pca.fit_transform(data_normalized) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- PCAの結果を確認\n",
        "    - data_pcaの形状: `data_pca.shape`\n",
        "    - data_pca の実際の値 `data_pca`\n",
        "    - 主成分の方向 (固有ベクトル): `pca.components_`\n",
        "    - 主成分の分散 (固有値): `pca.explained_variance_`\n",
        "    - 固有値の寄与率: `pca.explained_variance_ratio_`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# data_pca の形状\n",
        "# 569行2列\n",
        "data_pca.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# data_pca そのもの\n",
        "# 小数点2位で丸め\n",
        "\n",
        "np.round(data_pca,2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 固有ベクトル\n",
        "\n",
        "np.round(pca.components_.T,2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 固有値の標準偏差\n",
        "\n",
        "np.sqrt(pca.explained_variance_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 固有値の寄与率\n",
        "pca.explained_variance_ratio_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 主成分負荷量\n",
        "\n",
        "loadings = pca.components_.T * np.sqrt(pca.explained_variance_)\n",
        "\n",
        "np.round(loadings,2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- 特徴量が多いと、主成分が何を意味するかすぐに把握しづらい\n",
        "- こういう時に視覚化が役立つ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.5. PCAの結果の視覚化\n",
        "\n",
        "#### 5.5.1. 散布図"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 散布図にプロット\n",
        "# 横軸に data_pcaの第1列、縦軸に data_pcaの第2列をプロット\n",
        "\n",
        "# 悪性を0(青色), 良性を1(緑色)で表示\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "scatter = plt.scatter(data_pca[:, 0], data_pca[:, 1], \n",
        "                      c=target, cmap='winter')\n",
        "plt.colorbar()\n",
        "plt.xlabel('PC1')\n",
        "plt.ylabel('PC2')\n",
        "plt.title('PCA results')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 5.5.2. 各特徴量の寄与度"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 特徴量の寄与度\n",
        "plt.figure(figsize=(14, 6))\n",
        "\n",
        "# バーの幅と位置の設定\n",
        "bar_width = 0.35\n",
        "r1 = np.arange(len(feature_names))\n",
        "r2 = [x + bar_width for x in r1]\n",
        "\n",
        "# 2つの主成分のloadingsを少しずらして表示\n",
        "plt.bar(r1, loadings[:, 0], color='blue', width=bar_width, label='First Principal Component')\n",
        "plt.bar(r2, loadings[:, 1], color='red', width=bar_width, label='Second Principal Component')\n",
        "\n",
        "plt.xlabel('Features')\n",
        "plt.ylabel('PCA Loading')\n",
        "plt.title('PCA Loadings for First and Second Principal Components')\n",
        "plt.xticks([r + bar_width/2 for r in range(len(feature_names))], feature_names, rotation=90)\n",
        "\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- これらの結果から、以下が推測できる\n",
        "  - 第1主成分が良性か悪性かの軸\n",
        "    - 第1主成分の負荷量を見ることで、より重要な特徴量が見えてくる\n",
        "  - 第2主成分の寄与率は少ない\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6. k-means クラスタリング\n",
        "- k-means クラスタリングは、教師なし学習の中でも最も基本的でよく使われる手法の一つ\n",
        "- データセットを \\( k \\) 個のクラスターに分割し、各データポイントが最も近いクラスタの中心 (セントロイド) に割り当てられる\n",
        "\n",
        "- アルゴリズムの手順\n",
        "   1. 初期化\n",
        "      - クラスタ数 \\( k \\) を決定し、データポイントからランダムに \\( k \\) 個のセントロイドを選ぶ\n",
        "   2. 割り当て\n",
        "      - 各データポイントを最も近いセントロイドに割り当てる\n",
        "   3. セントロイドの更新\n",
        "      - 各クラスタのセントロイドを、クラスタ内のデータポイントの平均値に更新する\n",
        "   4. 収束\n",
        "      - セントロイドの位置が変わらなくなるまで、割り当てと更新を繰り返す\n",
        "\n",
        "- 評価\n",
        "   - クラスタリングの質を評価するために、内部評価指標（例: シルエットスコア、クラス内分散の総和）を使用する\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 6.1. パッケージのインポート\n",
        "\n",
        "- NumPy\n",
        "- Matplotlib\n",
        "- Seaborn\n",
        "- Scikit-learn の preprocessing モジュールから StandardScaler\n",
        "- Scikit-learn の cluster モジュールから KMeans\n",
        "- Scikit-learn の metrics モジュールから silhouette_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 6.1. データの読み込み\n",
        "- scikit-learnには、クラスターを作ってくれる関数 `make_blobs` があるので、それを利用して3つのクラスターのデータを作成する"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_blobs\n",
        "\n",
        "# データの生成\n",
        "n_samples = 120\n",
        "n_clusters = 3\n",
        "scores, y = make_blobs(n_samples=n_samples, centers=n_clusters, random_state=10)\n",
        "scores += 70"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2つの教科の試験結果　をイメージ\n",
        "np.round(scores,0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sns.scatterplot(x=scores[:,0], y=scores[:,1])\n",
        "plt.xlabel('Exam 1')\n",
        "plt.ylabel('Exam 2')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- 明らかに3つのクラスタに分かれそう\n",
        "- 3つのクラスタに分けてみる\n",
        "  - クラスタの推測法は後述"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 6.2. データの前処理: 標準化\n",
        "\n",
        "- データの前処理を行う\n",
        "- これまでと同様、StandardScaler 関数を使って標準化を行う"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "scaler = StandardScaler()\n",
        "scores_standardized = scaler.fit_transform(scores)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 標準化後のプロット\n",
        "sns.scatterplot(x=scores_standardized[:,0], y=scores_standardized[:,1])\n",
        "plt.xlabel('Standardized Exam 1')\n",
        "plt.ylabel('Standardized Exam 2')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.3. K-meansクラスタリング\n",
        "\n",
        "- K-means クラスタリングでは、まず、自分で「いくつのクラスターに分類するか」を決める\n",
        "- そのクラスターの数で分類器を作成する\n",
        "- あとは、fit_predict で学習し、予測させる"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# K-means クラスタリングの実行\n",
        "# クラスターの数を定義\n",
        "n_clusters = 3\n",
        "\n",
        "# KMeans関数で kmeans オブジェクトを生成\n",
        "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "\n",
        "# データを入れて学習させる\n",
        "clusters = kmeans.fit_predict(scores_standardized)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- kmeansオブジェクトの属性から情報を抽出する\n",
        "  - `cluster_centers_`: クラスタの中心 (セントロイド) の座標\n",
        "  - `labels_`: 各データがどれに分類されたかを含む配列\n",
        "  - `inertia_`: クラスタ内のデータとデータの中心の距離の二乗の総和\n",
        "    - inertia_ が小さいほど、各データがそのクラスタの中心に近いことを意味"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# セントロイドの座標\n",
        "kmeans.cluster_centers_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# クラスタリングで分類された結果\n",
        "# 上の clusters と同じ内容\n",
        "kmeans.labels_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 各クラスターに所属するデータとクラスター中心の距離の二乗の総和\n",
        "kmeans.inertia_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- シルエットスコアを計算する。シルエットスコアは -1から1 の範囲で、高いほどよいクラスタリングを示す"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# シルエットスコアを計算\n",
        "silhouette = silhouette_score(scores_standardized, clusters)\n",
        "\n",
        "print(f'シルエットスコア: {silhouette: .2f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 6.4. 結果の可視化\n",
        "\n",
        "- 各クラスタを色分けし、クラスタ中心を X で示す"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# 結果の可視化\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.scatter(scores_standardized[:, 0], scores_standardized[:, 1], c=kmeans.labels_, cmap='winter')\n",
        "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], \n",
        "            marker='x', s=200, linewidths=3, color='r', zorder=10)\n",
        "plt.title('K-means Clustering (3 clusters)')\n",
        "plt.xlabel('Standardized Exam 1')\n",
        "plt.ylabel('Standardized Exam 2')\n",
        "plt.colorbar(ticks=range(n_clusters))\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 6.5. クラスター数の設定\n",
        "\n",
        "- 上の例ではクラスター数は 3 であることが自明だが、そうはうまくいかない場合もままある\n",
        "- その時に、クラスター数を1ずつ増やし、kmeans.inertia_を計算し、その推移を観察する\n",
        "- グラフはたいていの場合、急激に減少した後に緩やかになる\"肘 elbow\"の形状となる\n",
        "- この肘の位置が適切なクラスタ数である可能性があり、これをエルボー法という"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# エルボー法による適切なクラスター数の探索\n",
        "inertias = []\n",
        "k_range = range(1, 11)\n",
        "\n",
        "for k in k_range:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "    kmeans.fit(scores_standardized)\n",
        "    inertias.append(kmeans.inertia_)\n",
        "\n",
        "plt.plot(k_range, inertias, 'bx-')\n",
        "plt.xlabel('k')\n",
        "plt.ylabel('Inertia')\n",
        "plt.title('Elbow Method For Optimal k')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7. 乳がんデータセットによるk-meansクラスタリング\n",
        "\n",
        "- 乳がんデータセットは良性・悪性がわかっている\n",
        "- 2つのクラスターにわけられるかトライする"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 7.1. パッケージのインポート\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 7.2. データの読み込みと前処理\n",
        "- 今日の前半で使用した breast_cancer_data.xlsx を読み込む\n",
        "- 正規化を行う"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_excel('breast_cancer_data.xlsx')\n",
        "\n",
        "# target 列は削除\n",
        "df_data = df.drop('target', axis=1)\n",
        "\n",
        "# df_data の列名を 取り出したものを feature_names とする\n",
        "feature_names = df_data.columns\n",
        "\n",
        "# NumPy配列に変換\n",
        "data = df_data.to_numpy()\n",
        "\n",
        "# target は target としてnumpyに変換しておく\n",
        "target = df['target'].to_numpy()\n",
        "\n",
        "# データの正規化\n",
        "\n",
        "# MinMaxScaler() 関数を使用して標準化を行うオブジェクト scaler を生成\n",
        "scaler = MinMaxScaler()  \n",
        "# 正規化するためのパラメータを計算(fit)し、適用(transform)する\n",
        "data_normalized = scaler.fit_transform(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "feature_names"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 7.3. k-means クラスタリングの適用\n",
        "- KMeans() 関数に、自分が分類したいクラスター数を指定し、kmeans という分類器を生成する\n",
        "- 今は、良性と悪性で2つのクラスターができるはずという思いから、2 とする\n",
        "- `kmeans.fit_predict()` で実際の分類を行う\n",
        "- シルエットスコアを計算する。シルエットスコアは -1から1 の範囲で、高いほどよいクラスタリングを示す"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# KMeans() を使って分類器のオブジェクトを生成\n",
        "kmeans = KMeans(n_clusters=2, random_state=42)\n",
        "\n",
        "# 実際にデータを入れて分類\n",
        "clusters = kmeans.fit_predict(data_normalized)\n",
        "\n",
        "# シルエットスコアを計算\n",
        "silhouette = silhouette_score(data_normalized, clusters)\n",
        "\n",
        "print(f'シルエットスコア: {silhouette: .2f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 7.4. 元の特徴量の寄与度を調べる\n",
        "\n",
        "##### 7.4.1. クラスターの中心と特徴量の関係を調べる\n",
        "- 以下の関数でクラスターの中心と特徴量の関係をグラフにできる"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_cluster_centers(kmeans, feature_names):\n",
        "    centers = kmeans.cluster_centers_\n",
        "    feature_importance = np.abs(centers[0] - centers[1])\n",
        "    sorted_idx = np.argsort(feature_importance)\n",
        "    pos = np.arange(sorted_idx.shape[0]) + .5\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.barh(pos, feature_importance[sorted_idx], align='center')\n",
        "    plt.yticks(pos, np.array(feature_names)[sorted_idx])\n",
        "    plt.xlabel('Absolute difference between cluster centers')\n",
        "    plt.title('Feature Importance in K-means Clustering')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_cluster_centers(kmeans, feature_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- kmeans.cluster_centers_ を改めて見てみる"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# この配列の形状を確認する\n",
        "\n",
        "kmeans.cluster_centers_.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "np.round(kmeans.cluster_centers_,1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- これは、特徴量ごとの、クラスター1とクラスター2の中心を示している\n",
        "- ある特徴量において、クラスター1とクラスター2の中心の距離が離れていたら、それは、その特徴量はクラスターを分けるのに大きく寄与しているということになる\n",
        "- 今の場合、\"concavity\" と \"perimeter\" あたりのパラメーターが大きく寄与している\n",
        "- \"mean concavity\" と \"mean perimeter\" のふたつでグラフを描いてみる"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 'mean concavity' と 'mean perimeter' のインデックスを取得\n",
        "concavity_idx = np.where(feature_names == 'mean concavity')[0][0]\n",
        "perimeter_idx = np.where(feature_names == 'mean perimeter')[0][0]\n",
        "\n",
        "# 散布図の作成\n",
        "plt.figure(figsize=(10, 8))\n",
        "scatter = plt.scatter(data_normalized[:, concavity_idx], data_normalized[:, perimeter_idx], \n",
        "                      c=kmeans.labels_, cmap='winter', alpha=0.7)\n",
        "plt.xlabel('Mean Concavity (standardized)')\n",
        "plt.ylabel('Mean Perimeter (standardized)')\n",
        "plt.title('Scatter Plot: Mean Concavity vs Mean Perimeter')\n",
        "\n",
        "# クラスターの中心をプロット\n",
        "centers = kmeans.cluster_centers_\n",
        "plt.scatter(centers[:, concavity_idx], centers[:, perimeter_idx], \n",
        "            c='red', s=200, alpha=0.8, marker='X', label='Cluster Centers')\n",
        "\n",
        "plt.legend()\n",
        "plt.colorbar(scatter, label='Cluster')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# クラスターごとの統計情報を表示\n",
        "for i in range(2):\n",
        "    cluster_points = data_normalized[kmeans.labels_ == i]\n",
        "    print(f\"Cluster {i}:\")\n",
        "    print(f\"  Mean Concavity: {np.mean(cluster_points[:, concavity_idx]):.4f}\")\n",
        "    print(f\"  Mean Perimeter: {np.mean(cluster_points[:, perimeter_idx]):.4f}\")\n",
        "    print(f\"  Number of points: {len(cluster_points)}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 7.5. 良性、悪性とクラスター分割の一致\n",
        "- kmeans クラスタリングの結果と良性、悪性の診断がどの程度一致するかを見てみる\n",
        "- kmeans クラスタリングはあくまでもクラスタリングであり、良性、悪性にわけるためのものではないことに注意\n",
        "- 混同行列を使用する"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "\n",
        "conf_matrix = confusion_matrix(clusters, target)\n",
        "\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', \n",
        "            xticklabels=['Cluster 1', 'Cluster 2'], \n",
        "            yticklabels=['Actual Benign', 'Actual Malignant'])\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_accuracy = accuracy_score(target, clusters)\n",
        "print(f'Test Accuracy: {test_accuracy: .2f}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 7.6. 乳がんデータセットのクラスター数の探索\n",
        "\n",
        "- 乳がんデータセットは良性か悪性かの2つに分かれているが、実際はクラスターはもう少しあるかもしれない\n",
        "- エルボー法で探索してみる"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# エルボー法による適切なクラスター数の探索\n",
        "inertias = []\n",
        "k_range = range(1, 11)\n",
        "\n",
        "for k in k_range:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "    kmeans.fit(data_normalized)\n",
        "    inertias.append(kmeans.inertia_)\n",
        "\n",
        "plt.plot(k_range, inertias, 'bx-')\n",
        "plt.xlabel('k')\n",
        "plt.ylabel('Inertia')\n",
        "plt.title('Elbow Method For Optimal k')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- 2 or 3 程度と考えられる"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### おわりに\n",
        "- 以上、Pythonを使った医療データの解析の基本を一通りカバーしました\n",
        "  - Pythonの基礎\n",
        "  - Pandas, NumPy, Seaborn, Matplotlib\n",
        "  - 機械学習\n",
        "  - 深層学習\n",
        "- 私自身、学びながらのところもたくさんありましたが、基本をおさえることで、応用しやすくなると思いますので、少しでもこのような領域に興味・関心を持っていただけたら準備した甲斐があります\n",
        "- 様々なご意見もありがとうございました。最後の感想もぜひお聞かせください\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### (参考) 主成分分析で使用したデータの生成方法\n",
        "\n",
        "- 150人の仮想の試験結果を生成\n",
        "- 理系科目が得意な75人と文系科目が得意な75人を想定\n",
        "- 5教科 (英語、数学、理科、国語、社会) を乱数で40-75点で生成\n",
        "- 全員に対して、英語は乱数で5-20点をかさ増し\n",
        "- 理系科目が得意な75人に対して、数学、理科を5-20点かさ増し\n",
        "- 文系科目が得意な75人に対して、国語、社会を5-20点かさ増し\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 主成分分析で使用したテストの成績の生成\n",
        "\n",
        "# 乱数のシードを設定して再現性を確保\n",
        "np.random.seed(42)\n",
        "\n",
        "# 初期化\n",
        "scores = []\n",
        "\n",
        "# 生徒の数\n",
        "n_students = 150\n",
        "\n",
        "# 理系群と文系群の生徒数（ほぼ半々に）\n",
        "# 理系群は生徒の数を2で割った時の商\n",
        "# 文系群は 全生徒の数 - 理系生徒の数\n",
        "n_science = n_students // 2\n",
        "n_humanities = n_students - n_science\n",
        "\n",
        "# 基本的な成績の生成（全科目）\n",
        "scores = np.random.randint(40, 76, size=(n_students, 5))\n",
        "\n",
        "# 英語の成績調整\n",
        "scores[:, 0] += np.random.randint(5, 20, n_students)\n",
        "\n",
        "# 理系群の成績調整\n",
        "scores[:n_science, 1] += np.random.randint(5, 20, n_science)  # 数学\n",
        "scores[:n_science, 2] += np.random.randint(5, 20, n_science)  # 理科\n",
        "\n",
        "# 文系群の成績調整\n",
        "scores[n_science:, 3] += np.random.randint(5, 20, n_humanities)  # 国語\n",
        "scores[n_science:, 4] += np.random.randint(5, 20, n_humanities)  # 社会\n",
        "\n",
        "# データフレームの作成\n",
        "df = pd.DataFrame(scores, columns=['English', 'Math', 'Science', 'Japanese', 'Social_studies'])\n",
        "\n",
        "# 実際の群（理系/文系）のラベルを追加\n",
        "df['group'] = ['science' if i < n_science else 'humanities' for i in range(n_students)]\n",
        "\n",
        "# データの保存\n",
        "df.to_csv('student_scores.csv', index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
