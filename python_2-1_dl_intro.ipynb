{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AL-MAILs\n",
    "## 深層学習入門: 概要\n",
    "\n",
    "Ver. 20220715"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 本セクションの目標\n",
    "- 深層学習の概要について理解する\n",
    "- 深層学習にまつわるキーワードを理解する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第3部の参考書\n",
    "\n",
    "| Francois Chollet | 斎藤 勇哉 | 斎藤 康毅 |\n",
    "| :--: | :--: | :--: |\n",
    "| PythonとKerasによる<br>ディープラーニング | 動かしながら学ぶ<br>PyTorchプログラミング入門 | ゼロから作るDeep Learning |\n",
    "| <img src=\"https://www.nemotos.net/nb/img/keras_cover.jpg\" width=\"100\"> | <img src=\"https://www.nemotos.net/nb/img/pytorch_cover.jpg\" width=\"100\"> | <img src=\"https://www.nemotos.net/nb/img/dl_from_scratch_cover.png\" width=\"100\"> |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参考サイト\n",
    "| K-lab/脳画像解析eラーニング |\n",
    "| :--: |\n",
    "|<a href=\"https://nemotos.net/?page_id=2436\" target=\"_blank\"><img src=\"https://www.nemotos.net/nb/img/k-lab.png\" width=\"300\"></a> |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 目次\n",
    "- A. 人工知能, 機械学習, 深層学習\n",
    "- B. 深層学習の概要\n",
    "    1. ニューラルネットワークの最小単位\n",
    "    2. 深層学習 の流れ\n",
    "    3. 隠れ層 hidden layer の意義\n",
    "    4. 全結合層 Fully-connected layer / Dense layer\n",
    "    5. 活性化関数\n",
    "    6. 画像および教師データの変形\n",
    "    7. 損失関数\n",
    "    8. オプティマイザ (最適化関数)\n",
    "    9. バッチ処理\n",
    "- C. 深層学習専用のフレームワーク"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. 人工知能, 機械学習, 深層学習\n",
    "\n",
    "| <img src=\"https://www.nemotos.net/nb/img/ai_ml_dl.png\" width=\"400\"> |\n",
    "| ---: |\n",
    "| 「PythonとKerasによるディープラーニング」より引用 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| 日本語名 | 英語名 | 略語 |\n",
    "| :-- | :-- | :-- |\n",
    "| 人工知能 | Artificial Intelligence | AI |\n",
    "| 機械学習 | Machine Learning | ML |\n",
    "| 深層学習 | Deep Learning | DL |\n",
    "\n",
    "- 人工知能: 「本来ならば人が行う知的な作業を自動化する取り組み」いちばん広い概念\n",
    "- 機械学習: 「特定のタスクの実行方法をコンピュータが学習する」\n",
    "    - 従来のプログラミング\n",
    "        - 人が「ルール」と「データ」を準備し、コンピュータはルールに従ってデータを処理する\n",
    "    - 機械学習のプログラミング\n",
    "        - 人は「データ」と「データから期待される答え」を準備し、コンピュータは「ルール」を出力する。\n",
    "        - システムは「訓練」されながらルールの精度を高めていく\n",
    "\n",
    "| <img src=\"https://www.nemotos.net/nb/img/programming_paradigm.png\" width=\"400\"> |\n",
    "| ---: |\n",
    "| 「PythonとKerasによるディープラーニング」より引用 |\n",
    "    \n",
    "- 深層学習: 機械学習の一種であり、データの特徴がどう表現(representation)されるかを学習する枠組み\n",
    "    - 入力されたデータを、複数の「層」からなるネットワークを通す。各層は入力されたデータから「特徴量」を学習し、それを次の層に渡していく。入力層と出力層の間にある層は「隠れ層」と呼ばれる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. 深層学習の概要\n",
    "\n",
    "### 1. ニューラルネットワークの最小単位\n",
    "- ニューラルネットワークの最小単位は、入力信号 $x$、バイアス(bias)、重み(weight)、活性化関数(activation function)、出力(かつ次の入力)信号 $y$ で構成される。図の○をユニットと呼ぶ\n",
    "\n",
    "<img src=\"https://www.nemotos.net/nb/img/minimal_component.png\" width=\"250\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 第1層にある 信号 $x$ が次の層に渡される時、重みを掛けることで重みづけがされる。重み weight は $w$ で表す。そして、$xw$ の結果に対し、さらにバイアス $b$ を付加する。一部のニューラルネットワークにはハンデを与えるようなイメージである。バイアス bias は $b$ として表す。この場合、次の層に行く信号は、$xw + b$ で表される\n",
    "- 第2層には関数が準備されている。先程の $xw + b$ がこの関数で処理され、その結果、出力信号 $y$ となる。この関数を、**活性化関数 activation function** という。上の図は、数式で表すならば、$ y = a(xw + b)$ と表される\n",
    "- 実際は、$y$ が次の層への入力となる。第2層が隠れ層 hidden layer の場合、頭文字をとって $h$ と表される"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 深層学習の流れ\n",
    "\n",
    "| <img src=\"https://www.nemotos.net/nb/img/dl_overview.png\" width=\"400\"> |\n",
    "| ---: |\n",
    "| 「PythonとKerasによるディープラーニング」より引用 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 入力 $x$ は重み $w$ で重みづけされて第1層に入っていく。第1層で活性化関数で処理された値となる\n",
    "- 第1層で処理された値は、別の重みで重みづけされて第2層に入っていく。第2層でも別の活性化関数で処理される\n",
    "- これらを繰り返し、最後に予測値 $y'$ が出力される\n",
    "    - 上の3つの流れを、**順伝播 forward propagation** という \n",
    "- 予測値 $y'$ は 真の値 $y$ と比較される\n",
    "- 予測値と真の値を **損失関数 loss function** で処理することで、**損失値 loss** を算出する\n",
    "- 損失値が0に近づけば近づくほど、モデルは正確となる\n",
    "- 損失関数を最小にするように重みを更新する\n",
    "    - 上の4つの流れを、**逆伝播 backward propagation** という\n",
    "- 順伝播と逆伝播を繰り返し、パラメータが適切な値に収束したら、別のデータセットを使用して、そのモデルが妥当かどうかを評価する。これを**バリデーション validation** という"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 隠れ層 hidden layer の意義\n",
    "\n",
    "- ここで、隠れ層の意義について考えてみる\n",
    "\n",
    "<img src=\"https://www.nemotos.net/nb/img/dl_overview_1.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 問題\n",
    "- 2つのベクトル $x_1$, $x_2$ と 4つの活性化関数 NOT, OR, AND, NAND を使って、以下のベクトル を表現してください\n",
    "- $x_1$ と $x_2$\n",
    "\n",
    "| x1 | x2 |\n",
    "| :--: | :--: |\n",
    "| 0 | 0 |\n",
    "| 0 | 1 |\n",
    "| 1 | 0 |\n",
    "| 1 | 1 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 活性化関数\n",
    "    - NOT: 0と1が入れ替わる\n",
    "    - OR: $x_1$ と $x_2$ の要素のどちらかが 1 ならば 1 (どちらが1でも1), どちらも 0 ならば 0\n",
    "    - AND: $x_1$ と $x_2$ の要素のどちらも 1 ならば 1, それ以外は0\n",
    "    - NAND: AND の結果をNOTに代入する    \n",
    "\n",
    "| x1 | x2 | NOT(x2) | OR(x1,x2) | AND(x1,x2) | NAND(x1,x2) |\n",
    "| -- | -- | -- | -- | -- | -- |\n",
    "| 0 | 0 | 1 | 0 | 0 | 1 |\n",
    "| 0 | 1 | 0 | 1 | 0 | 1 |\n",
    "| 1 | 0 | 1 | 1 | 0 | 1 |\n",
    "| 1 | 1 | 0 | 1 | 1 | 0 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 問題1. $y_1$ (簡単にとける)\n",
    "\n",
    "| y1 |\n",
    "| -- |\n",
    "| 1 |\n",
    "| 0 |\n",
    "| 0 |\n",
    "| 0 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOT, OR, AND, NAND を定義\n",
    "\n",
    "# NOT は 1 を 0 に、0 を 1 に変える。値が 0 か 1 しかないのであれば、1から引けばよい\n",
    "def NOT(a):\n",
    "    b = 1 - a\n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OR は 論理和 bool型を応用する\n",
    "def OR(a,b):\n",
    "    # 0 より大きい場合は True\n",
    "    c = (a + b) > 0\n",
    "    # astype() メソッド と np.int を使うと bool型を 0, 1に変換できる\n",
    "    c = c.astype(np.int)\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AND は 論理積 0 と 1 ならば、かければよい\n",
    "def AND(a,b):\n",
    "    c = a * b\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NAND は AND の結果を NOT にするので、1 から AND をひけばよい\n",
    "def NAND(a,b):\n",
    "    c = 1 - a * b\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = np.array([[0],[0],[1],[1]])\n",
    "x2 = np.array([[0],[1],[0],[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x1 を表示\n",
    "x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x2 を表示\n",
    "x2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| x1 | x2 | NOT(x2) | OR(x1,x2) | AND(x1,x2) | NAND(x1,x2) |\n",
    "| -- | -- | -- | -- | -- | -- |\n",
    "| 0 | 0 | 1 | 0 | 0 | 1 |\n",
    "| 0 | 1 | 0 | 1 | 0 | 1 |\n",
    "| 1 | 0 | 1 | 1 | 0 | 1 |\n",
    "| 1 | 1 | 0 | 1 | 1 | 0 |\n",
    "\n",
    "\n",
    "| y1 |\n",
    "| -- |\n",
    "| 1 |\n",
    "| 0 |\n",
    "| 0 |\n",
    "| 0 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OR関数\n",
    "OR(x1,x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AND関数\n",
    "AND(x1,x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NAND\n",
    "NAND(x1,x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y1 は [1, 0, 0, 0] これは OR(x1,x2) を NOT に代入したものといえる\n",
    "# NAND と同じ要領で NOR という\n",
    "NOT(OR(x1,x2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 今行ったことを図式にすると以下のように示せる\n",
    "\n",
    "<img src=\"https://www.nemotos.net/nb/img/nor_y1.png\" width=\"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- NOR($x_1$,$x_2$) は下のようなグラフを書くための関数ともいえる\n",
    "\n",
    "| x1 | x2 | NOR(x1,x2) |\n",
    "| -- | -- | -- |\n",
    "| 0 | 0 | 1 |\n",
    "| 0 | 1 | 0 |\n",
    "| 1 | 0 | 0 |\n",
    "| 1 | 1 | 0 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.nemotos.net/nb/img/nor_x1_x2_graph.png\" width=\"300\">\n",
    "\n",
    "- 直線で○と◇を2分割できるようなグラフは関数に1回かけることで解決する\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 問題2. $y_2$ (難しい)\n",
    "\n",
    "| x1 | x2 |  | y2 |\n",
    "| -- | -- | -- | -- |\n",
    "| 0 | 0 |   | 0 |\n",
    "| 0 | 1 |   | 1 |\n",
    "| 1 | 0 |   | 1 |\n",
    "| 1 | 1 |   | 0 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $y_2$ はグラフで表すと以下のように示せる\n",
    "\n",
    "<img src=\"https://www.nemotos.net/nb/img/xor_graph.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- この○と◇を2分割するには曲線を書くしかない\n",
    "\n",
    "<img src=\"https://www.nemotos.net/nb/img/xor_graph_with_line.png\" width=\"300\">\n",
    "\n",
    "- この問題を解くには、中間層を作る必要がある"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 入力データ: x1 と x2\n",
    "# 関数: NAND\n",
    "# 出力: s1\n",
    "s1 = NAND(x1,x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| x1 | x2 |  | s1 = NAND(x1,x2) |\n",
    "| -- | -- | -- | -- |\n",
    "| 0 | 0 |   | 1 |\n",
    "| 0 | 1 |   | 1 |\n",
    "| 1 | 0 |   | 1 |\n",
    "| 1 | 1 |   | 0 |  \n",
    "\n",
    "<img src=\"https://www.nemotos.net/nb/img/x1_x2_nand_s1.png\" width=\"200\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# s1 を表示\n",
    "s1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 入力データ: x1 と x2\n",
    "# 関数: OR\n",
    "# 出力: s2\n",
    "s2 = OR(x1, x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| x1 | x2 |  | s2 = OR(x1,x2) |\n",
    "| -- | -- | -- | -- |\n",
    "| 0 | 0 |   | 0 |\n",
    "| 0 | 1 |   | 1 |\n",
    "| 1 | 0 |   | 1 |\n",
    "| 1 | 1 |   | 1 |\n",
    "\n",
    "<img src=\"https://www.nemotos.net/nb/img/x1_x2_or_s2.png\" width=\"200\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s2 を表示\n",
    "s2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 入力データ: s1 と s2\n",
    "# 関数: AND\n",
    "# 出力: y2\n",
    "y2 = AND(s1,s2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| s1 | s2 |  | y2 = AND(s1,s2) |\n",
    "| -- | -- | -- | -- |\n",
    "| 1 | 0 |   | 0 |\n",
    "| 1 | 1 |   | 1 |\n",
    "| 1 | 1 |   | 1 |\n",
    "| 0 | 1 |   | 0 |\n",
    "\n",
    "<img src=\"https://www.nemotos.net/nb/img/s1_s2_and_y2.png\" width=\"200\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y2 を表示\n",
    "y2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- これらをすべてまとめると、以下になる\n",
    "\n",
    "<img src=\"https://www.nemotos.net/nb/img/nand_or_and_y2.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $x_1$, $x_2$ を **NAND**関数で処理した出力を $s_1$, **OR**関数で処理した出力を $s_2$ とした後、$s_1$, $s_2$ を入力データとして **AND**関数 で処理することにより、$y_2$ を求めることができた\n",
    "- 重み付けはしていない。$w$ はすべて1となる\n",
    "- 今の場合の $s_1$, $s_2$ に相当する層を 「**中間層**」 または 「**隠れ層**」 と呼ぶ\n",
    "- 深層学習は、隠れ層を上手に使うことによって、**データを線形だけでなく非線形に分類できる**\n",
    "- \n",
    "ちなみに「ニューラルネットワーク」と呼ばれるが、実際、脳のネットワークを表しているわけではない。ただのコンピュータ回路"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 全結合層 Fully-connected layer / Dense layer\n",
    "- 全結合層の「層」はユニットのある層でなく、ユニットとユニットが結合されている層を指す\n",
    "- Dense層とも呼ばれる\n",
    "- 全結合層の定義：ある層における個々のユニットと、次の層にあるユニットがすべて連結している"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.nemotos.net/nb/img/fully_connected_layer.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 全結合層は、行列演算で簡単に表示することができる\n",
    "    - 結合の重みづけを w, バイアスを b で表示\n",
    "    \n",
    "<img src=\"https://www.nemotos.net/nb/img/weight_bias.png\" width=\"250\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 深層学習でよく使われる w のルール\n",
    "    - 今いる層の 1番め のユニットと、前の層の 3番目 のユニットを結ぶ線の重みは、$w_{13}$ と表現する\n",
    "    - 重みを考える際、基準になる層は、データが「入ってくる」層\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "h_1 \\\\\n",
    "h_2 \\\\\n",
    "h_3\n",
    "\\end{bmatrix}\n",
    "= \n",
    "\\begin{bmatrix}\n",
    "1 & x_1 & x_2 & x_3\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "b_1 & b_2 & b_3 \\\\ \n",
    "w_{11} & w_{21} & w_{31}\\\\\n",
    "w_{12} & w_{22} & w_{32}\\\\\n",
    "w_{13} & w_{23} & w_{33}\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 活性化関数\n",
    "\n",
    "<img src=\"https://www.nemotos.net/nb/img/activation_function.png\" width=\"500\">\n",
    "\n",
    "- 深層学習によく用いられる活性化関数として、以下の3つが挙げられる\n",
    "    - **ReLU**関数: 隠れ層に用いられる\n",
    "    - **Sigmoid**関数: 2クラス分類の出力層に用いられる (犬/猫, 健常/疾患)\n",
    "    - **Softmax**関数: 多クラス分類の出力層に用いられる (数字の判別, 3クラス以上の判別)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 活性化関数の特徴\n",
    "\n",
    "| 関数名 | 特徴 | \n",
    "| :-- | :-- |\n",
    "| ReLU | 隠れ層に使うことで、非線形問題を解くことができるようになる <br> Sigmoid関数は0-1の値しかとらないので層が厚くなるほど誤差が小さくなっていき、<br>入力層まで誤差が伝搬する前に誤差が消失するという勾配消失問題が発生する |\n",
    "| Sigmoid | 0-1の間の確率で表現可能なため、2クラス分類の出力層に用いる |\n",
    "| Softmax | 各クラスの確率の総和が1となるように正規化された関数であるため、多クラス分類の出力層に用いる |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ReLU関数\n",
    "- **Re**ctified **L**inear **U**nit 関数の略\n",
    "    - rectify: 整流する\n",
    "- 負の信号はすべて0とし、正の信号はそのままとする\n",
    "\n",
    "\n",
    "- 隠れ層で使われることが多い\n",
    "    - 計算式がシンプルなので処理が速い\n",
    "    - 0以下は0となるため、信号を発生しないユニットを表現できる\n",
    "\n",
    "\n",
    "- 数式では以下で表される\n",
    "\n",
    "$$\n",
    "relu(x) = \n",
    "\\begin{cases}\n",
    "x & (x > 0)\\\\\n",
    "0 & (x \\leq 0)\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matplotlib でグラフを描画するために matplotlib.pyplot を plt としてインポート\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relu 関数を定義\n",
    "def relu(x):\n",
    "  return np.maximum(0, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relu 関数を描画\n",
    "# xは-5.0から5.0まで0.1刻み\n",
    "x = np.arange(-5.0, 5.0, 0.1)\n",
    "y = relu(x)\n",
    "# x と y をプロット\n",
    "plt.plot(x, y)\n",
    "# グラフのタイトルを \"ReLU function\" に設定\n",
    "plt.title(\"ReLU function\")\n",
    "# いざ、描画\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLU の効果を実感してみる\n",
    "# 正規分布の 5 x 3 の行列を作成 np.random.randn で作成できる\n",
    "np.random.seed(seed=42) #皆が同じ結果を得ることができるようにする\n",
    "a = np.random.randn(5,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aの内容を表示\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relu 関数にかけると、負の値が 0 となり、それ以外はそのままであることがわかる\n",
    "relu(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sigmoid関数\n",
    "\n",
    "- 以下の数式であらわされる関数であり、出力は0と1の間に収束する\n",
    "\n",
    "$$\n",
    "sigmoid(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "- $x$ が大きくなる → $e^{-x}$ が小さくなる → 値は 1 に近づく\n",
    "- $x$ が小さくなる → $e^{-x}$ が大きくなる → 値は 0 に近づく\n",
    "\n",
    "\n",
    "- 2つのものを分類するような時に使われる\n",
    "    - 0.5以上だったら A, 0.5未満だったら B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigmoid関数を定義\n",
    "def sigmoid(x):\n",
    "  return 1 / (1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigmoid関数を描画\n",
    "x = np.arange(-5.0, 5.0, 0.1)\n",
    "y = sigmoid(x)\n",
    "plt.plot(x, y)\n",
    "# y軸は -0.1 から 1.1 までの範囲に設定\n",
    "plt.ylim(-0.1, 1.1)\n",
    "plt.title(\"Sigmoid function\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Softmax関数\n",
    "- 主に多クラスの分類問題の出力層で使われる関数\n",
    "- 出力層がn個のユニットで構成される時、k番目の出力 $y_k$ は、数式では以下であらわされる\n",
    "\n",
    "$$\n",
    "y_k = \\frac{e^{a_k}}{\\sum_{i=1}^n e^{a_i}} = \\frac{e^{a_k}}{e^{a_1} + e^{a_2} + ... + e^{a_n}}\n",
    "$$\n",
    "\n",
    "\n",
    "- Softmax関数は、出力をすべて足すと 1 になることから、それぞれの出力を「確率」として扱える\n",
    "    - 出力層が4つのユニットから構成され、それぞれの出力が [0.1, 0.6, 0.1, 0.2] だとすると、「2番めのユニットが最も確率が高いので、答えは2番め」「答えは60%の確率で2番め、20%で4番目、10%で1番目か3番目」といったように表現できる\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# softmax 関数を定義\n",
    "def softmax(a):\n",
    "  exp_a = np.exp(a)\n",
    "  sum_exp_a = np.sum(exp_a)\n",
    "  y = exp_a / sum_exp_a\n",
    "  return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# softmax関数は、ひとつのクラスの値の変化だけ見ると sigmoid と変わらない\n",
    "# 出力が 3クラスあるとした時、第3クラスの値だけが -5 から 5 に変わった時の\n",
    "# softmax関数の出力の推移を見る\n",
    "\n",
    "# 第1クラスと第2クラスは 0 で固定, 第3クラスのみ -5から5に値が変わるとする\n",
    "x = np.array([[0, 0, x3] for x3 in np.arange(-5, 5, 0.1)])\n",
    "# x を softmax関数に代入\n",
    "y = np.array([softmax(x[i]) for i in range(len(x))])\n",
    "# y を最初の5行だけ出力\n",
    "print(y[:5])\n",
    "# 第3クラスのみ取り出してプロット\n",
    "plt.plot(x[:,2], y[:,2])\n",
    "plt.title('softmax of a class')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. 画像および教師データの変形\n",
    "#### 画像の変換\n",
    "- 深層学習は大量のデータを扱う\n",
    "- 2次元画像は行列で表示されるが、大量に扱う時は、使い勝手が悪い\n",
    "- このため、1つの画像を1行の行列で表示する。こうすることで、多くの画像をひとつの行列で扱うことができる\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数字を 5x4 の行列で(画像として)表示してみる\n",
    "zero = np.array([[1,1,1,1],[1,0,0,1],[1,0,0,1],[1,0,0,1],[1,1,1,1]])\n",
    "one = np.array([[0,0,1,0],[0,0,1,0],[0,0,1,0],[0,0,1,0],[0,0,1,0]])\n",
    "two = np.array([[1,1,1,1],[0,0,0,1],[1,1,1,1],[1,0,0,0],[1,1,1,1]])\n",
    "three = np.array([[1,1,1,1],[0,0,0,1],[1,1,1,1],[0,0,0,1],[1,1,1,1]])\n",
    "four = np.array([[1,0,0,1],[1,0,0,1],[1,1,1,1],[0,0,0,1],[0,0,0,1]])\n",
    "five = np.array([[1,1,1,1],[1,0,0,0],[1,1,1,1],[0,0,0,1],[1,1,1,1]])\n",
    "\n",
    "# 0, 1, 2, 3, 4, 5 を nsets という3次元の配列に格納する\n",
    "nsets = np.array([zero,one,two,three,four,five])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nsets は 6つの数字 x 5 x 4\n",
    "nsets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nsets の次元は 3次元\n",
    "nsets.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for ループを使って、それぞれの数字の配列を可視化する\n",
    "for i in nsets:\n",
    "    # 図のサイズを 1.25 x 1 インチに設定\n",
    "    plt.figure(figsize=(1.25,1))\n",
    "    # 配列 nsets の内容をひとつずつ表示\n",
    "    plt.imshow(i, cmap = 'gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape メソッドを使うことで数字の 0 をベクトル化する\n",
    "# 今は 5 x 4 の行列のため、20の要素がある → 1行20列にしたい\n",
    "# reshape の 引数を -1 にすると、もうひとつの引数から推測してくれる\n",
    "\n",
    "# 変数 zero を ベクトルにしたものを zero_vector に代入する\n",
    "zero_vector = zero.reshape(1,-1) # zero.reshape(1,20) と同じ意味\n",
    "# 可視化してみる\n",
    "plt.imshow(zero_vector,cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nsets も同様に、全ての数字をベクトルにする\n",
    "# 数字が 6つ あるので、6行20列で今、ここにあるすべての数字を表現できる\n",
    "nsets_vector = nsets.reshape(6,-1)\n",
    "plt.imshow(nsets_vector,cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "# 1行がひとつの数字をあらわす"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape の特性から情報がきちんと保存されていることを確認\n",
    "# nsets の 3行目(インデックスは2)を取り出して変数 a に代入する\n",
    "a = nsets[2,:]\n",
    "# 5行4列に戻す\n",
    "b = a.reshape(5,4)\n",
    "# 可視化する\n",
    "plt.figure(figsize=(1.25,1))\n",
    "plt.imshow(b, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 教師データの変換\n",
    "- 手書き数字の分類を行う場合、教師データは、シンプルに\"0\" \"1\" \"2\" という数字で提供されている\n",
    "- 深層学習では、この数字を **\"One-hot ベクトル\"** に変換する\n",
    "- One-hot とは、「ひとつだけホット、あとはコールド」という意味\n",
    "- ホットは 1, コールドは 0 で表現する\n",
    "- 数字の分類であれば、カテゴリ(クラスとも言う)は 10 (0-9)\n",
    "- 0, 1, 2, 3, 4, 5 を one-hot ベクトル に次のように変換する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "0 = \n",
    "\\begin{bmatrix}\n",
    "1 \\\\\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "0\n",
    "\\end{bmatrix}\n",
    "1 = \n",
    "\\begin{bmatrix}\n",
    "0 \\\\\n",
    "1 \\\\\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "0\n",
    "\\end{bmatrix}\n",
    "2 = \n",
    "\\begin{bmatrix}\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "1 \\\\\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "0\n",
    "\\end{bmatrix}\n",
    "3 = \n",
    "\\begin{bmatrix}\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "1 \\\\\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "0\n",
    "\\end{bmatrix}\n",
    "4 = \n",
    "\\begin{bmatrix}\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "1 \\\\\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "0\n",
    "\\end{bmatrix}\n",
    "5 = \n",
    "\\begin{bmatrix}\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "1 \\\\\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "0\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- こうすることで、Softmax関数の結果と照らし合わせることが可能となる\n",
    "\n",
    "$$\n",
    "softmax \\ output =\n",
    "\\begin{bmatrix}\n",
    "0.7 \\\\\n",
    "0.01 \\\\\n",
    "0.03 \\\\\n",
    "0.01 \\\\\n",
    "0.01 \\\\\n",
    "0.01 \\\\\n",
    "0.01 \\\\\n",
    "0.01 \\\\\n",
    "0.2 \\\\\n",
    "0.01\n",
    "\\end{bmatrix}\n",
    "one \\ hot \\ vector = \n",
    "\\begin{bmatrix}\n",
    "1 \\\\\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "0\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. 損失関数\n",
    "- 損失関数 loss function: 予測値と真の値の違いを表現する関数\n",
    "- 脳画像解析の世界では cost function と呼ばれるものと同じ\n",
    "- 損失関数の出力: 損失値 loss\n",
    "    - 損失値 loss が最小になれば、予測値は真の値に近づく\n",
    "- 損失関数には複数あるが、代表的なものとして、2乗和誤差と交差エントロピー誤差を紹介\n",
    "- 損失関数の使い分け\n",
    "\n",
    "| 目的 | 関数名 | Function Name | 損失関数名<br>(Tensorflow) | 損失関数名(PyTorch) |\n",
    "| :-- | :-- | :-- | :-- | :-- |\n",
    "| 回帰 | 平均二乗誤差 | Mean Squared Error | mean_squared_error | nn.MSELoss |\n",
    "| 2クラス分類 | バイナリ交差エントロピー | Binary Cross Entropy | binary_crossentropy | nn.BCELoss |\n",
    "| 多クラス分類 | ソフトマックス交差エントロピー | Softmax Cross Entropy | categorical_crossentropy (one-hot vector用)<br> sparse_categorical_crossentropy | nn.CrossEntropyLoss |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.nemotos.net/nb/img/dl_overview_2.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2乗和誤差 sum of squared error\n",
    "- k次元の予測値を $y_k$, 真の値(教師データ)を $t_k$ とすると、2乗和誤差は以下で表現される\n",
    "\n",
    "$$\n",
    "E = \\frac{1}{2}\\sum_k (y_k - t_k)^2\n",
    "$$\n",
    "\n",
    "- 各データに対して、「真の値と予測値の差分の2乗」を計算し、それを総和した値\n",
    "- $\\frac{1}{2}$ は微分する時に計算がしやすくなるような工夫\n",
    "- one hot ベクトルを使うと、すぐに計算できる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 損失関数として、2乗和誤差を定義\n",
    "def sum_of_squared_error(y,t):\n",
    "    return 0.5 * np.sum((y - t)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1次元における2乗和誤差を描画\n",
    "x = np.arange(0,1,0.01)\n",
    "y = 0.5 * x**2\n",
    "plt.plot(x,y)\n",
    "plt.title('Sum of squared error')\n",
    "plt.xlabel(\"predict - true\")\n",
    "plt.ylabel('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 手書きの数字画像を認識させるモデルを考える\n",
    "# 出力層の活性化関数は Softmax関数\n",
    "# ここでの ya は上図の予測値 Y' に相当\n",
    "ya = np.array([[0.01],[0.03],[0.7],[0.1],[0.01],[0.01],[0.01],[0.02],[0.1],[0.01]])\n",
    "ya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 2 が正解の one-hot ベクトル t2 を生成\n",
    "# 上図での真の目的値 Y に相当\n",
    "t2 = np.array([[0],[0],[1],[0],[0],[0],[0],[0],[0],[0]])\n",
    "t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 損失関数に代入\n",
    "# この結果が、損失値となる\n",
    "sum_of_squared_error(ya,t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 5 が正解の one-hot ベクトル t5 を生成し、\n",
    "# ya と t5 から損失を求める\n",
    "# この場合、t5 が真の目的値となる\n",
    "t5 = np.array([[0],[0],[0],[0],[0],[1],[0],[0],[0],[0]])\n",
    "t5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 損失関数に代入\n",
    "# 正解が2のときよりはるかに大きい → 損失が大きい → モデルがよくない\n",
    "sum_of_squared_error(ya,t5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 交差エントロピー誤差 cross entropy error\n",
    "- 交差エントロピー誤差は、以下の数式で定義される\n",
    "\n",
    "$$\n",
    "E = -\\sum_{k=1}^n t_k log (y_k)\n",
    "$$\n",
    "\n",
    "- n クラスのデータセットがあり、教師データが one-hot ベクトルの場合、 $t_k$ は正解は 1, 不正解は 0 となる\n",
    "- k番目のクラスが正解とし、その時の予測確率を $y_k$ すると、その時の交差エントロピー誤差は以下となる\n",
    "\n",
    "$$\n",
    "E = -log(y_k)\n",
    "$$\n",
    "\n",
    "- つまり、交差エントロピー誤差は、予測値(確率)を対数変換したものであり、予測値が1に近づけば近づくほど小さくなる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 損失関数として、交差エントロピー誤差を定義\n",
    "def cross_entropy_error(y,t):\n",
    "    #yが0になると無限大に発散してしまうため、小さな値を足すことで\n",
    "    # 無限大に発散することを予防する\n",
    "    delta = 1e-7 # 1e-7 は '10 の -7乗' を意味する\n",
    "    return -np.sum(t * np.log(y + delta))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 交差エントロピー誤差を描画\n",
    "x = np.arange(0,1,0.01)\n",
    "y = -np.log(x + 1e-7)\n",
    "plt.plot(x,y)\n",
    "plt.title('Cross-entropy error')\n",
    "plt.xlabel('output of softmax function')\n",
    "plt.ylabel('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 予測値 ya, 真の目的値 t2 に対して 交差エントロピー誤差を計算\n",
    "cross_entropy_error(ya,t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 予測値 ya, 真の目的値 t5 に対して 交差エントロピー誤差を計算\n",
    "cross_entropy_error(ya,t5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. オプティマイザ (最適化関数)\n",
    "\n",
    "<img src=\"https://www.nemotos.net/nb/img/dl_overview_3.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 深層学習の究極のゴールは、「損失値を0にすること」\n",
    "- 損失を少しでも 0 に近づけるように重み付けパラメータを更新する\n",
    "- パラメータと損失値の関係は下図のように関数で表現できる\n",
    "\n",
    "| <img src=\"https://www.nemotos.net/nb/img/optimizer.png\" width=400> |\n",
    "| --: |\n",
    "| 「PythonとKerasによるディープラーニング」より引用 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- オプティマイザ(最適化関数)は最小値を見つける\n",
    "- 直感的な理解としては、関数を微分することで局所における傾きを求め、傾きが 0 になるところを探す\n",
    "- 微分だけでは、上図での極小値にトラップされてしまう可能性もある\n",
    "- 微分に加えて関数の勾配による速度と加速度で生み出される「モーメンタム」を考慮することで、その問題を回避する\n",
    "- ボールを落とした時にボールが坂道を転がりながら一番低いところにおさまっていくイメージ\n",
    "- このプロセスが back propagation 逆伝播\n",
    "\n",
    "| アルゴリズム | 大まかな説明 |\n",
    "| :-- | :-- |\n",
    "| 確率的勾配法 (SGD) | 勾配の分だけパラメータを減らす |\n",
    "| モーメンタムSGD | SGD + モーメンタム (速度と慣性) |\n",
    "| AdaGrad | 学習が進むほど学習率を小さくする |\n",
    "| RMSprop |\tAdaGrad + 直近の勾配ほど強く影響 |\n",
    "| Adam | モーメンタムSGD + RMSprop |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. バッチ処理\n",
    "\n",
    "- オプティマイザでパラメータを最適化する際に、一度に入力するデータ数をバッチサイズという\n",
    "- 以下のような特性があり、ミニバッチ学習がよく使われる\n",
    "\n",
    "| 学習方法 | 学習手順 (図は [DXCEL WAVE](https://di-acc2.com/analytics/ai/6320/) より引用) | \n",
    "| :-- | :-- |\n",
    "| バッチ学習 | <img src=\"https://www.nemotos.net/nb/img/batch_learning.jpg\" width=\"300\"> |\n",
    "| オンライン学習 | <img src=\"https://www.nemotos.net/nb/img/online_learning.jpg\" width=\"300\"> |\n",
    "| ミニバッチ学習 | <img src=\"https://www.nemotos.net/nb/img/minibatch_learning.jpg\" width=\"300\"> |\n",
    "\n",
    "\n",
    "| 学習方法 | バッチサイズ | メリット | デメリット |\n",
    "| :-- | :-- | :-- | :-- |\n",
    "| バッチ学習 | 全データセット| ・全データを使うので<br>結果は安定しやすい | ・パラメータを1回更新するのに<br>全データセットに対して勾配を計算するため、<br>処理速度が遅い<br>・データが新しく追加になるたびに計算を<br>し直さないといけない |\n",
    "| オンライン学習 | 1例 | ・リアルタイムでモデル<br>更新を頻繁に行う<br>ケースに適用しやすい | ・外れ値に影響され、<br>結果が不安定になりやすい |\n",
    "| ミニバッチ学習 | ある程度<br>まとまった数 | バッチ学習と<br>オンライン学習の中間 | |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. 深層学習専用のフレームワーク\n",
    "- 深層学習をPythonで実装する場合、深層学習に特化したフレームワークを使うのが一般的\n",
    "- 2021年現在、2大巨頭は Tensorflow と PyTorch\n",
    "\n",
    "| フレームワーク | Tensorflow | PyTorch |\n",
    "| :--: | :-- | :-- |\n",
    "| 開発 | Google | Facebook |\n",
    "| 登場年| 2015年 | 2016年|\n",
    "| 特徴 | Kerasを使うことで実装しやすい | ロジックの把握がしやすく、研究方面に強い |\n",
    "\n",
    "- 今回は Tensorflow/Keras を使用\n",
    "- 基本をしっかり理解できれば PyTorch にもすんなり移行できるはず\n",
    "\n",
    "- 次のセクションで、実際にTensorflow/Keras を使って、手書き数字の識別にトライする"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
