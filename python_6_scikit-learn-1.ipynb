{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a href=\"https://colab.research.google.com/github/kytk/AI-MAILs/blob/main/python_6_scikit-learn-1.ipynb?hl=ja\" target=\"_blank\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHVIs_ypyjg0"
      },
      "source": [
        "\n",
        "## 医療従事者のためのPython: 機械学習\n",
        "\n",
        "根本清貴 (筑波大学医学医療系精神医学)\n",
        "\n",
        "Ver.20240730\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "### 1. 機械学習とは\n",
        "- 機械学習の定義\n",
        "    - 「データからルールやパターンを導き出し、予測や意思決定を行う技術」\n",
        "    - (従来: 人間がルールを決める)\n",
        "- 機械学習の種類\n",
        "    - 教師あり学習\n",
        "        - 入力データとその対応する正解(ラベル)がペアになったデータセットを用いてモデルを訓練\n",
        "        - 新しいデータが入ってきた時にそのモデルから正しいラベルを予測\n",
        "        - 例: 画像の分類(犬と猫の画像を分類)、スパムメールの分類、価格予測\n",
        "    - 教師なし学習\n",
        "        - ラベルのないデータを用いてモデルを訓練\n",
        "        - データの内部構造やパターンを見つけ出す\n",
        "        - 例: クラスタリング(似たデータをグループに分ける)、次元削減(データの特徴を少数の重要な特徴に圧縮する)\n",
        "    - 強化学習\n",
        "        - エージェント(学習者)が環境と相互作用しながら学習する。エージェントは行動を選択し、その結果として得られる報酬を基に次の行動を改善する\n",
        "        - 長期的な累積報酬を最大化することが目標\n",
        "        - 例: ゲーム、ロボット制御、自動運転\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### 2. 機械学習のステップ\n",
        "\n",
        "1. データの収集 (Data Collection):\n",
        "    - 必要なデータを収集する\n",
        "    - 例：センサーデータ、ユーザー行動データ、画像データなど\n",
        "\n",
        "2. データの確認とクレンジング (Data Inspection and Cleaning):\n",
        "    - データの品質を確認し、不足している値やノイズを処理する\n",
        "    - 例：欠損値の補完、異常値の除去など\n",
        "\n",
        "3. **データの分割 (Data Splitting)**:\n",
        "    - データセットを訓練データ、検証データ、テストデータに分ける\n",
        "    - 例：訓練データ 70%、検証データ 15%、テストデータ 15%\n",
        "\n",
        "4. **データの前処理 (Data Preprocessing)**:\n",
        "    - データをモデルに適した形式に変換する\n",
        "    - 例：標準化、正規化、カテゴリ変数のエンコーディングなど\n",
        "\n",
        "5. 特徴量エンジニアリング (Feature Engineering):\n",
        "    - モデルの性能を向上させるために、新しい特徴量を作成する\n",
        "    - 例：日付から曜日や月を抽出する、テキストデータから単語の出現頻度を計算するなど\n",
        "\n",
        "6. **モデルの選択 (Model Selection)**:\n",
        "    - 解決する問題に最適なモデルを選ぶ\n",
        "    - 例：回帰モデル、決定木、ニューラルネットワークなど\n",
        "\n",
        "7. **モデルの学習 (Model Training)**:\n",
        "    - 訓練データを用いてモデルを訓練する\n",
        "    - 例：パラメータの最適化、ハイパーパラメータチューニングなど\n",
        "\n",
        "8. **モデルの検証 (Model Validation)**:\n",
        "    - 検証データを使ってモデルの性能を評価する\n",
        "    - 例：クロスバリデーション、精度、再現率、F1スコアなどの評価指標を計算する\n",
        "\n",
        "9. **データの予測 (Data Prediction)**:\n",
        "    - テストデータを使って予測を行う\n",
        "    - 例：未知のデータに対する予測結果を得る\n",
        "\n",
        "10. **モデルの評価 (Model Evaluation)**:\n",
        "    - テストデータを用いてモデルの最終的な性能を評価する\n",
        "    - 例：精度、再現率、F1スコア、混同行列などの評価指標を計算する\n",
        "\n",
        "11. モデルの改善 (Model Improvement):\n",
        "    - 必要に応じてモデルを改善し、再訓練する\n",
        "    - 例：特徴量の追加や削除、別のモデルの試行など\n",
        "\n",
        "12. モデルの展開 (Model Deployment):\n",
        "    - モデルを実際の環境に展開し、運用する\n",
        "    - 例：Webサービスとしてデプロイ、組み込みシステムへの実装など\n",
        "\n",
        "13. モデルの監視とメンテナンス (Model Monitoring and Maintenance):\n",
        "    - 展開後のモデルの性能を監視し、必要に応じて更新や再訓練を行う\n",
        "    - 例：モデルのドリフト検出、定期的な再訓練など\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Pythonでの機械学習\n",
        "- Pythonには、scikit-learn (サイキット ラーン)という機械学習に特化したパッケージがある\n",
        "- scikit-learn の中に医療のサンプルデータとして、糖尿病データセット(Pandasで使用)と乳がんデータセットが提供されている\n",
        "- 今回は乳がんデータセットを使って教師あり学習を体験する"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4. 乳がんデータセット\n",
        "- データセットの名称: Breast Cancer Wisconsin (Diagnostic) dataset\n",
        "- サンプル数: 569\n",
        "- 特徴量の数: 30\n",
        "- ターゲットの種類: 2クラス（良性と悪性）\n",
        "- 特徴量の種類: 実数値\n",
        "\n",
        "#### 4.1. データセットの30の特徴量\n",
        "- 細胞診における細胞核の特徴\n",
        "\n",
        "| 英語 | 日本語 |\n",
        "| --- | --- |\n",
        "| mean radius | 平均半径 |\n",
        "| mean texture | 平均テクスチャ |\n",
        "| mean perimeter | 平均周囲長 |\n",
        "| mean area | 平均面積 |\n",
        "| mean smoothness | 平均平滑度 |\n",
        "| mean compactness | 平均コンパクト度 |\n",
        "| mean concavity | 平均陥凹度 |\n",
        "| mean concave points | 平均陥凹点数 |\n",
        "| mean symmetry | 平均対称性 |\n",
        "| mean fractal dimension | 平均フラクタル次元 |\n",
        "| radius error | 半径誤差 |\n",
        "| texture error | テクスチャ誤差 |\n",
        "| perimeter error | 周囲長誤差 |\n",
        "| area error | 面積誤差 |\n",
        "| smoothness error | 平滑度誤差 |\n",
        "| compactness error | コンパクト度誤差 |\n",
        "| concavity error | 陥凹度誤差 |\n",
        "| concave points error | 陥凹点数誤差 |\n",
        "| symmetry error | 対称性誤差 |\n",
        "| fractal dimension error | フラクタル次元誤差 |\n",
        "| worst radius | 最悪の半径 |\n",
        "| worst texture | 最悪のテクスチャ |\n",
        "| worst perimeter | 最悪の周囲長 |\n",
        "| worst area | 最悪の面積 |\n",
        "| worst smoothness | 最悪の平滑度 |\n",
        "| worst compactness | 最悪のコンパクト度 |\n",
        "| worst concavity | 最悪の陥凹度 |\n",
        "| worst concave points | 最悪の陥凹点数 |\n",
        "| worst symmetry | 最悪の対称性 |\n",
        "| worst fractal dimension | 最悪のフラクタル次元 |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 4.2. 教師あり学習: 乳がんデータセットの読み込みと確認\n",
        "- scikit-learn ライブラリを使用してデータセットを読み込み、データを確認する"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "import pandas as pd\n",
        "\n",
        "# データセットの読み込み\n",
        "dataset = load_breast_cancer()\n",
        "\n",
        "# データセットの基本情報を表示\n",
        "# DESCR属性に情報が含まれている\n",
        "print(dataset.DESCR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# この情報の中に良性と悪性の数などが記載されているはず\n",
        "# malignant という表現がある文を見つける\n",
        "\n",
        "# 説明文を行ごとに分割\n",
        "# split() は1行毎日に分割するメソッド\n",
        "description_lines = dataset.DESCR.split('\\n')\n",
        "\n",
        "# 'malignant' を含む行を抽出\n",
        "# リスト内包表記を使用\n",
        "matching_lines = [line for line in description_lines if 'malignant' in line.lower()]\n",
        "\n",
        "# 結果の表示\n",
        "# strip() は str型 のメソッドで、文字列の前もしくは後ろにある空白を取り除く\n",
        "for line in matching_lines:\n",
        "    print(line.strip())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### (参考) リスト内包表記\n",
        "\n",
        "- 既存のリスト、タプル、文字列など、「繰り返し可能なデータ(オブジェクト)」から新しいリストを作る際に使う、Pythonの便利な機能\n",
        "- 普通のforループを使うよりも、短くてきれいなコードが書ける\n",
        "- 以下のようなルール\n",
        "    ```\n",
        "    新しいリスト = [変数を操作したもの for 変数 in 繰り返し可能なデータ]\n",
        "    ```\n",
        "\n",
        "- あるリストの数字を2倍にした新しいリストを作成する例を、forループの場合とリスト内包表記の場合で書く\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# forループ\n",
        "numbers = [1, 2, 3, 4, 5]\n",
        "doubled = []\n",
        "for num in numbers:\n",
        "    doubled.append(num * 2)\n",
        "print(doubled)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# リスト内包表記\n",
        "# 元のリスト: numbers\n",
        "# 変数: num\n",
        "# 変数を操作したもの: num*2 を並べたもの\n",
        "\n",
        "numbers = [1, 2, 3, 4, 5]\n",
        "doubled = [num * 2 for num in numbers]\n",
        "print(doubled)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 4.2. 教師あり学習: 乳がんデータセットの読み込みと確認 (続き)\n",
        "- データを Pandas の DataFrame に変換して、データ内容を確認していく"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# データをPandas の DataFrameに変換\n",
        "df = pd.DataFrame(dataset.data, columns=dataset.feature_names)\n",
        "df['target'] = dataset.target\n",
        "\n",
        "# データの先頭30行を表示\n",
        "df.head(30)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 正解ラベル target の 0 と 1 の数を数える\n",
        "# target が 0 であるかどうかを判断\n",
        "df['target'] == 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# (df['target'] == 0).sum() とすると 0 がTrue の数を調べられる\n",
        "# 悪性が212, 良性が357 より、0 は悪性、1 は良性\n",
        "\n",
        "# (df['target'] ==0) はdfのターゲット列が0のものだけ抽出したもの(オブジェクト)であり、\n",
        "# その合計を求めたいので、()でくくってひとつのまとまりとしている\n",
        "num0 = (df['target'] == 0).sum()\n",
        "num1 = (df['target'] == 1).sum()\n",
        "\n",
        "print(f'number of 0 = {num0}')\n",
        "print(f'number of 1 = {num1}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- 欠損値の確認\n",
        "    - 乳がんデータセットはサンプルデータなので欠損値はないが、実際のデータを扱う際には欠損値の確認が重要\n",
        "    - 欠損値がどこにあるかを調べるためには、 `df.isnull().sum()` を使う\n",
        "    - このメソッドは、各列に含まれる欠損値の数を表示する"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 欠損値の確認\n",
        "missing_values = df.isnull().sum()\n",
        "print(missing_values)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 参考\n",
        "# df を Excelファイルで出力しておくと、自分が機械学習を実際に行う時の参考になる\n",
        "df.to_excel('breast_cancer.xlsx', header=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 4.3. データの分割 (Data Splitting)\n",
        "\n",
        "- データセットを訓練用データ（training data）とテスト用データ（test data）に分割することで、モデルの訓練と評価を行う\n",
        "- モデルのハイパーパラメータ調整や性能評価を行うために、検証用データ（validation data）を使用することもある\n",
        "\n",
        "    1. 訓練データ (Training Data):\n",
        "        - モデルの訓練に使用する。モデルがこのデータに基づいて学習する\n",
        "    2. 検証データ (Validation Data):\n",
        "        - モデルのハイパーパラメータ調整や性能評価に使用する\n",
        "        - 通常、訓練データの一部を検証データとして使用する\n",
        "    3. テストデータ (Test Data):\n",
        "        - モデルの最終的な評価に使用する\n",
        "        - このデータは、訓練や検証には使用しない\n",
        "\n",
        "- 一般的な分割比率\n",
        "    - 訓練データ: 70% - 80%\n",
        "    - 検証データ: 10% - 15%\n",
        "    - テストデータ: 10% - 15%\n",
        "\n",
        "- scikit-learn では、 `train_test_split` という関数を用いて、データを分割できる\n",
        "- `train_test_split` はデータセットを2つにしか分割できないため、以下の方法をとる\n",
        "    - まずは、テストデータと訓練・検証データに分割\n",
        "    - その後、訓練・検証データを訓練データと検証データにに分割 (下図は金子貴久子先生提供)\n",
        "\n",
        "    <img src=\"https://www.nemotos.net/nb/img/data_splitting_test1st.png\" width=\"500\">\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# train_test_split をインポートする\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# データフレーム df を 特徴量 X とターゲット(正解ラベル) y に分ける\n",
        "X = df.drop(columns=['target'])\n",
        "y = df['target']\n",
        "\n",
        "# データをテスト用、訓練用、検証用に分割する\n",
        "# test_size=0.1 は、データの10%をテストデータに割り当てることを意味する\n",
        "# random_state=42 は、分割の再現性を保つためのシード値 任意の値でよい\n",
        "# 今、データ全体を訓練・検証データ (train_val) 90%、テストデータ (test) 10% として分割化し、\n",
        "# 訓練・検証データ を訓練データ (train) 80% と 検証データ (val) 20% に分割する\n",
        "# トータルで考えると、訓練データ 72%、検証データ 18%、テストデータ 10%\n",
        "\n",
        "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.2, random_state=42)\n",
        "\n",
        "# 分割後のデータサイズを表示\n",
        "print(f'訓練データのサイズ: {X_train.shape}')\n",
        "print(f'検証データのサイズ: {X_val.shape}')\n",
        "print(f'テストデータのサイズ: {X_test.shape}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 4.4. データの標準化と正規化\n",
        "\n",
        "- 標準化 (Standardization):\n",
        "    - データの平均を0、標準偏差を1にするスケーリング手法\n",
        "    - 各データポイントから平均を引き、標準偏差で割ることで計算\n",
        "    - 外れ値の影響を受けにくい\n",
        "    - 正規分布に従うデータに対して効果的\n",
        "    - scikit-learnでは`StandardScaler`クラスを使用\n",
        "\n",
        "- 正規化 (Normalization):\n",
        "    - データを0から1の範囲にスケーリングする手法\n",
        "    - 各データポイントを最小値との差分を、全体の範囲(最大値 - 最小値)で割ることで計算\n",
        "    - 主に異なる単位や尺度のデータを比較可能にするために使用\n",
        "    - scikit-learnでは`MinMaxScaler`クラスを使用\n",
        "\n",
        "- 標準化と正規化は、データを分割した後に行う。そのことで、テストデータに関する情報が訓練データに漏れる (リーク) ことを防止する\n",
        "- 標準化と正規化をどう使うかは自分のデータにあわせて考慮する\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "##### 標準化が必要な理由\n",
        "- 具体例:\n",
        "    - ある住宅価格予測モデルで、「部屋数」（1-10の範囲）と「面積」（10-1000平方メートルの範囲）という2つの特徴量があると仮定\n",
        "    - 標準化前: 2つの値のレンジが大きく異なるため、部屋数と面積の重要性の評価が難しい\n",
        "    - 標準化後: 両方の特徴量が同じスケールになり、部屋数と面積の相対的な重要性を適切に評価できる\n",
        "- 標準化を適用することで、特徴量間の相対的な大きさや変動を保ちながら、スケールの違いによる悪影響を軽減できる\n",
        "- モデルがデータの本質的な構造をより正確に捉えることが可能になる\n",
        "\n",
        "##### 正規化が必要な理由\n",
        "- 具体例:\n",
        "    - 家計の支出パターン分析\n",
        "        - 特徴量1: 食費（月10,000〜100,000円）\n",
        "        - 特徴量2: 娯楽費（月0〜50,000円）\n",
        "    - 正規化後：\n",
        "        - 食費: 0〜1の範囲（0が10,000円、1が100,000円に対応）\n",
        "        - 娯楽費: 0〜1の範囲（0が0円、1が50,000円に対応）\n",
        "- 正規化により、各カテゴリー内での相対的な支出の大きさが保持される。例えば、食費が0.5の場合、その家庭の食費は範囲の中間程度であることがわかる\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "##### 標準化と正規化の使い分け\n",
        "\n",
        "- 標準化を適用すべき特徴量:\n",
        "   - データが正規分布に従う、または近似的に正規分布とみなせる場合\n",
        "   - 外れ値が存在する可能性がある場合\n",
        "   - 特徴量間の相対的な大きさや変動が重要な場合\n",
        "       - 平均と標準偏差は変わるが、データの分布は維持される\n",
        "   - 距離ベースのアルゴリズム（例：K近傍法、主成分分析）を使用する場合\n",
        "   - 勾配降下法を用いるアルゴリズム（例：線形回帰、ロジスティック回帰）を使用する場合\n",
        "\n",
        "- 正規化を適用すべき特徴量:\n",
        "   - 異なる単位や尺度のデータを比較する必要がある場合\n",
        "   - データの元のスケールとの関連性を保持する必要がある場合（例：画像処理におけるピクセル値）\n",
        "   - データの分布が正規分布に従わない場合\n",
        "   - 0から1の範囲の値が必要なアルゴリズムを使用する場合（例：ニューラルネットワークの入力層）\n",
        "   - K-meansクラスタリングのアルゴリズムを使用する場合\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 標準化\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# StandardScaler() 関数を用いて、std_scaler という標準化するオブジェクトを作成\n",
        "std_scaler = StandardScaler()\n",
        "\n",
        "# 訓練データに対して標準化を適用\n",
        "# fit_transform は、標準化するパラメータを計算(fit)して、それをあてはめる(transform)\n",
        "X_train_standardized = std_scaler.fit_transform(X_train)\n",
        "\n",
        "# 検証用、テストデータに対しても標準化を適用\n",
        "# 訓練データで求めた標準化パラメータを用いて訓練データに適用\n",
        "X_val_standardized = std_scaler.transform(X_val)\n",
        "X_test_standardized = std_scaler.transform(X_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# X_train, X_val, X_test の元データの1列目の分布を確認\n",
        "# X_train, X_val, X_test は Pandas DataFrame 形式なので、X_train.iloc[:,0] で取り出す\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.histplot(X_train.iloc[:,0], label='train')\n",
        "sns.histplot(X_val.iloc[:,0], label='val')\n",
        "sns.histplot(X_test.iloc[:,0], label='test')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 標準化された X_train_standardized, X_val_standardized, X_test_standardized の1列目の分布を確認\n",
        "# X_train_standardized, X_val_standardized, X_test_standardized は numpy 形式なので、\n",
        "# X_train_standardized[:,0] で取り出す\n",
        "# ヒストグラムは同じでも、X軸の値が変わっていることに注意\n",
        "sns.histplot(X_train_standardized[:,0], label='train')\n",
        "sns.histplot(X_val_standardized[:,0], label='val')\n",
        "sns.histplot(X_test_standardized[:,0], label='test')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#標準化データの平均値と標準偏差を確認\n",
        "\n",
        "print(f'X_train mean: {X_train.iloc[:,0].mean(): .2f}')\n",
        "print(f'X_train std: {X_train.iloc[:,0].std(): .2f}')\n",
        "\n",
        "print(f'X_train_standardized mean: {X_train_standardized[:,0].mean(): .2f}')\n",
        "print(f'X_train_standardized std: {X_train_standardized[:,0].std(): .2f}')\n",
        "print('')\n",
        "print(f'X_val mean: {X_val.iloc[:,0].mean(): .2f}')\n",
        "print(f'X_val std: {X_val.iloc[:,0].std(): .2f}')\n",
        "\n",
        "print(f'X_val_standardized mean: {X_val_standardized[:,0].mean(): .2f}')\n",
        "print(f'X_val_standardized std: {X_val_standardized[:,0].std(): .2f}')\n",
        "\n",
        "print('')\n",
        "print(f'X_test mean: {X_test.iloc[:,0].mean(): .2f}')\n",
        "print(f'X_test std: {X_test.iloc[:,0].std(): .2f}')\n",
        "\n",
        "print(f'X_test_standardized mean: {X_test_standardized[:,0].mean(): .2f}')\n",
        "print(f'X_test_standardized std: {X_test_standardized[:,0].std(): .2f}')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 正規化\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# MinMaxScaler() 関数を使用して、norm_scaler という正規化を行うオブジェクトを作成\n",
        "norm_scaler = MinMaxScaler()\n",
        "\n",
        "# 訓練データに対して正規化を適用\n",
        "X_train_normalized = norm_scaler.fit_transform(X_train)\n",
        "\n",
        "# 検証データに対して正規化を適用（訓練データのスケーリングパラメータを使用）\n",
        "X_val_normalized = norm_scaler.transform(X_val)\n",
        "\n",
        "# テストデータに対して正規化を適用（訓練データのスケーリングパラメータを使用）\n",
        "X_test_normalized = norm_scaler.transform(X_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#正規化された X_train_noramlized, X_val_normalized, X_test_normalized の1列目を確認\n",
        "sns.histplot(X_train_normalized[:,0], label='train')\n",
        "sns.histplot(X_val_normalized[:,0], label='val')\n",
        "sns.histplot(X_test_normalized[:,0], label='test')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#正規化データの平均値と標準偏差を確認\n",
        "\n",
        "print(f'X_train mean: {X_train.iloc[:,0].mean(): .2f}')\n",
        "print(f'X_train std: {X_train.iloc[:,0].std(): .2f}')\n",
        "\n",
        "print(f'X_train_normalized mean: {X_train_normalized[:,0].mean(): .2f}')\n",
        "print(f'X_train_normalized std: {X_train_normalized[:,0].std(): .2f}')\n",
        "print('')\n",
        "print(f'X_val mean: {X_val.iloc[:,0].mean(): .2f}')\n",
        "print(f'X_val std: {X_val.iloc[:,0].std(): .2f}')\n",
        "\n",
        "print(f'X_val_normalized mean: {X_val_normalized[:,0].mean(): .2f}')\n",
        "print(f'X_val_normalized std: {X_val_normalized[:,0].std(): .2f}')\n",
        "\n",
        "print('')\n",
        "print(f'X_test mean: {X_test.iloc[:,0].mean(): .2f}')\n",
        "print(f'X_test std: {X_test.iloc[:,0].std(): .2f}')\n",
        "\n",
        "print(f'X_test_normalized mean: {X_test_normalized[:,0].mean(): .2f}')\n",
        "print(f'X_test_normalized std: {X_test_normalized[:,0].std(): .2f}')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.columns[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# このデータセットの各特徴量の分布を確認する\n",
        "\n",
        "# プロットの設定\n",
        "fig, axes = plt.subplots(6, 5, figsize=(15, 18))\n",
        "fig.suptitle('Histograms of X_train columns')\n",
        "\n",
        "# 30の特徴量のヒストグラムを描画\n",
        "for i in range(30):\n",
        "    row = i // 5  #行のインデックス (0,1,2,3,4,5) は5で割った時の商\n",
        "    col = i % 5   #列は5で割った時のあまり\n",
        "    ax = axes[row, col]\n",
        "    \n",
        "    sns.histplot(X_train.iloc[:, i], ax=ax, kde=True)\n",
        "    ax.set_title(df.columns[i])\n",
        "    ax.set_xlabel('')\n",
        "\n",
        "# レイアウトの調整\n",
        "plt.tight_layout()\n",
        "plt.subplots_adjust(top=0.95)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- 上記、ロジスティック回帰では、標準化を使用したほうがよいという原則があるが、データが正規分布ではないので、以下の解析では正規化データ (データ範囲が0-1) を使用する"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 4.6. モデルの選択\n",
        "\n",
        "- 教師あり学習でよく使われるモデルとその特徴\n",
        "\n",
        "| モデル名 | 特徴 | 主な用途 |\n",
        "| --- | --- | --- |\n",
        "| ロジスティック回帰 (Logistic Regression) | 線形モデル。二項分類問題に適している。解釈しやすい。 | 二項分類（例：スパム検出、病気の有無の判定） |\n",
        "| 線形回帰 (Linear Regression) | 連続値の予測に使用。特徴量とターゲットの関係を線形で表現。 | 連続値の予測（例：住宅価格予測、売上予測） |\n",
        "| 決定木 (Decision Tree) | 直感的で解釈しやすい。過学習しやすいが、剪定によって改善可能。 | 分類・回帰（例：顧客分類、売上予測） |\n",
        "| ランダムフォレスト (Random Forest) | 多数の決定木を組み合わせて性能を向上。過学習を抑制。 | 分類・回帰（例：顧客分類、売上予測） |\n",
        "| サポートベクターマシン (Support Vector Machine, SVM) | 高次元空間でも有効。カーネルを使用して非線形問題も解決可能。 | 分類（例：画像分類、テキスト分類） |\n",
        "| k近傍法 (k-Nearest Neighbors, k-NN) | シンプルで直感的。計算コストが高いが、効果的。 | 分類・回帰（例：パターン認識、推奨システム） |\n",
        "| ナイーブベイズ (Naive Bayes) | 確率モデル。特にテキスト分類に有効。計算が高速。 | 分類（例：スパムフィルタリング、感情分析） |\n",
        "| ニューラルネットワーク (Neural Network) | 複雑な関係を学習可能。深層学習を含む。大量のデータに適している。 | 分類・回帰（例：画像認識、自然言語処理） |\n",
        "| 勾配ブースティング (Gradient Boosting) | 多数の弱い学習器を組み合わせて性能を向上。過学習しにくい。 | 分類・回帰（例：競技プログラミング、金融予測） |\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- 参考: 各モデルをscikit-learnからインポートする時のコマンドの一覧\n",
        "\n",
        "| モデル名 | インポートコマンド |\n",
        "| --- | --- |\n",
        "| ロジスティック回帰 (Logistic Regression) | `from sklearn.linear_model import LogisticRegression` |\n",
        "| 線形回帰 (Linear Regression) | `from sklearn.linear_model import LinearRegression` |\n",
        "| 決定木 (Decision Tree) | `from sklearn.tree import DecisionTreeClassifier` （分類の場合）<br>`from sklearn.tree import DecisionTreeRegressor` （回帰の場合） |\n",
        "| ランダムフォレスト (Random Forest) | `from sklearn.ensemble import RandomForestClassifier` （分類の場合）<br>`from sklearn.ensemble import RandomForestRegressor` （回帰の場合） |\n",
        "| サポートベクターマシン (Support Vector Machine, SVM) | `from sklearn.svm import SVC` （分類の場合）<br>`from sklearn.svm import SVR` （回帰の場合） |\n",
        "| k近傍法 (k-Nearest Neighbors, k-NN) | `from sklearn.neighbors import KNeighborsClassifier` （分類の場合）<br>`from sklearn.neighbors import KNeighborsRegressor` （回帰の場合） |\n",
        "| ナイーブベイズ (Naive Bayes) | `from sklearn.naive_bayes import GaussianNB` |\n",
        "| ニューラルネットワーク (Neural Network) | `from sklearn.neural_network import MLPClassifier` （分類の場合）<br>`from sklearn.neural_network import MLPRegressor` （回帰の場合） |\n",
        "| 勾配ブースティング (Gradient Boosting) | `from sklearn.ensemble import GradientBoostingClassifier` （分類の場合）<br>`from sklearn.ensemble import GradientBoostingRegressor` （回帰の場合） |\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 4.7. モデルの訓練\n",
        "- LogisticRegression() 関数から、 model という推定器となるオブジェクトを生成する\n",
        "- modelのメソッド fit() 関数を用いて、訓練データと正解ラベルを指定することで、学習が行われる"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ロジスティック回帰モデルを用いて分類する\n",
        "\n",
        "# LogisticRegression をインポートする\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# ロジスティック回帰モデルの推定器 model を作成\n",
        "model = LogisticRegression()\n",
        "\n",
        "# モデルの訓練\n",
        "model.fit(X_train_standardized, y_train)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 4.8 モデルの検証\n",
        "- model のメソッド predict() に 検証データを入れ、どのような結果を出すかを見る\n",
        "- この時、以下を計算する\n",
        "    - 混同行列 Confusion matrix\n",
        "\n",
        "|     | 予測: Positive | 予測: Negative |\n",
        "| --- | --- | --- |\n",
        "| 実際: Positive | True Positive (TP) | False Negative (FN) |\n",
        "| 実際: Negative | False Positive (FP) | True Negative (TN) |\n",
        "\n",
        "- **True Positive (TP)**: 悪性を悪性と予測した数\n",
        "- **True Negative (TN)**: 良性を良性と予測した数\n",
        "- **False Positive (FP)**: 良性を悪性と予測した数（偽陽性）\n",
        "- **False Negative (FN)**: 悪性を良性と予測した数（偽陰性）\n",
        "- 正解率 Accuracy        \n",
        "    - Accuracy = 正しく予測されたサンプルの数 / 全サンプル数\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 検証データをモデルに代入し、予測ラベルを計算する\n",
        "y_val_pred = model.predict(X_val_standardized)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 検証データに対する評価結果を表示\n",
        "# confusion matrix を計算する\n",
        "# scikit-learn の metrics モジュールから confusion_matrix() 関数をインポートする\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# 検証データの正解ラベル y_val と 予測ラベル y_val_pred から混同行列を作成する\n",
        "val_conf_matrix = confusion_matrix(y_val, y_val_pred)\n",
        "\n",
        "# 混同行列を可視化する\n",
        "sns.heatmap(val_conf_matrix, annot=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 検証データに対する評価結果を表示\n",
        "# 正確度 accuracy を見る\n",
        "# scikit-learn の metrics モジュールから accuracy_score() 関数をインポートする\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
        "print(f'Validation Accuracy: {val_accuracy: .2f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 分類のレポートを見る\n",
        "# scikit-learn の metrics モジュールから classification_report() 関数をインポートする\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "val_class_report = classification_report(y_val, y_val_pred)\n",
        "\n",
        "print('Validation Classification Report')\n",
        "print(val_class_report)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 4.9 テストデータを使ったデータの予測\n",
        "- 本来は、データの一部を分割し、検証データとし、その後、全く違うデータセットをテストデータとして使用し、モデルの汎化性能を見る\n",
        "- model のメソッド predict() に テストデータを入れ、予測ラベルを計算する"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# テストデータをモデルに代入し、予測ラベルを計算する\n",
        "y_test_pred = model.predict(X_test_standardized)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 4.10 テストデータを使ったモデルの評価\n",
        "- 検証データと同様、混同行列を作成し、正解率を計算する"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# テストデータの正解ラベル y_test と 予測ラベル y_test_pred から混同行列を作成する\n",
        "test_conf_matrix = confusion_matrix(y_test, y_test_pred)\n",
        "\n",
        "# 混同行列を可視化する\n",
        "sns.heatmap(test_conf_matrix, annot=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# テストデータに対する評価結果を表示\n",
        "# 正確度 accuracy を見る\n",
        "\n",
        "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "print(f'Test Accuracy: {test_accuracy: .2f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 分類のレポートを見る\n",
        "\n",
        "test_class_report = classification_report(y_test, y_test_pred)\n",
        "\n",
        "print('Test Classification Report')\n",
        "print(test_class_report)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5. 異なるモデルを選択する\n",
        "- scikit-learn では、モデルを選択する1行を変更するだけで、異なる方法での機械学習を行うことができる"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 上記で行ったことをまとめて、関数とする\n",
        "# 練習問題でモデルの違いを簡単に検討できるようにするため\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
        "\n",
        "# グローバル変数として定義\n",
        "X_train, X_val, X_test = None, None, None\n",
        "y_train, y_val, y_test = None, None, None\n",
        "X_train_preproc, X_val_preproc, X_test_preproc = None, None, None\n",
        "y_train_preproc, y_val_preproc, y_test_preproc = None, None, None\n",
        "\n",
        "# 前処理の関数: 読み込みと分割\n",
        "def breast_cancer_preproc_load():\n",
        "    global X_train, X_val, X_test, y_train, y_val, y_test\n",
        "\n",
        "    # データセットの読み込み\n",
        "    dataset = load_breast_cancer()\n",
        "\n",
        "    # データをPandas の DataFrameに変換\n",
        "    df = pd.DataFrame(dataset.data, columns=dataset.feature_names)\n",
        "    df['target'] = dataset.target\n",
        "\n",
        "    # データフレーム df を 特徴量 X とターゲット(正解ラベル) y に分ける\n",
        "    X = df.drop(columns=['target'])\n",
        "    y = df['target']\n",
        "\n",
        "    # データを訓練用, 検証用, テスト用に分割する\n",
        "    # 訓練データを70%, 残りのデータを30%として分割化し、\n",
        "    # 残りのデータをさらに 半分ずつ 検証用 (val) と テスト用 (test) に分割する\n",
        "    # つまり検証用 15%, テスト用 15%\n",
        "\n",
        "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)    \n",
        "\n",
        "\n",
        "# 前処理の関数: 標準化\n",
        "def breast_cancer_preproc_standardized():\n",
        "    global X_train_preproc, X_val_preproc, X_test_preproc\n",
        "    global y_train_preproc, y_val_preproc, y_test_preproc\n",
        "\n",
        "    # StandardScaler() 関数を使用して、standard_scalerという標準化を行うオブジェクトを作成\n",
        "    std_scaler = StandardScaler()\n",
        "\n",
        "    # 訓練データに対して標準化を適用\n",
        "    # fit_transform は、標準化するパラメータを計算(fit)して、それをあてはめる(transform)\n",
        "    X_train_preproc = std_scaler.fit_transform(X_train)\n",
        "\n",
        "    # 検証用、テストデータに対しても標準化を適用\n",
        "    # 訓練データで求めた標準化パラメータを用いて訓練データに適用\n",
        "    X_val_preproc = std_scaler.transform(X_val)\n",
        "    X_test_preproc = std_scaler.transform(X_test)\n",
        "\n",
        "\n",
        "# 前処理の関数: 正規化\n",
        "def breast_cancer_preproc_normalized():\n",
        "    global X_train_preproc, X_val_preproc, X_test_preproc\n",
        "    global y_train_preproc, y_val_preproc, y_test_preproc\n",
        "    \n",
        "    # MinMaxScaler() 関数を使用して、norm_scaler という正規化を行うオブジェクトを作成\n",
        "    norm_scaler = MinMaxScaler()\n",
        "\n",
        "    # 正規化を使用したい場合は、上の標準化をコメントアウトし、以下のコメントの冒頭の # を削除\n",
        "    # 訓練データに対して正規化を適用\n",
        "    X_train_preproc = norm_scaler.fit_transform(X_train)\n",
        "\n",
        "    # 検証データ、テストデータに対して正規化を適用（訓練データのスケーリングパラメータを使用）\n",
        "    X_val_preproc = norm_scaler.transform(X_val)\n",
        "    X_test_preproc = norm_scaler.transform(X_test)\n",
        "\n",
        "\n",
        "# 学習・検証・評価の関数\n",
        "def breast_cancer_evaluation():\n",
        "    #選択したモデルを用いて訓練データを用いて学習する\n",
        "    model.fit(X_train_preproc, y_train)\n",
        "\n",
        "    # 検証データをモデルに代入し、予測ラベルを計算する\n",
        "    y_val_pred = model.predict(X_val_preproc)\n",
        "    \n",
        "    # 検証データに対する評価結果を表示\n",
        "    # confusion matrix を計算する\n",
        "    # 検証データの正解ラベル y_val と 予測ラベル y_val_pred から混同行列を作成する\n",
        "    val_conf_matrix = confusion_matrix(y_val, y_val_pred)\n",
        "    print('Validation confusion matrix')\n",
        "    print(val_conf_matrix)\n",
        "\n",
        "    # 正確度 accuracy を見る\n",
        "    val_accuracy = accuracy_score(y_val, y_val_pred)\n",
        "    print(f'Validation Accuracy: {val_accuracy: .2f}')\n",
        "\n",
        "    # 分類のレポートを見る\n",
        "    val_class_report = classification_report(y_val, y_val_pred)\n",
        "    print('Validation Classification Report')\n",
        "    print(val_class_report)\n",
        "\n",
        "    # テストデータをモデルに代入し、予測ラベルを計算する\n",
        "    y_test_pred = model.predict(X_test_preproc)\n",
        "    \n",
        "    # テストデータの正解ラベル y_test と 予測ラベル y_test_pred から混同行列を作成する\n",
        "    test_conf_matrix = confusion_matrix(y_test, y_test_pred)\n",
        "    print('Test confusion matrix')\n",
        "    print(test_conf_matrix)\n",
        "\n",
        "    # テストデータに対する評価結果を表示\n",
        "    # 正確度 accuracy を見る\n",
        "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "    print(f'Test Accuracy: {test_accuracy: .2f}')\n",
        "\n",
        "    # 分類のレポートを見る\n",
        "    test_class_report = classification_report(y_test, y_test_pred)\n",
        "    print('Test Classification Report')\n",
        "    print(test_class_report)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 5.1 練習問題\n",
        "- 下のセルで異なるモデルを選んでみましょう\n",
        "- 前処理と評価はすでに上のセルで関数で定義してあるので心配いりません\n",
        "- 自分が使いたいと思うモデルのコメントを2行外してください\n",
        "- 他のモデルはすべてコメントアウト(行頭に # がついている)されていることを確認してください"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#前処理(1)読み込み: ここはいじらなくて大丈夫です\n",
        "breast_cancer_preproc_load()\n",
        "\n",
        "#前処理(2)標準化・正規化\n",
        "# どちらか選択してください\n",
        "#breast_cancer_preproc_standardized()\n",
        "breast_cancer_preproc_normalized()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "######## モデル一覧 ########\n",
        "### このセクションに自分が使いたいモデルのコメントを外してください\n",
        "### 初期状態では、決定木のモデルを使うようになっています\n",
        "\n",
        "#from sklearn.linear_model import LogisticRegression #ロジスティック回帰\n",
        "#model = LogisticRegression()\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier #決定木\n",
        "model = DecisionTreeClassifier()\n",
        "\n",
        "#from sklearn.ensemble import RandomForestClassifier #ランダムフォレスト\n",
        "#model = RandomForestClassifier()\n",
        "\n",
        "#from sklearn.svm import SVC #サポートベクターマシン\n",
        "#model = SVC()\n",
        "\n",
        "#from sklearn.neighbors import KNeighborsClassifier #k近傍法\n",
        "#model = KNeighborsClassifier()\n",
        "\n",
        "#from sklearn.naive_bayes import GaussianNB #ナイーブベイズ\n",
        "#model = GaussianNB()\n",
        "\n",
        "#from sklearn.neural_network import MLPClassifier #ニューラルネットワーク\n",
        "#model = MLPClassifier()\n",
        "\n",
        "#from sklearn.ensemble import GradientBoostingClassifier #勾配ブースティング\n",
        "#model = GradientBoostingClassifier()\n",
        "\n",
        "######## モデル一覧 ここまで ########\n",
        "\n",
        "\n",
        "####ここから下はいじらなくて大丈夫です。\n",
        "#評価\n",
        "breast_cancer_evaluation()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
