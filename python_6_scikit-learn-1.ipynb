{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a href=\"https://colab.research.google.com/github/kytk/AI-MAILs/blob/main/python_4_scikit-learn-1.ipynb?hl=ja\" target=\"_blank\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHVIs_ypyjg0"
      },
      "source": [
        "\n",
        "## 医療従事者のためのPython: 機械学習\n",
        "\n",
        "根本清貴 (筑波大学医学医療系精神医学)\n",
        "\n",
        "Ver.20240719\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "### 1. 機械学習とは\n",
        "- 機械学習の定義\n",
        "    - 「データからルールやパターンを導き出し、予測や意思決定を行う技術」\n",
        "    - (従来: 人間がルールを決める)\n",
        "- 機械学習の種類\n",
        "    - 教師あり学習\n",
        "        - 入力データとその対応する正解(ラベル)がペアになったデータセットを用いてモデルを訓練\n",
        "        - 新しいデータが入ってきた時にそのモデルから正しいラベルを予測\n",
        "        - 例: 画像の分類(犬と猫の画像を分類)、スパムメールの分類、価格予測\n",
        "    - 教師なし学習\n",
        "        - ラベルのないデータを用いてモデルを訓練\n",
        "        - データの内部構造やパターンを見つけ出す\n",
        "        - 例: クラスタリング(似たデータをグループに分ける)、次元削減(データの特徴を少数の重要な特徴に圧縮する)\n",
        "    - 強化学習\n",
        "        - エージェント(学習者)が環境と相互作用しながら学習する。エージェントは行動を選択し、その結果として得られる報酬を基に次の行動を改善する\n",
        "        - 長期的な累積報酬を最大化することが目標\n",
        "        - 例: ゲーム、ロボット制御、自動運転\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### 2. 機械学習のステップ\n",
        "\n",
        "1. データの収集 (Data Collection):\n",
        "    - 必要なデータを収集する\n",
        "    - 例：センサーデータ、ユーザー行動データ、画像データなど\n",
        "\n",
        "2. データの確認とクレンジング (Data Inspection and Cleaning):\n",
        "    - データの品質を確認し、不足している値やノイズを処理する\n",
        "    - 例：欠損値の補完、異常値の除去など\n",
        "\n",
        "3. **データの分割 (Data Splitting)**:\n",
        "    - データセットを訓練データ、検証データ、テストデータに分ける\n",
        "    - 例：訓練データ 70%、検証データ 15%、テストデータ 15%\n",
        "\n",
        "4. **データの前処理 (Data Preprocessing)**:\n",
        "    - データをモデルに適した形式に変換する\n",
        "    - 例：標準化、正規化、カテゴリ変数のエンコーディングなど\n",
        "\n",
        "5. 特徴量エンジニアリング (Feature Engineering):\n",
        "    - モデルの性能を向上させるために、新しい特徴量を作成する\n",
        "    - 例：日付から曜日や月を抽出する、テキストデータから単語の出現頻度を計算するなど\n",
        "\n",
        "6. **モデルの選択 (Model Selection)**:\n",
        "    - 解決する問題に最適なモデルを選ぶ\n",
        "    - 例：回帰モデル、決定木、ニューラルネットワークなど\n",
        "\n",
        "7. **モデルの学習 (Model Training)**:\n",
        "    - 訓練データを用いてモデルを訓練する\n",
        "    - 例：パラメータの最適化、ハイパーパラメータチューニングなど\n",
        "\n",
        "8. **モデルの検証 (Model Validation)**:\n",
        "    - 検証データを使ってモデルの性能を評価する\n",
        "    - 例：クロスバリデーション、精度、再現率、F1スコアなどの評価指標を計算する\n",
        "\n",
        "9. **データの予測 (Data Prediction)**:\n",
        "    - テストデータを使って予測を行う\n",
        "    - 例：未知のデータに対する予測結果を得る\n",
        "\n",
        "10. **モデルの評価 (Model Evaluation)**:\n",
        "    - テストデータを用いてモデルの最終的な性能を評価する\n",
        "    - 例：精度、再現率、F1スコア、混同行列などの評価指標を計算する\n",
        "\n",
        "11. モデルの改善 (Model Improvement):\n",
        "    - 必要に応じてモデルを改善し、再訓練する\n",
        "    - 例：特徴量の追加や削除、別のモデルの試行など\n",
        "\n",
        "12. モデルの展開 (Model Deployment):\n",
        "    - モデルを実際の環境に展開し、運用する\n",
        "    - 例：Webサービスとしてデプロイ、組み込みシステムへの実装など\n",
        "\n",
        "13. モデルの監視とメンテナンス (Model Monitoring and Maintenance):\n",
        "    - 展開後のモデルの性能を監視し、必要に応じて更新や再訓練を行う\n",
        "    - 例：モデルのドリフト検出、定期的な再訓練など\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Pythonでの機械学習\n",
        "- Pythonには、scikit-learn (サイキット ラーン)という機械学習に特化したパッケージがある\n",
        "- scikit-learn の中に医療のサンプルデータとして、糖尿病データセット(Pandasで使用)と乳がんデータセットが提供されている\n",
        "- 今回は乳がんデータセットを使って教師あり学習を体験する"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4. 乳がんデータセット\n",
        "- データセットの名称: Breast Cancer Wisconsin (Diagnostic) dataset\n",
        "- サンプル数: 569\n",
        "- 特徴量の数: 30\n",
        "- ターゲットの種類: 2クラス（良性と悪性）\n",
        "- 特徴量の種類: 実数値\n",
        "\n",
        "#### 4.1. データセットの30の特徴量\n",
        "- 細胞診における細胞核の特徴\n",
        "\n",
        "| 英語 | 日本語 |\n",
        "| --- | --- |\n",
        "| mean radius | 平均半径 |\n",
        "| mean texture | 平均テクスチャ |\n",
        "| mean perimeter | 平均周囲長 |\n",
        "| mean area | 平均面積 |\n",
        "| mean smoothness | 平均平滑度 |\n",
        "| mean compactness | 平均コンパクト度 |\n",
        "| mean concavity | 平均陥凹度 |\n",
        "| mean concave points | 平均陥凹点数 |\n",
        "| mean symmetry | 平均対称性 |\n",
        "| mean fractal dimension | 平均フラクタル次元 |\n",
        "| radius error | 半径誤差 |\n",
        "| texture error | テクスチャ誤差 |\n",
        "| perimeter error | 周囲長誤差 |\n",
        "| area error | 面積誤差 |\n",
        "| smoothness error | 平滑度誤差 |\n",
        "| compactness error | コンパクト度誤差 |\n",
        "| concavity error | 陥凹度誤差 |\n",
        "| concave points error | 陥凹点数誤差 |\n",
        "| symmetry error | 対称性誤差 |\n",
        "| fractal dimension error | フラクタル次元誤差 |\n",
        "| worst radius | 最悪の半径 |\n",
        "| worst texture | 最悪のテクスチャ |\n",
        "| worst perimeter | 最悪の周囲長 |\n",
        "| worst area | 最悪の面積 |\n",
        "| worst smoothness | 最悪の平滑度 |\n",
        "| worst compactness | 最悪のコンパクト度 |\n",
        "| worst concavity | 最悪の陥凹度 |\n",
        "| worst concave points | 最悪の陥凹点数 |\n",
        "| worst symmetry | 最悪の対称性 |\n",
        "| worst fractal dimension | 最悪のフラクタル次元 |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 4.2. 教師あり学習: 乳がんデータセットの読み込みと確認\n",
        "- scikit-learn ライブラリを使用してデータセットを読み込み、データを確認する"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "import pandas as pd\n",
        "\n",
        "# データセットの読み込み\n",
        "dataset = load_breast_cancer()\n",
        "\n",
        "# データセットの基本情報を表示\n",
        "print(dataset.DESCR)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# データをPandas の DataFrameに変換\n",
        "df = pd.DataFrame(dataset.data, columns=dataset.feature_names)\n",
        "df['target'] = dataset.target\n",
        "\n",
        "# データの先頭30行を表示\n",
        "df.head(30)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 正解ラベル target の 0 と 1 の数を数える\n",
        "# target が 0 であるかどうかを判断\n",
        "df['target'] == 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# (df['target'] == 0).sum() とすると 0 がTrue の数を調べられる\n",
        "# 悪性が212, 良性が357 より、0 は悪性、1 は良性\n",
        "num0 = (df['target'] == 0).sum()\n",
        "num1 = (df['target'] == 1).sum()\n",
        "\n",
        "print(f'number of 0 = {num0}')\n",
        "print(f'number of 1 = {num1}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- 欠損値の確認\n",
        "    - 乳がんデータセットはサンプルデータなので欠損値はないが、実際のデータを扱う際には欠損値の確認が重要\n",
        "    - 欠損値がどこにあるかを調べるためには、 `df.isnull().sum()` を使う\n",
        "    - このメソッドは、各列に含まれる欠損値の数を表示する"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 欠損値の確認\n",
        "missing_values = df.isnull().sum()\n",
        "print(missing_values)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 参考\n",
        "# df を Excelファイルで出力しておくと、自分が機械学習を実際に行う時の参考になる\n",
        "df.to_excel('breast_cancer.xlsx', header=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 4.3. データの分割 (Data Splitting)\n",
        "\n",
        "- データセットを訓練用データ（training data）とテスト用データ（test data）に分割することで、モデルの訓練と評価を行う\n",
        "- モデルのハイパーパラメータ調整や性能評価を行うために、検証用データ（validation data）を使用することもある\n",
        "\n",
        "    1. 訓練データ (Training Data):\n",
        "        - モデルの訓練に使用する。モデルがこのデータに基づいて学習する\n",
        "    2. 検証データ (Validation Data):\n",
        "        - モデルのハイパーパラメータ調整や性能評価に使用する\n",
        "        - 通常、訓練データの一部を検証データとして使用する\n",
        "    3. テストデータ (Test Data):\n",
        "        - モデルの最終的な評価に使用する\n",
        "        - このデータは、訓練や検証には使用しない\n",
        "\n",
        "- 一般的な分割比率\n",
        "    - 訓練データ: 70% - 80%\n",
        "    - 検証データ: 10% - 15%\n",
        "    - テストデータ: 10% - 15%\n",
        "\n",
        "- scikit-learn では、 `train_test_split` という関数を用いて、データを分割できる\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# train_test_split をインポートする\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# データフレーム df を 特徴量 X とターゲット(正解ラベル) y に分ける\n",
        "X = df.drop(columns=['target'])\n",
        "y = df['target']\n",
        "\n",
        "# データを訓練用, 検証用, テスト用に分割する\n",
        "# test_size=0.4 は、データの30%をテストデータに割り当てることを意味する\n",
        "# random_state=42 は、分割の再現性を保つためのシード値 任意の値でよい\n",
        "# 今、訓練データを70%, 残りのデータを30%として分割化し、\n",
        "# 残りのデータをさらに 半分ずつ 検証用 (val) と テスト用 (test) に分割する\n",
        "# つまり検証用 15%, テスト用 15%\n",
        "\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "# 分割後のデータサイズを表示\n",
        "print(f'訓練データのサイズ: {X_train.shape}')\n",
        "print(f'検証データのサイズ: {X_val.shape}')\n",
        "print(f'テストデータのサイズ: {X_test.shape}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 4.4. データの標準化と正規化\n",
        "- 標準化 (Standardization)\n",
        "    - データの平均を0、標準偏差を1に変換することで、異なるスケールの特徴量を同じスケールに揃える方法\n",
        "    - これにより、モデルが特徴量のスケールに影響されずに学習できるようになる\n",
        "    - scikit-learn では、 `StandardScalar` という関数を用いる\n",
        "- 正規化 (Normalization)\n",
        "    - データを特定の範囲 (通常0から1) にスケーリングする\n",
        "    - 特徴量の範囲が大きく異なる場合に有効\n",
        "    - scikit-learn では、 `MinMaxScaler` という関数を用いることが多い\n",
        "- なお、標準化と正規化は、データを分割した後に行う。そのことで、テストデータに関する情報が訓練データに漏れる (リーク) を防止する\n",
        "- 標準化と正規化は必須ではない。自分のデータにあわせて考慮する\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 標準化\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# StandardScaler() 関数を用いて、std_scaler という標準化するオブジェクトを作成\n",
        "std_scaler = StandardScaler()\n",
        "\n",
        "# 訓練データに対して標準化を適用\n",
        "# fit_transform は、標準化するパラメータを計算(fit)して、それをあてはめる(transform)\n",
        "X_train_standardized = std_scaler.fit_transform(X_train)\n",
        "\n",
        "# 検証用、テストデータに対しても標準化を適用\n",
        "# 訓練データで求めた標準化パラメータを用いて訓練データに適用\n",
        "X_val_standardized = std_scaler.transform(X_val)\n",
        "X_test_standardized = std_scaler.transform(X_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# X_train, X_val, X_test の元データの1列目の分布を確認\n",
        "# X_train, X_val, X_test は Pandas DataFrame 形式なので、X_train.iloc[:,0] で取り出す\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.histplot(X_train.iloc[:,0], label='train')\n",
        "sns.histplot(X_val.iloc[:,0], label='val')\n",
        "sns.histplot(X_test.iloc[:,0], label='test')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 標準化された X_train_standardized, X_val_standardized, X_test_standardized の1列目の分布を確認\n",
        "# X_train_standardized, X_val_standardized, X_test_standardized は numpy 形式なので、\n",
        "# X_train_standardized[:,0] で取り出す\n",
        "# ヒストグラムは同じでも、X軸の値が変わっていることに注意\n",
        "sns.histplot(X_train_standardized[:,0], label='train')\n",
        "sns.histplot(X_val_standardized[:,0], label='val')\n",
        "sns.histplot(X_test_standardized[:,0], label='test')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#平均値と標準偏差を確認\n",
        "\n",
        "print(f'X_train mean: {X_train.iloc[:,0].mean(): .2f}')\n",
        "print(f'X_train std: {X_train.iloc[:,0].std(): .2f}')\n",
        "\n",
        "print(f'X_train_standardized mean: {X_train_standardized[:,0].mean(): .2f}')\n",
        "print(f'X_train_standardized std: {X_train_standardized[:,0].std(): .2f}')\n",
        "print('')\n",
        "print(f'X_val mean: {X_val.iloc[:,0].mean(): .2f}')\n",
        "print(f'X_val std: {X_val.iloc[:,0].std(): .2f}')\n",
        "\n",
        "print(f'X_val_standardized mean: {X_val_standardized[:,0].mean(): .2f}')\n",
        "print(f'X_val_standardized std: {X_val_standardized[:,0].std(): .2f}')\n",
        "\n",
        "print('')\n",
        "print(f'X_test mean: {X_test.iloc[:,0].mean(): .2f}')\n",
        "print(f'X_test std: {X_test.iloc[:,0].std(): .2f}')\n",
        "\n",
        "print(f'X_test_standardized mean: {X_test_standardized[:,0].mean(): .2f}')\n",
        "print(f'X_test_standardized std: {X_test_standardized[:,0].std(): .2f}')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 正規化\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# MinMaxScaler() 関数を使用して、norm_scaler という正規化を行うオブジェクトを作成\n",
        "norm_scaler = MinMaxScaler()\n",
        "\n",
        "# 訓練データに対して正規化を適用\n",
        "X_train_normalized = norm_scaler.fit_transform(X_train_standardized)\n",
        "\n",
        "# 検証データに対して正規化を適用（訓練データのスケーリングパラメータを使用）\n",
        "X_val_normalized = norm_scaler.transform(X_val_standardized)\n",
        "\n",
        "# テストデータに対して正規化を適用（訓練データのスケーリングパラメータを使用）\n",
        "X_test_normalized = norm_scaler.transform(X_test_standardized)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#正規化された X_train_noramlized, X_val_normalized, X_test_normalized の1列目を確認\n",
        "sns.histplot(X_train_normalized[:,0], label='train')\n",
        "sns.histplot(X_val_normalized[:,0], label='val')\n",
        "sns.histplot(X_test_normalized[:,0], label='test')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 4.6. モデルの選択\n",
        "\n",
        "- 教師あり学習でよく使われるモデルとその特徴\n",
        "\n",
        "| モデル名 | 特徴 | 主な用途 |\n",
        "| --- | --- | --- |\n",
        "| ロジスティック回帰 (Logistic Regression) | 線形モデル。二項分類問題に適している。解釈しやすい。 | 二項分類（例：スパム検出、病気の有無の判定） |\n",
        "| 線形回帰 (Linear Regression) | 連続値の予測に使用。特徴量とターゲットの関係を線形で表現。 | 連続値の予測（例：住宅価格予測、売上予測） |\n",
        "| 決定木 (Decision Tree) | 直感的で解釈しやすい。過学習しやすいが、剪定によって改善可能。 | 分類・回帰（例：顧客分類、売上予測） |\n",
        "| ランダムフォレスト (Random Forest) | 多数の決定木を組み合わせて性能を向上。過学習を抑制。 | 分類・回帰（例：顧客分類、売上予測） |\n",
        "| サポートベクターマシン (Support Vector Machine, SVM) | 高次元空間でも有効。カーネルを使用して非線形問題も解決可能。 | 分類（例：画像分類、テキスト分類） |\n",
        "| k近傍法 (k-Nearest Neighbors, k-NN) | シンプルで直感的。計算コストが高いが、効果的。 | 分類・回帰（例：パターン認識、推奨システム） |\n",
        "| ナイーブベイズ (Naive Bayes) | 確率モデル。特にテキスト分類に有効。計算が高速。 | 分類（例：スパムフィルタリング、感情分析） |\n",
        "| ニューラルネットワーク (Neural Network) | 複雑な関係を学習可能。深層学習を含む。大量のデータに適している。 | 分類・回帰（例：画像認識、自然言語処理） |\n",
        "| 勾配ブースティング (Gradient Boosting) | 多数の弱い学習器を組み合わせて性能を向上。過学習しにくい。 | 分類・回帰（例：競技プログラミング、金融予測） |\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- 参考: 各モデルをscikit-learnからインポートする時のコマンドの一覧\n",
        "\n",
        "| モデル名 | インポートコマンド |\n",
        "| --- | --- |\n",
        "| ロジスティック回帰 (Logistic Regression) | `from sklearn.linear_model import LogisticRegression` |\n",
        "| 線形回帰 (Linear Regression) | `from sklearn.linear_model import LinearRegression` |\n",
        "| 決定木 (Decision Tree) | `from sklearn.tree import DecisionTreeClassifier` （分類の場合）<br>`from sklearn.tree import DecisionTreeRegressor` （回帰の場合） |\n",
        "| ランダムフォレスト (Random Forest) | `from sklearn.ensemble import RandomForestClassifier` （分類の場合）<br>`from sklearn.ensemble import RandomForestRegressor` （回帰の場合） |\n",
        "| サポートベクターマシン (Support Vector Machine, SVM) | `from sklearn.svm import SVC` （分類の場合）<br>`from sklearn.svm import SVR` （回帰の場合） |\n",
        "| k近傍法 (k-Nearest Neighbors, k-NN) | `from sklearn.neighbors import KNeighborsClassifier` （分類の場合）<br>`from sklearn.neighbors import KNeighborsRegressor` （回帰の場合） |\n",
        "| ナイーブベイズ (Naive Bayes) | `from sklearn.naive_bayes import GaussianNB` |\n",
        "| ニューラルネットワーク (Neural Network) | `from sklearn.neural_network import MLPClassifier` （分類の場合）<br>`from sklearn.neural_network import MLPRegressor` （回帰の場合） |\n",
        "| 勾配ブースティング (Gradient Boosting) | `from sklearn.ensemble import GradientBoostingClassifier` （分類の場合）<br>`from sklearn.ensemble import GradientBoostingRegressor` （回帰の場合） |\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 4.7. モデルの訓練\n",
        "- LogisticRegression() 関数から、 model という推定器となるオブジェクトを生成する\n",
        "- modelのメソッド fit() 関数を用いて、訓練データと正解ラベルを指定することで、学習が行われる"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ロジスティック回帰モデルを用いて分類する\n",
        "\n",
        "# LogisticRegression をインポートする\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# ロジスティック回帰モデルの推定器 model を作成\n",
        "model = LogisticRegression()\n",
        "\n",
        "# モデルの訓練\n",
        "model.fit(X_train_normalized, y_train)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 4.8 モデルの検証\n",
        "- model のメソッド predict() に 検証データを入れ、どのような結果を出すかを見る\n",
        "- この時、以下を計算する\n",
        "    - 混同行列 Confusion matrix\n",
        "\n",
        "|     | 予測: Positive | 予測: Negative |\n",
        "| --- | --- | --- |\n",
        "| 実際: Positive | True Positive (TP) | False Negative (FN) |\n",
        "| 実際: Negative | False Positive (FP) | True Negative (TN) |\n",
        "\n",
        "- **True Positive (TP)**: 悪性を悪性と予測した数\n",
        "- **True Negative (TN)**: 良性を良性と予測した数\n",
        "- **False Positive (FP)**: 良性を悪性と予測した数（偽陽性）\n",
        "- **False Negative (FN)**: 悪性を良性と予測した数（偽陰性）\n",
        "- 正解率 Accuracy        \n",
        "    - Accuracy = 正しく予測されたサンプルの数 / 全サンプル数\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 検証データをモデルに代入し、予測ラベルを計算する\n",
        "y_val_pred = model.predict(X_val_normalized)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 検証データに対する評価結果を表示\n",
        "# confusion matrix を計算する\n",
        "# scikit-learn の metrics モジュールから confusion_matrix() 関数をインポートする\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# 検証データの正解ラベル y_val と 予測ラベル y_val_pred から混同行列を作成する\n",
        "val_conf_matrix = confusion_matrix(y_val, y_val_pred)\n",
        "\n",
        "# 混同行列を可視化する\n",
        "sns.heatmap(val_conf_matrix, annot=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 検証データに対する評価結果を表示\n",
        "# 正確度 accuracy を見る\n",
        "# scikit-learn の metrics モジュールから accuracy_score() 関数をインポートする\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
        "print(f'Validation Accuracy: {val_accuracy: .2f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 分類のレポートを見る\n",
        "# scikit-learn の metrics モジュールから classification_report() 関数をインポートする\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "val_class_report = classification_report(y_val, y_val_pred)\n",
        "\n",
        "print('Validation Classification Report')\n",
        "print(val_class_report)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 4.9 テストデータを使ったデータの予測\n",
        "- 本来は、データの一部を分割し、検証データとし、その後、全く違うデータセットをテストデータとして使用し、モデルの汎化性能を見る\n",
        "- model のメソッド predict() に テストデータを入れ、予測ラベルを計算する"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# テストデータをモデルに代入し、予測ラベルを計算する\n",
        "y_test_pred = model.predict(X_test_normalized)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 4.10 テストデータを使ったモデルの評価\n",
        "- 検証データと同様、混同行列を作成し、正解率を計算する"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# テストデータの正解ラベル y_test と 予測ラベル y_test_pred から混同行列を作成する\n",
        "test_conf_matrix = confusion_matrix(y_test, y_test_pred)\n",
        "\n",
        "# 混同行列を可視化する\n",
        "sns.heatmap(test_conf_matrix, annot=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# テストデータに対する評価結果を表示\n",
        "# 正確度 accuracy を見る\n",
        "\n",
        "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "print(f'Test Accuracy: {test_accuracy: .2f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 分類のレポートを見る\n",
        "\n",
        "test_class_report = classification_report(y_test, y_test_pred)\n",
        "\n",
        "print('Test Classification Report')\n",
        "print(test_class_report)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5. 異なるモデルを選択する\n",
        "- scikit-learn では、モデルを選択する1行を変更するだけで、異なる方法での機械学習を行うことができる"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 上記で行ったことをまとめて、関数とする\n",
        "# 練習問題でモデルの違いを簡単に検討できるようにするため\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
        "\n",
        "\n",
        "# 前処理の関数\n",
        "def breast_cancer_preprocessing():\n",
        "    # データセットの読み込み\n",
        "    dataset = load_breast_cancer()\n",
        "\n",
        "    # データをPandas の DataFrameに変換\n",
        "    df = pd.DataFrame(dataset.data, columns=dataset.feature_names)\n",
        "    df['target'] = dataset.target\n",
        "\n",
        "    # データフレーム df を 特徴量 X とターゲット(正解ラベル) y に分ける\n",
        "    X = df.drop(columns=['target'])\n",
        "    y = df['target']\n",
        "\n",
        "    # データを訓練用, 検証用, テスト用に分割する\n",
        "    # 訓練データを70%, 残りのデータを30%として分割化し、\n",
        "    # 残りのデータをさらに 半分ずつ 検証用 (val) と テスト用 (test) に分割する\n",
        "    # つまり検証用 15%, テスト用 15%\n",
        "\n",
        "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)    \n",
        "\n",
        "    # StandardScaler() 関数を使用して、standard_scalerという標準化を行うオブジェクトを作成\n",
        "    std_scaler = StandardScaler()\n",
        "\n",
        "    # 訓練データに対して標準化を適用\n",
        "    # fit_transform は、標準化するパラメータを計算(fit)して、それをあてはめる(transform)\n",
        "    X_train_standardized = std_scaler.fit_transform(X_train)\n",
        "\n",
        "    # 検証用、テストデータに対しても標準化を適用\n",
        "    # 訓練データで求めた標準化パラメータを用いて訓練データに適用\n",
        "    X_val_standardized = std_scaler.transform(X_val)\n",
        "    X_test_standardized = std_scaler.transform(X_test)\n",
        "\n",
        "    # MinMaxScaler() 関数を使用して、norm_scaler という正規化を行うオブジェクトを作成\n",
        "    norm_scaler = MinMaxScaler()\n",
        "\n",
        "    # 訓練データに対して正規化を適用\n",
        "    X_train_normalized = norm_scaler.fit_transform(X_train_standardized)\n",
        "\n",
        "    # 検証データ、テストデータに対して正規化を適用（訓練データのスケーリングパラメータを使用）\n",
        "    X_val_normalized = norm_scaler.transform(X_val_standardized)\n",
        "    X_test_normalized = norm_scaler.transform(X_test_standardized)\n",
        "\n",
        "\n",
        "# 学習・検証・評価の関数\n",
        "def breast_cancer_evaluation():\n",
        "    #選択したモデルを用いて訓練データを用いて学習する\n",
        "    model.fit(X_train_normalized, y_train)\n",
        "\n",
        "    # 検証データをモデルに代入し、予測ラベルを計算する\n",
        "    y_val_pred = model.predict(X_val_normalized)\n",
        "    \n",
        "    # 検証データに対する評価結果を表示\n",
        "    # confusion matrix を計算する\n",
        "    # 検証データの正解ラベル y_val と 予測ラベル y_val_pred から混同行列を作成する\n",
        "    val_conf_matrix = confusion_matrix(y_val, y_val_pred)\n",
        "    print('Validation confusion matrix')\n",
        "    print(val_conf_matrix)\n",
        "\n",
        "    # 正確度 accuracy を見る\n",
        "    val_accuracy = accuracy_score(y_val, y_val_pred)\n",
        "    print(f'Validation Accuracy: {val_accuracy: .2f}')\n",
        "\n",
        "    # 分類のレポートを見る\n",
        "    val_class_report = classification_report(y_val, y_val_pred)\n",
        "    print('Validation Classification Report')\n",
        "    print(val_class_report)\n",
        "\n",
        "    # テストデータをモデルに代入し、予測ラベルを計算する\n",
        "    y_test_pred = model.predict(X_test_normalized)\n",
        "    \n",
        "    # テストデータの正解ラベル y_test と 予測ラベル y_test_pred から混同行列を作成する\n",
        "    test_conf_matrix = confusion_matrix(y_test, y_test_pred)\n",
        "    print('Test confusion matrix')\n",
        "    print(test_conf_matrix)\n",
        "\n",
        "    # テストデータに対する評価結果を表示\n",
        "    # 正確度 accuracy を見る\n",
        "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "    print(f'Test Accuracy: {test_accuracy: .2f}')\n",
        "\n",
        "    # 分類のレポートを見る\n",
        "    test_class_report = classification_report(y_test, y_test_pred)\n",
        "    print('Test Classification Report')\n",
        "    print(test_class_report)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 5.1 練習問題\n",
        "- 下のセルで異なるモデルを選んでみましょう\n",
        "- 前処理と評価はすでに上のセルで関数で定義してあるので心配いりません\n",
        "- 自分が使いたいと思うモデルのコメントを2行外してください\n",
        "- 他のモデルはすべてコメントアウト(行頭に # がついている)されていることを確認してください"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#前処理: ここはいじらなくて大丈夫です\n",
        "breast_cancer_preprocessing()\n",
        "\n",
        "######## モデル一覧 ########\n",
        "### このセクションに自分が使いたいモデルのコメントを外してください\n",
        "### 初期状態では、決定木のモデルを使うようになっています\n",
        "\n",
        "#from sklearn.linear_model import LogisticRegression #ロジスティック回帰\n",
        "#model = LogisticRegression()\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier #決定木\n",
        "model = DecisionTreeClassifier()\n",
        "\n",
        "#from sklearn.ensemble import RandomForestClassifier #ランダムフォレスト\n",
        "#model = RandomForestClassifier()\n",
        "\n",
        "#from sklearn.svm import SVC #サポートベクターマシン\n",
        "#model = SVC()\n",
        "\n",
        "#from sklearn.neighbors import KNeighborsClassifier #k近傍法\n",
        "#model = KNeighborsClassifier()\n",
        "\n",
        "#from sklearn.naive_bayes import GaussianNB #ナイーブベイズ\n",
        "#model = GaussianNB()\n",
        "\n",
        "#from sklearn.neural_network import MLPClassifier #ニューラルネットワーク\n",
        "#model = MLPClassifier()\n",
        "\n",
        "#from sklearn.ensemble import GradientBoostingClassifier #勾配ブースティング\n",
        "#model = GradientBoostingClassifier()\n",
        "\n",
        "######## モデル一覧 ここまで ########\n",
        "\n",
        "\n",
        "####ここから下はいじらなくて大丈夫です。\n",
        "#評価\n",
        "breast_cancer_evaluation()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
