{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/kytk/AI-MAILs/blob/main/python_7_scikit-learn-2.ipynb?hl=ja\" target=\"_blank\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JHVIs_ypyjg0"
   },
   "source": [
    "## 医療従事者のためのPython: 機械学習 (2)\n",
    "\n",
    "根本清貴 (筑波大学医学医療系精神医学)\n",
    "\n",
    "Ver.20240823"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 目次\n",
    "1. 機械学習とは (復習)\n",
    "2. 機械学習の適切なモデルの選び方\n",
    "3. 教師なし学習の概要\n",
    "4. 主成分分析\n",
    "5. 乳がんデータセットでの主成分分析\n",
    "6. k-meansクラスタリング\n",
    "7. 乳がんデータセットでのk-meansクラスタリング"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 機械学習とは (復習)\n",
    "- 機械学習の定義\n",
    "    - 「データからルールやパターンを導き出し、予測や意思決定を行う技術」\n",
    "    - (従来: 人間がルールを決める) \n",
    "- 機械学習の種類\n",
    "    - 教師あり学習\n",
    "        - 入力データとその対応する正解(ラベル)がペアになったデータセットを用いてモデルを訓練\n",
    "        - 新しいデータが入ってきた時にそのモデルから正しいラベルを予測\n",
    "        - 例: 画像の分類(犬と猫の画像を分類)、スパムメールの分類、価格予測\n",
    "    - 教師なし学習\n",
    "        - ラベルのないデータを用いてモデルを訓練\n",
    "        - データの内部構造やパターンを見つけ出す\n",
    "        - 例: クラスタリング(似たデータをグループに分ける)、次元削減(データの特徴を少数の重要な特徴に圧縮する)\n",
    "    - 強化学習\n",
    "        - エージェント(学習者)が環境と相互作用しながら学習する。エージェントは行動を選択し、その結果として得られる報酬を基に次の行動を改善する\n",
    "        - 長期的な累積報酬を最大化することが目標\n",
    "        - 例: ゲーム、ロボット制御、自動運転\n",
    "- 本日は、教師なし学習 (unsupervised learning) について学ぶ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 機械学習の適切なモデルの選び方\n",
    "\n",
    "- scikit-learn のホームページにわかりやすい図が示されている\n",
    "    - https://scikit-learn.org/1.3/tutorial/machine_learning_map/index.html\n",
    "<img src=\"https://scikit-learn.org/1.3/_static/ml_map.png\">\n",
    "- これを見ると、機械学習を行うには、データは最低50例必要であることがわかる\n",
    "    - 根拠を探したが見つけられなかった"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 教師なし学習の概要\n",
    "\n",
    "- データに対する正解ラベルを必要とせず、データの内部構造やパターンを学習する方法\n",
    "- ラベルのないデータを用いて、データの特性を理解し、データを構造化することが目的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 3.1. 教師なし学習の主要な手法\n",
    "\n",
    "1. **次元削減 (Dimensionality Reduction)**\n",
    "   - データの特徴量の次元を減らし、データを簡潔に表現する手法\n",
    "   - 主なアルゴリズム: 主成分分析 (PCA)、独立成分分析 (ICA)、t-SNEなど\n",
    "   - **例**: 遺伝子発現解析において、数千の遺伝子データを主成分分析(PCA)で数十の主要パターンに圧縮することで、疾患の特徴や患者群の違いを効率的に可視化する\n",
    "\n",
    "2. **クラスタリング (Clustering)**\n",
    "   - データを似た特徴を持つグループ（クラスター）に分ける手法\n",
    "   - 主なアルゴリズム: k-means、階層型クラスタリング、DBSCANなど\n",
    "   - **例**: 患者の血液検査結果を用いて、似た特徴を持つ患者群を自動的に分類する。k-meansアルゴリズムを使用して、例えば3つのクラスターに分けると、「健康な患者群」「代謝異常リスク群」「炎症性疾患リスク群」といった意味のあるグループが形成され、それぞれに適した予防策や治療方針の策定に役立つ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **異常検知 (Anomaly Detection)**\n",
    "   - 正常なデータから外れる異常なデータポイントを検出する手法。\n",
    "   - 主なアルゴリズム: 一クラスSVM、孤立森林、ガウス混合モデルなど。\n",
    "   - **例**: 心電図データに孤立森林アルゴリズムを適用することで、通常の心拍パターンから逸脱した不整脈や心臓異常を自動的に検出し、早期診断や緊急対応につなげることができる\n",
    "\n",
    "- 今回は主成分分析とk-meansクラスタリングを実装する\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 主成分分析\n",
    "- 次元削減のうち、よく用いられている主成分分析をscikit-learnで実装する\n",
    "- 医療データに応用する前に、わかりやすいように学生の5教科 (5次元) の試験結果を主成分分析で2次元にしてみる\n",
    "\n",
    "#### 4.1. パッケージのインポート\n",
    "- NumPy\n",
    "- Pandas\n",
    "- Matplotlib\n",
    "- Seaborn\n",
    "- Scikit-learn の preprocessingモジュール から 標準化関数 StandardScaler\n",
    "- Scikit-learn の decompositionモジュール から PCA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2. データの読み込み\n",
    "\n",
    "- student_scores.csv をダウンロード\n",
    "- Pandas で df として読み込む\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# コマンドの前に ! をつけると、Linuxコマンドが動作できる\n",
    "![[ -f student_scores.csv ]] || wget https://raw.githubusercontent.com/kytk/AI-MAILs/main/data/student_scores.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# student_scores.csv を df という名前の Pandasデータフレームとして読み込み\n",
    "df = pd.read_csv('student_scores.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データを表示\n",
    "# group は science が理系、humanities が文系\n",
    "# 150人の5教科のテストの成績\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3. PandasのDataFrameをNumpyのndarrayに変換\n",
    "\n",
    "- scikit-learn で扱うデータは基本、NumPy配列\n",
    "- 前回はサンプルデータを使っているのであまり意識しなかったが、PandasのDataFrameをNumPyに変換する\n",
    "- PandasからNumPyへの変換は、`to_numpy()` メソッドを使うだけなのでとても簡単\n",
    "- group以外の5教科の点数をNumPy配列に変換する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfから 'group' の列だけ drop して、df_scoresとして代入\n",
    "# df.drop('group', axis=1) で削除できる\n",
    "df_scores = df.drop('group', axis=1)\n",
    "\n",
    "# df_scores を確認\n",
    "df_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PandasのDataFrame df_scores を NumPy配列に変換\n",
    "# to_numpy() メソッドを使用\n",
    "scores = df_scores.to_numpy()\n",
    "\n",
    "# scores の最初の10人を確認\n",
    "# scores[0:10,:] だが、後半の : は省略できる\n",
    "scores[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3. データの前処理: 標準化\n",
    "- 5教科を 平均値0, 標準偏差1 となるように標準化する\n",
    "- `StandardScaler()` 関数を使用する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データの標準化\n",
    "\n",
    "# StandardScaler() 関数を使用して標準化を行うオブジェクト scaler を生成\n",
    "scaler = StandardScaler()  \n",
    "# 標準化するためのパラメータを計算(fit)し、適用(transform)する\n",
    "scores_standardized = scaler.fit_transform(scores)\n",
    "\n",
    "# 最初の数行を確認\n",
    "# scores, scores_scaled は numpy型 なので、scores_scaled[:3] で最初の3行が表示される\n",
    "\n",
    "# scores\n",
    "print('scores')\n",
    "print(scores[:3])\n",
    "\n",
    "# scores_standardized\n",
    "# np.round(X,2)はXの結果を小数点2位で丸めている\n",
    "print('\\nscores_standardized')\n",
    "print(np.round(scores_standardized[:3],2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 平均と標準偏差を確認\n",
    "# axis=0 とすることで、各列における行の平均や標準偏差を求められる\n",
    "print('\\nscoresの平均:', np.round(scores.mean(axis=0),1))\n",
    "print('scoresの標準偏差:', np.round(scores.std(axis=0),1))\n",
    "\n",
    "print('\\nscores_standardizedの平均:', np.round(scores_standardized.mean(axis=0),2))\n",
    "print('scores_standardizedの標準偏差:', np.round(scores_standardized.std(axis=0),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4. 主成分分析の理解\n",
    "\n",
    "- 主成分分析の本質は、「データの分散が一番大きくなるような軸を見つける」こと\n",
    "- 以下のサイトが理解に役立つ\n",
    "  - https://www.billconnelly.net/?p=697\n",
    "- 5教科だと図示しづらいため、5教科ではなく、3教科(英語、数学、国語)の試験結果を2次元に圧縮してみる\n",
    "- 最初に3教科の試験結果を3次元で表示する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "# 前処理データから、英語、数学、国語の結果を抽出\n",
    "X = scores_standardized[:,[0,1,3]]\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# PCA実行 (この後説明するので、今は理解できなくていい)\n",
    "pca3 = PCA(n_components=2)\n",
    "X_pca = pca3.fit_transform(X)\n",
    "\n",
    "# 以下は、図示のためのコード\n",
    "# 3D散布図（元のデータ）\n",
    "fig1 = plt.figure(figsize=(8, 6))\n",
    "ax1 = fig1.add_subplot(111, projection='3d')\n",
    "scatter1 = ax1.scatter(X[:, 0], X[:, 1], X[:, 2],\n",
    "                     c=df['group'].map({'science': 0, 'humanities': 1}), \n",
    "                     cmap='winter')\n",
    "ax1.set_xlabel('English')\n",
    "ax1.set_ylabel('Math')\n",
    "ax1.set_zlabel('Japanese')\n",
    "ax1.set_title('3D Data with PCA Plane')\n",
    "\n",
    "# データの範囲を取得\n",
    "x_min, x_max = ax1.get_xlim()\n",
    "y_min, y_max = ax1.get_ylim()\n",
    "z_min, z_max = ax1.get_zlim()\n",
    "\n",
    "# 原点を通る点線を追加\n",
    "ax1.plot([x_min, x_max], [0, 0], [0, 0], 'k--', alpha=0.5)  # x軸\n",
    "ax1.plot([0, 0], [y_min, y_max], [0, 0], 'k--', alpha=0.5)  # y軸\n",
    "ax1.plot([0, 0], [0, 0], [z_min, z_max], 'k--', alpha=0.5)  # z軸\n",
    "\n",
    "# カラーバーの追加\n",
    "plt.colorbar(scatter1, ax=ax1, label='Group (0: Science, 1: Humanities)')\n",
    "\n",
    "# 視点の調整\n",
    "ax1.view_init(elev=15, azim=135)  # 仰角15度、方位角135度\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- この空間に線を一本ひく。その線は、データが一番ばらついて見える線であり、それが第1主成分の軸\n",
    "- その次に、第1主成分に垂直に線をひく。その線もできるだけデータがばらつくようにする。これが第2主成分の軸\n",
    "- これらの軸を構成する単位ベクトルを **固有ベクトル** という。固有ベクトルの大きさは1\n",
    "    - 下の図では、ベクトルを見やすくするために、固有ベクトルを3.5倍している\n",
    "- そのベクトルにおけるデータの分散を **固有値** という"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 先程のプロットと同じ内容\n",
    "fig1 = plt.figure(figsize=(8, 6))\n",
    "ax1 = fig1.add_subplot(111, projection='3d')\n",
    "scatter1 = ax1.scatter(X[:, 0], X[:, 1], X[:, 2],\n",
    "                     c=df['group'].map({'science': 0, 'humanities': 1}), \n",
    "                     cmap='winter')\n",
    "ax1.set_xlabel('English')\n",
    "ax1.set_ylabel('Math')\n",
    "ax1.set_zlabel('Japanese')\n",
    "ax1.set_title('3D Data with PCA Plane')\n",
    "\n",
    "x_min, x_max = ax1.get_xlim()\n",
    "y_min, y_max = ax1.get_ylim()\n",
    "z_min, z_max = ax1.get_zlim()\n",
    "\n",
    "ax1.plot([x_min, x_max], [0, 0], [0, 0], 'k--', alpha=0.5)  # x軸\n",
    "ax1.plot([0, 0], [y_min, y_max], [0, 0], 'k--', alpha=0.5)  # y軸\n",
    "ax1.plot([0, 0], [0, 0], [z_min, z_max], 'k--', alpha=0.5)  # z軸\n",
    "#### ここまで\n",
    "\n",
    "# 主成分ベクトルの描画\n",
    "for i, color in enumerate(['r', 'g']):\n",
    "    ax1.quiver(pca3.mean_[0], pca3.mean_[1], pca3.mean_[2],\n",
    "              pca3.components_[i, 0], pca3.components_[i, 1], pca3.components_[i, 2],\n",
    "              length=3.5, color=color, label=f'PC{i+1}')\n",
    "\n",
    "ax1.legend()\n",
    "\n",
    "# カラーバーの追加\n",
    "plt.colorbar(scatter1, ax=ax1, label='Group (0: Science, 1: Humanities)')\n",
    "\n",
    "# 視点の調整\n",
    "ax1.view_init(elev=15, azim=135)  # 仰角15度、方位角135度\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 第1主成分と第2主成分で構成される平面を図示する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 先程のプロットと同じ内容\n",
    "fig1 = plt.figure(figsize=(8, 6))\n",
    "ax1 = fig1.add_subplot(111, projection='3d')\n",
    "scatter1 = ax1.scatter(X[:, 0], X[:, 1], X[:, 2],\n",
    "                     c=df['group'].map({'science': 0, 'humanities': 1}), \n",
    "                     cmap='winter')\n",
    "ax1.set_xlabel('English')\n",
    "ax1.set_ylabel('Math')\n",
    "ax1.set_zlabel('Japanese')\n",
    "ax1.set_title('3D Data with PCA Plane')\n",
    "\n",
    "x_min, x_max = ax1.get_xlim()\n",
    "y_min, y_max = ax1.get_ylim()\n",
    "z_min, z_max = ax1.get_zlim()\n",
    "\n",
    "ax1.plot([x_min, x_max], [0, 0], [0, 0], 'k--', alpha=0.5)  # x軸\n",
    "ax1.plot([0, 0], [y_min, y_max], [0, 0], 'k--', alpha=0.5)  # y軸\n",
    "ax1.plot([0, 0], [0, 0], [z_min, z_max], 'k--', alpha=0.5)  # z軸\n",
    "\n",
    "for i, color in enumerate(['r', 'g']):\n",
    "    ax1.quiver(pca3.mean_[0], pca3.mean_[1], pca3.mean_[2],\n",
    "              pca3.components_[i, 0], pca3.components_[i, 1], pca3.components_[i, 2],\n",
    "              length=3.5, color=color, label=f'PC{i+1}')\n",
    "#### ここまで\n",
    "\n",
    "# PCA平面の描画\n",
    "# 平面の角を計算\n",
    "corner00 = pca3.mean_\n",
    "corner01 = corner00 + pca3.components_[0] * 3.5\n",
    "corner10 = corner00 + pca3.components_[1] * 3.5\n",
    "corner11 = corner00 + pca3.components_[0] * 3.5 + pca3.components_[1] * 3.5\n",
    "\n",
    "# 平面を描画\n",
    "ax1.plot_surface(\n",
    "    X=np.array([[corner00[0], corner01[0]], [corner10[0], corner11[0]]]),\n",
    "    Y=np.array([[corner00[1], corner01[1]], [corner10[1], corner11[1]]]),\n",
    "    Z=np.array([[corner00[2], corner01[2]], [corner10[2], corner11[2]]]),\n",
    "    alpha=0.2, color='r'\n",
    ")\n",
    "\n",
    "ax1.legend()\n",
    "\n",
    "# カラーバーの追加\n",
    "plt.colorbar(scatter1, ax=ax1, label='Group (0: Science, 1: Humanities)')\n",
    "\n",
    "# 視点の調整\n",
    "ax1.view_init(elev=15, azim=135)  # 仰角15度、方位角135度\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "### 参考: 以下は、matplotlib の結果を保存したい場合やアニメーションにしたい場合のコード\n",
    "\n",
    "## グラフを保存したい場合\n",
    "#fig1.savefig('3d_with_PCA.png')\n",
    "\n",
    "## アニメーションの作成\n",
    "#def animate(frame):\n",
    "# ax1.view_init(elev=15, azim=frame)\n",
    "# return fig1,\n",
    "\n",
    "#ani = animation.FuncAnimation(fig1, animate, frames=np.arange(0, 360, 2), interval=100)\n",
    "\n",
    "## アニメーションの表示\n",
    "#from IPython.display import HTML\n",
    "#HTML(ani.to_jshtml())\n",
    "\n",
    "## アニメーションをファイルに保存\n",
    "#ani.save('rotating_3d_plot.gif', writer='pillow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 上記の3次元グラフを回転させることで、よりイメージがつかみやすくなる (金子貴久子先生提供)\n",
    "    <img src = \"https://www.nemotos.net/nb/img/rotating_3d_plot.gif\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 元のデータを、第1主成分、第2主成分で構成される平面に投影する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D散布図（PCAの第1主成分、第2主成分で構成される座標軸）\n",
    "fig2 = plt.figure(figsize=(6, 4))\n",
    "ax2 = fig2.add_subplot(111)\n",
    "scatter2 = ax2.scatter(X_pca[:, 0], X_pca[:, 1],\n",
    "                       c=df['group'].map({'science': 0, 'humanities': 1}), cmap='winter')\n",
    "\n",
    "ax2.set_xlabel('First Principal Component')\n",
    "ax2.set_ylabel('Second Principal Component')\n",
    "ax2.set_title('2D Projection after PCA')\n",
    "ax2.set_xlim(-3.5,3.5)\n",
    "ax2.set_ylim(-3.5,3.5)\n",
    "\n",
    "# カラーバーの追加\n",
    "plt.colorbar(scatter2, ax=ax2, label='Group (0: Science, 1: Humanities)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- データの第1主成分のばらつきは-3〜3ぐらいにわたる\n",
    "- 第2主成分のばらつきは、-2.5〜2.5程度で第1主成分ほどばらついていない\n",
    "- このように、3次元のデータをデータのばらつきに基づいて、2次元で表示できた"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5. 5教科の試験結果に対して、PCAを適用し、2次元に削減\n",
    "- Scikit-learn の PCAの実行そのものは非常に簡単\n",
    "- `PCA()` 関数を使用して PCAを行うオブジェクトを生成する\n",
    "    - その際、`n_components=2` のように次元を指定する\n",
    "    - ここでは 2 に設定しているので、データを2次元に圧縮する\n",
    "    - つまり、元のデータから、もっとも重要な2つの主成分だけを保持するようにPCAに指示する\n",
    "- その後、準備したオブジェクトのメソッド `fit_transform()` にデータを投入する\n",
    "    - `fit()`: 入力データに基づいて、固有ベクトルを見つけ、それぞれのベクトルにおけるデータの分散 (固有値) を計算する\n",
    "    - `transform()`: 学習したモデルを使って、入力データを主成分空間に変換する\n",
    "    - `fit_transform()`: `fit()` と `transform()` を同時に行う\n",
    "- scores_pca は、元のデータを2次元の主成分空間に投影した結果\n",
    "    - 各行は元のデータに対応し、2つの列はそれぞれ第1主成分と第2主成分の値を意味する\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCAの適用\n",
    "\n",
    "# 5教科のスコアに対してPCAを行うオブジェクト pca5 を生成\n",
    "pca5 = PCA(n_components=2)\n",
    "\n",
    "# 前処理が終わったデータを投入し、fitで学習、transformで変換\n",
    "scores_pca = pca5.fit_transform(scores_standardized) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.5.1. PCAの結果を確認\n",
    "- PCAの結果は、上記の `pca5` と `scores_pca` の2つのオブジェクトを確認する必要がある\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### scores_pca (PCAを行った結果がおさめられているオブジェクト) \n",
    "- scores_pca: PCAの結果得られた各個人の主成分(今の場合は2)からできる座標軸における値\n",
    "  - scores_pcaの形状: `scores_pca.shape`\n",
    "  - scores_pca の実際の値 `scores_pca`\n",
    "    - 第1主成分をx軸、第2主成分をy軸とした時の座標というイメージ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 大きさは150行2列\n",
    "# 150人の5教科のデータが2つの情報に集約された\n",
    "print('scoresの形状: ', scores_pca.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores_pca そのものを見る\n",
    "# 各データが新たな第1軸, 第2軸に対する座標で表現されている\n",
    "print('scores_pca の実際の値: 5人分\\n', np.round(scores_pca[:5],2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### pca5 (PCAを行うために生成したオブジェクト) の属性\n",
    "- このオブジェクトの属性にPCAを行ったことにより得られた主成分の固有ベクトル、固有値、寄与率などがおさめられている\n",
    "  - 各主成分の元の特徴量空間 (今の場合は5次元) における固有ベクトル: `pca5.components_`\n",
    "  - 各固有ベクトルにおける固有値 (元データの分散): `pca5.explained_variance_`\n",
    "  - 寄与率 (元データの分散を1とした時、各主成分の固有値がその何割を説明するか: `pca5.explained_variance_ratio_`\n",
    "  - 累積寄与率: 主成分でデータの何％を説明できるか(累積寄与率): `np.sum(pca5.explained_variance_ratio_)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `pca5.components_`\n",
    "   - 各主成分の固有ベクトルを表す行列\n",
    "   - 各行が主成分(第1主成分、第2主成分)に対応し、列は元の特徴量(教科)に対応"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 固有ベクトルは、元の特徴量空間において、第1軸、第2軸がどの方向を向いているかを示す\n",
    "print('固有ベクトル(主成分の方向): \\n', np.round(pca5.components_,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `pca5.components_.T`\n",
    "   - `.T`は転置を意味\n",
    "   - 転置することで、各行が元の特徴量(教科)に、各列が主成分(第1主成分、第2主成分)に対応"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('固有ベクトルの転置: \\n', np.round(pca5.components_.T,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `pca5.explained_variance_`\n",
    "   - 固有値 (各主成分における元データの分散)\n",
    "   - 各主成分がデータの全体の分散のうちどれだけを説明しているかを意味\n",
    "   - 固有値は、データ分析や次元削減を行う際に、どの主成分を重視すべきかを判断する上で重要な指標となる\n",
    "- `np.sqrt(pca5.explained_variance_)`\n",
    "   - 分散の平方根を取ることで、固有値を標準偏差のスケールに変換"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('固有値(主成分の分散): ', np.round(pca5.explained_variance_,2))\n",
    "print('固有値の標準偏差: ', np.round(np.sqrt(pca5.explained_variance_),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 大きい固有値は以下のように解釈できる\n",
    "    1. データの分散を多く捉えている: 固有値が大きいほど、その主成分は元のデータの分散（ばらつき）をより多く表現している\n",
    "    2. 情報量が多い: 大きな固有値を持つ主成分は、元のデータセットの中でより多くの情報を保持している\n",
    "    3. 特徴の重要性が高い: 固有値が大きいほど、その主成分が表す特徴がデータ全体の構造を理解する上で重要である\n",
    "    4. データの変動パターンをよく表現している: 大きな固有値を持つ主成分は、データ内の主要な変動パターンやトレンドをより正確に捉えている\n",
    "    5. 次元圧縮時の情報保持率が高い: 固有値が大きいほど、その主成分を使ってデータを低次元に圧縮した際に、元のデータの特性をより多く保持できている\n",
    "    6. データの構造をよく反映している: 大きな固有値を持つ主成分は、データセット内の重要な構造や関係性をより良く表現している"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `pca5.explained_variance_ratio_`\n",
    "  - 固有値の寄与率\n",
    "    - 各主成分がデータの全体的な変動（分散）のうち、どれだけの割合を説明しているかを示す\n",
    "    - 値は0から1の間で、全ての値の合計は1（つまり100%）になる\n",
    "    - 大きい値ほど、その主成分がデータの構造をよく捉えていることを意味する\n",
    "    - 最初の数個の主成分の寄与率が高い場合、少数の主成分でデータの大部分を表現できることを示す\n",
    "- `sum(pca5.explained_variance_ratio_)`\n",
    "  - 累積寄与率\n",
    "    - 主成分全体でデータの何％を説明できるかを示す\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('固有値の寄与率: ', np.round(pca5.explained_variance_ratio_,2))\n",
    "print('累積寄与率: ', np.round(sum(pca5.explained_variance_ratio_),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5.2. 主成分負荷量\n",
    "\n",
    "- 主成分負荷量とは\n",
    "    - 主成分の固有ベクトルに主成分の固有値の標準偏差をかけ合わせたもの\n",
    "    - 元の特徴量が各主成分にどの程度寄与しているかを示す\n",
    "    - -1から1の間の値をとり、絶対値が大きいほど寄与度が高い\n",
    "- 主成分負荷量の解釈\n",
    "    - 正の値: その特徴量が主成分に正の影響を与える\n",
    "    - 負の値: その特徴量が主成分に負の影響を与える\n",
    "    - 絶対値が大きい: その特徴量がその主成分に強く影響している    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 各特徴量（教科）の主成分への寄与度 (主成分負荷量)\n",
    "\n",
    "# pca5.components_ を列で示したいので、.T で転置する\n",
    "# それに固有値の標準偏差をかけあわせる\n",
    "loadings = np.round(pca5.components_.T * np.sqrt(pca5.explained_variance_),2)\n",
    "\n",
    "# 結果を表にしたいので、Pandasを使って表にする\n",
    "# NumPy配列をPandasにするときには、pd.DataFrame(numpy配列, columns=[列名], index=[行名])とする\n",
    "loadings_df = pd.DataFrame(loadings, columns=['PC1', 'PC2'], index=df.columns[:-1])\n",
    "\n",
    "\n",
    "print(\"\\n各特徴量の主成分負荷量:\")\n",
    "loadings_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 主成分負荷量からの考察\n",
    "  - 英語は PC2 の負の方向に大きく寄与している\n",
    "  - 数学と理科は PC1 の負の方向に寄与している\n",
    "  - 国語は PC1 と PC2 に同程度寄与している\n",
    "  - 社会は PC1 の正の方向に寄与している\n",
    "\n",
    "- 第1主成分は、プラスなら文系、マイナスなら理系ということで、「文理軸」と言えるか\n",
    "- 第2主成分は、プラスなら国語、マイナスなら英語ということで、「言語軸」と言えるか"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.6. PCAの結果の可視化\n",
    "\n",
    "##### 4.6.1. 散布図\n",
    "- scores_pca を横軸 第1主成分 PC1, 縦軸 第2主成分 PC2 の座標にプロット\n",
    "- 個々の点は個人を示す\n",
    "- 色は、その人が理系か文系を示す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 散布図にプロット\n",
    "# 横軸に scores_pcaの第1列、縦軸に scores_pcaの第2列をプロット\n",
    "\n",
    "# 理系の人を0(青色), 文系の人を1(緑色)で表示\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "scatter = plt.scatter(scores_pca[:, 0], scores_pca[:, 1], \n",
    "                      c=df['group'].map({'science': 0, 'humanities': 1}), cmap='winter')\n",
    "plt.colorbar()\n",
    "plt.xlabel('First Principal Component')\n",
    "plt.ylabel('Second  Principal Component')\n",
    "plt.title('PCA results')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 以下のコードで、どの点がどの個人かを同定することもできる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "scatter = plt.scatter(scores_pca[:, 0], scores_pca[:, 1], \n",
    "                      c=df['group'].map({'science': 0, 'humanities': 1}), cmap='winter')\n",
    "plt.colorbar()\n",
    "plt.xlabel('First Principal Component')\n",
    "plt.ylabel('Second  Principal Component')\n",
    "plt.title('Student Scores in PC Space')\n",
    "for i, txt in enumerate(df.index):\n",
    "    plt.annotate(txt, (scores_pca[i, 0], scores_pca[i, 1]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.6.2. 主成分平面における各教科の位置\n",
    "- 固有ベクトルは、各主成分において元の特徴量がどの程度寄与するかを意味しているが、見方を変えて、「各教科が主成分にどのように寄与しているか」という観点で見ると、各教科が第1主成分、第2主成分にどう影響しているかを見ることができる\n",
    "- 各教科が、第1主成分、第2主成分を軸とする空間の中でどこに位置するかを可視化する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 固有ベクトルを改めて表示\n",
    "print(\"固有ベクトル:\")\n",
    "print(pd.DataFrame(np.round(pca5.components_.T,2), \n",
    "                   columns=['PC1', 'PC2'], \n",
    "                   index=df.columns[:-1]))\n",
    "\n",
    "# 各教科のPC1, PC2の値からなるベクトルをプロット\n",
    "plt.figure(figsize=(6, 6))\n",
    "for i, (x, y) in enumerate(zip(pca5.components_[0], pca5.components_[1])):\n",
    "    plt.arrow(0, 0, x, y, head_width=0.05, head_length=0.05, fc='r', ec='r')\n",
    "    plt.text(x*1.2, y*1.1, df.columns[i], fontsize=12)\n",
    "\n",
    "# 現在のグラフに原点を通る水平線 (x軸, y軸) を追加\n",
    "plt.axhline(y=0, color='k', linestyle='--', linewidth=0.5)\n",
    "plt.axvline(x=0, color='k', linestyle='--', linewidth=0.5)\n",
    "\n",
    "plt.xlim(-1, 1)\n",
    "plt.ylim(-1, 1)\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.title('Variables factor map (PCA)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- これから改めて以下が考察される\n",
    "    - PC1 は文系か理系かを示す。値が正ならば文系、負ならば理系\n",
    "    - PC2 は言語を示す。値が正ならば国語がより得意、負ならば英語が得意"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 乳がんデータセットを用いた主成分分析\n",
    "- 乳がんデータセットの30の特徴量を2次元まで削減する\n",
    "- その2次元がもしかしたら良性・悪性の特徴に近いかもしれないので、その一致度も見てみる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.0. 乳がんデータセット (復習)\n",
    "- データセットの名称: Breast Cancer Wisconsin (Diagnostic) dataset\n",
    "- サンプル数: 569\n",
    "- 特徴量の数: 30\n",
    "- ターゲットの種類: 2クラス（良性と悪性）\n",
    "  - 0: 悪性; 1: 良性\n",
    "- 特徴量の種類: 実数値\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 細胞診における細胞核の30の特徴量\n",
    "\n",
    "| 英語 | 日本語 | 英語 | 日本語 |\n",
    "| --- | --- | --- | --- |\n",
    "| mean radius | 平均半径 | mean texture | 平均テクスチャ |\n",
    "| mean perimeter | 平均周囲長 | mean area | 平均面積 |\n",
    "| mean smoothness | 平均平滑度 | mean compactness | 平均コンパクト度 |\n",
    "| mean concavity | 平均陥凹度 | mean concave points | 平均陥凹点数 |\n",
    "| mean symmetry | 平均対称性 | mean fractal dimension | 平均フラクタル次元 |\n",
    "| radius error | 半径誤差 | texture error | テクスチャ誤差 |\n",
    "| perimeter error | 周囲長誤差 | area error | 面積誤差 |\n",
    "| smoothness error | 平滑度誤差 | compactness error | コンパクト度誤差 |\n",
    "| concavity error | 陥凹度誤差 | concave points error | 陥凹点数誤差 |\n",
    "| symmetry error | 対称性誤差 | fractal dimension error | フラクタル次元誤差 |\n",
    "| worst radius | 最悪の半径 | worst texture | 最悪のテクスチャ |\n",
    "| worst perimeter | 最悪の周囲長 | worst area | 最悪の面積 |\n",
    "| worst smoothness | 最悪の平滑度 | worst compactness | 最悪のコンパクト度 |\n",
    "| worst concavity | 最悪の陥凹度 | worst concave points | 最悪の陥凹点数 |\n",
    "| worst symmetry | 最悪の対称性 | worst fractal dimension | 最悪のフラクタル次元 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1. パッケージのインポート\n",
    "- NumPy\n",
    "- Pandas\n",
    "- Matplotlib\n",
    "- Seaborn\n",
    "- Scikit-learn の preprocessingモジュール から 正規化関数 MinMaxScaler\n",
    "- Scikit-learn の decompositionモジュール から PCA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2. データの読み込み\n",
    "\n",
    "- 先程と同じように、サンプルデータセットを読み込むのではなく、Excelファイルを Pandas に読み込む\n",
    "- target は不要のために外す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# コマンドの前に ! をつけると、Linuxコマンドが動作できる\n",
    "![[ -f breast_cancer_data.xlsx ]] || wget https://raw.githubusercontent.com/kytk/AI-MAILs/main/data/breast_cancer_data.xlsx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pandas の DataFrame への読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('breast_cancer_data.xlsx')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target 列は削除\n",
    "df_data = df.drop('target', axis=1)\n",
    "\n",
    "df_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_data の列名を feature_names とする\n",
    "feature_names = df_data.columns\n",
    "\n",
    "feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# NumPy配列に変換\n",
    "data = df_data.to_numpy()\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target は target としてnumpyに変換しておく\n",
    "target = df['target'].to_numpy()\n",
    "\n",
    "target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3. データの前処理: 正規化\n",
    "- 30の特徴量に対して正規化 (0-1 に変換)を行う\n",
    "- 教師あり学習の時と同じ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データの正規化\n",
    "\n",
    "# MinMaxScaler() 関数を使用して標準化を行うオブジェクト scaler を生成\n",
    "scaler = MinMaxScaler()  \n",
    "# 正規化するためのパラメータを計算(fit)し、適用(transform)する\n",
    "data_normalized = scaler.fit_transform(data)\n",
    "\n",
    "# 最初の数行を確認\n",
    "# data, data_normalized は numpy型 なので、data[:3] で最初の3行が表示される\n",
    "\n",
    "# data\n",
    "print('data')\n",
    "print(data[:3],1)\n",
    "\n",
    "# data_normalized\n",
    "print('\\ndata_normalized')\n",
    "print(np.round(data_normalized[:3],1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.4. PCAの適用\n",
    "\n",
    "- `PCA()` 関数を使用して PCAを行うオブジェクトを生成する\n",
    "  - `n_components` で 圧縮する次元を指定する\n",
    "- その後、準備したオブジェクトのメソッド `fit_transform()` にデータを投入する\n",
    "    - `fit()`: 入力データに基づいてPCAモデルを学習\n",
    "        - これにより、主成分（固有ベクトル）と、各主成分の重要度（固有値）が計算される\n",
    "    - `transform()`: 学習したモデルを使って、入力データを新しい主成分空間に変換する\n",
    "- `scores_pca` は、元のデータを2次元の主成分空間に投影した結果\n",
    "    - 各行は元のデータに対応し、2つの列はそれぞれ第1主成分と第2主成分の値を意味する\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCAの適用\n",
    "\n",
    "# PCAを行うオブジェクト pca を生成\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# 前処理が終わったデータを投入し、fitで学習、transformで変換\n",
    "pca_results = pca.fit_transform(data_normalized) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- PCAの結果を確認\n",
    "    - pca_resultsの形状: `pca_results.shape`\n",
    "    - pca_results の実際の値 `pca_results`\n",
    "    - 固有ベクトル: `pca.components_`\n",
    "    - 固有値: `pca.explained_variance_`\n",
    "    - 寄与率: `pca.explained_variance_ratio_`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca_results の形状\n",
    "# 569行2列\n",
    "pca_results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca_results そのもの\n",
    "# 小数点2位で丸め\n",
    "np.round(pca_results,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 固有ベクトル\n",
    "np.round(pca.components_.T,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 固有値の標準偏差\n",
    "\n",
    "np.round(np.sqrt(pca.explained_variance_),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 固有値の寄与率\n",
    "np.round(pca.explained_variance_ratio_,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 主成分負荷量\n",
    "\n",
    "loadings = pca.components_.T * np.sqrt(pca.explained_variance_)\n",
    "\n",
    "np.round(loadings,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 特徴量が多いと、主成分が何を意味するかすぐに把握しづらい\n",
    "- こういう時に視覚化が役立つ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5. PCAの結果の視覚化\n",
    "\n",
    "#### 5.5.1. 散布図"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 散布図にプロット\n",
    "# 横軸に pca_resultsの第1列、縦軸に pca_resultsの第2列をプロット\n",
    "\n",
    "# データフレームの作成\n",
    "df_pca = pd.DataFrame(data=pca_results, columns=['PC1', 'PC2'])\n",
    "df_pca['diagnosis'] = target\n",
    "\n",
    "# プロットの作成\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x='PC1', y='PC2', hue='diagnosis', data=df_pca, palette='winter')\n",
    "\n",
    "plt.title('PCA of Breast Cancer Dataset')\n",
    "plt.xlabel(f'First Principal Component (Variance explained: {pca.explained_variance_ratio_[0]:.2f})')\n",
    "plt.ylabel(f'Second Principal Component (Variance explained: {pca.explained_variance_ratio_[1]:.2f})')\n",
    "\n",
    "# 凡例の追加\n",
    "plt.legend(title='Diagnosis', labels=['Malignant', 'Benign'])\n",
    "\n",
    "# 軸の原点に線を追加\n",
    "plt.axhline(y=0, color='k', linestyle='--', linewidth=0.5)\n",
    "plt.axvline(x=0, color='k', linestyle='--', linewidth=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.5.2. 各特徴量の寄与度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特徴量の寄与度\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# バーの幅と位置の設定\n",
    "bar_width = 0.35\n",
    "r1 = np.arange(len(feature_names))\n",
    "r2 = [x + bar_width for x in r1]\n",
    "\n",
    "# 2つの主成分のloadingsを少しずらして表示\n",
    "plt.bar(r1, loadings[:, 0], color='#ff7f50', width=bar_width, label='First Principal Component')\n",
    "plt.bar(r2, loadings[:, 1], color='#1e90ff', width=bar_width, label='Second Principal Component')\n",
    "\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('PCA Loading')\n",
    "plt.title('PCA Loadings for First and Second Principal Components')\n",
    "plt.xticks([r + bar_width/2 for r in range(len(feature_names))], feature_names, rotation=90)\n",
    "\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第1主成分の負荷量を取得\n",
    "pc1_loadings = loadings[:, 0]\n",
    "\n",
    "# 負荷量の絶対値でソートするためのインデックスを取得\n",
    "sorted_indices = np.argsort(np.abs(pc1_loadings))[::-1]\n",
    "\n",
    "# ソートされた負荷量と対応する特徴量名を取得\n",
    "sorted_loadings = pc1_loadings[sorted_indices]\n",
    "sorted_features = np.array(feature_names)[sorted_indices]\n",
    "\n",
    "# 上位10の特徴量のみを可視化\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(10), sorted_loadings[:10], color='#ff7f50')\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('PC1 Loadings')\n",
    "plt.title('Top 10 Features by PC1 Loading Magnitude')\n",
    "plt.xticks(range(10), sorted_features[:10], rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 上位10の負荷量と特徴量名を表示\n",
    "print(\"Top 10 features by PC1 loading magnitude:\")\n",
    "for feature, loading in list(zip(sorted_features, sorted_loadings))[:10]:\n",
    "    print(f\"{feature}: {loading:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第2主成分の負荷量を取得\n",
    "pc2_loadings = loadings[:, 1]\n",
    "\n",
    "# 負荷量の絶対値でソートするためのインデックスを取得\n",
    "sorted_indices2 = np.argsort(np.abs(pc2_loadings))[::-1]\n",
    "\n",
    "# ソートされた負荷量と対応する特徴量名を取得\n",
    "sorted_loadings2 = pc2_loadings[sorted_indices2]\n",
    "sorted_features2 = np.array(feature_names)[sorted_indices2]\n",
    "\n",
    "# 上位10の特徴量のみを可視化\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(10), sorted_loadings2[:10], color='#1e90ff')\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('PC2 Loadings')\n",
    "plt.title('Top 10 Features by PC2 Loading Magnitude')\n",
    "plt.xticks(range(10), sorted_features2[:10], rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 上位10の負荷量と特徴量名を表示\n",
    "print(\"Top 10 features by PC2 loading magnitude:\")\n",
    "for feature, loading in list(zip(sorted_features2, sorted_loadings2))[:10]:\n",
    "    print(f\"{feature}: {loading:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.6 PCA結果の医学的解釈\n",
    "\n",
    "##### 5.6.1. 第1主成分の解釈\n",
    "\n",
    "1. 腫瘍の形状と大きさを反映\n",
    "   - 高寄与率: 凹点、凹性（形状の不規則性）\n",
    "   - 高寄与率: 周囲長、半径（腫瘍の大きさ）\n",
    "\n",
    "2. 悪性度との関連\n",
    "   - \"worst\"特徴量の多さ: 最も異常な部分を捉える\n",
    "   - 高寄与率: 密度（腫瘍の固さ）\n",
    "\n",
    "医学的意味: 腫瘍の不規則な形状と大きさを表す。値が高いほど悪性の可能性が高い。\n",
    "\n",
    "##### 5.6.2. 第2主成分の解釈\n",
    "\n",
    "1. 腫瘍の微細構造と質感\n",
    "   - 高正寄与率: フラクタル次元（境界線の複雑さ）\n",
    "   - 高正寄与率: 滑らかさ（表面の局所的変動）\n",
    "\n",
    "2. 大きさとの逆相関\n",
    "   - 負寄与率: 半径、周囲長、面積\n",
    "\n",
    "3. 誤差と変動性\n",
    "   - 含有: 密度の誤差（測定の不確実性）\n",
    "\n",
    "医学的意味: 腫瘍の微細構造、質感、ばらつきを表す。大きさとは独立した特徴を捉える。高値は小さいが複雑な構造の腫瘍を示唆。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. k-means クラスタリング\n",
    "- k-means クラスタリングは、教師なし学習の中でも最も基本的でよく使われる手法の一つ\n",
    "- データセットを \\( k \\) 個のクラスターに分割し、各データポイントが最も近いクラスタの中心 (セントロイド) に割り当てられる\n",
    "\n",
    "- アルゴリズムの手順\n",
    "   1. 初期化\n",
    "      - クラスタ数 \\( k \\) を決定し、データポイントからランダムに \\( k \\) 個のセントロイドを選ぶ\n",
    "   2. 割り当て\n",
    "      - 各データポイントを最も近いセントロイドに割り当てる\n",
    "   3. セントロイドの更新\n",
    "      - 各クラスタのセントロイドを、クラスタ内のデータポイントの平均値に更新する\n",
    "   4. 収束\n",
    "      - セントロイドの位置が変わらなくなるまで、割り当てと更新を繰り返す\n",
    "\n",
    "- 評価\n",
    "   - クラスタリングの質を評価するために、内部評価指標（例: シルエットスコア、クラス内分散の総和）を使用する\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1. パッケージのインポート\n",
    "\n",
    "- NumPy\n",
    "- Matplotlib\n",
    "- Seaborn\n",
    "- Scikit-learn の preprocessing モジュールから StandardScaler\n",
    "- Scikit-learn の cluster モジュールから KMeans\n",
    "- Scikit-learn の metrics モジュールから silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1. データの生成\n",
    "- デモンストレーションとして、明確に3つのクラスターに分かれているデータを作ったうえで、クラスタリングを行う\n",
    "- scikit-learnには、クラスターを作ってくれる関数 `make_blobs` があるので、それを利用して3つのクラスターのデータを生成する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# データの生成\n",
    "n_samples = 120\n",
    "n_clusters = 3\n",
    "scores, y = make_blobs(n_samples=n_samples, centers=n_clusters, random_state=10)\n",
    "scores += 70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2つの教科の試験結果　をイメージ\n",
    "np.round(scores[:5,:],0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=scores[:,0], y=scores[:,1])\n",
    "plt.xlabel('Exam 1')\n",
    "plt.ylabel('Exam 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 明らかに3つのクラスタに分かれそう\n",
    "- 3つのクラスタに分けてみる\n",
    "  - クラスタの推測法は後述"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2. データの前処理: 標準化\n",
    "\n",
    "- データの前処理を行う\n",
    "- これまでと同様、StandardScaler 関数を使って標準化を行う"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scores_standardized = scaler.fit_transform(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 標準化後のプロット\n",
    "sns.scatterplot(x=scores_standardized[:,0], y=scores_standardized[:,1])\n",
    "plt.xlabel('Standardized Exam 1')\n",
    "plt.ylabel('Standardized Exam 2')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3. K-meansクラスタリング\n",
    "\n",
    "- K-means クラスタリングでは、まず、自分で「いくつのクラスターに分類するか」を決める\n",
    "- そのクラスターの数で分類器を作成する\n",
    "- あとは、`fit_predict()` で学習し、予測させる\n",
    "- 正確には以下のようになる\n",
    "    - `fit(X)`\n",
    "        - 与えられたデータXに対してk-meansアルゴリズムを実行し、クラスタ中心を学習する\n",
    "        - 学習後、クラスタ中心の情報がモデルに保存されるが、各データポイントのクラスタ割り当ては返さない\n",
    "    - `fit_predict(X)`\n",
    "        - `fit(X)` を実行し、その後すぐに各データポイントのクラスタ割り当てを返す\n",
    "        - 学習と予測を1ステップで行いたい場合に便利\n",
    "    - `predict(X)`\n",
    "        - 既に学習済みのモデル（`fit()`または`fit_predict()`で学習済み）に対して新しいデータXのクラスタ割り当てを予測する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-means クラスタリングの実行\n",
    "# クラスターの数を定義\n",
    "n_clusters = 3\n",
    "\n",
    "# KMeans関数で kmeans オブジェクトを生成\n",
    "# 今後のセクションと混じらないように、kmeans_score とする\n",
    "# n_init='auto' はデフォルト。記載しないとWarningが出るのでその抑止のため\n",
    "kmeans_score = KMeans(n_init='auto', n_clusters=n_clusters, random_state=42)\n",
    "\n",
    "# データを入れて学習させる\n",
    "clusters_score = kmeans_score.fit_predict(scores_standardized)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- kmeansオブジェクトの属性から情報を抽出する\n",
    "  - `cluster_centers_`: クラスタの中心 (セントロイド) の座標\n",
    "  - `labels_`: 各データがどれに分類されたかを含む配列\n",
    "  - `inertia_`: クラスタ内のデータとデータの中心の距離の二乗の総和\n",
    "    - inertia_ が小さいほど、各データがそのクラスタの中心に近いことを意味"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# セントロイドの座標\n",
    "np.round(kmeans_score.cluster_centers_,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# クラスタリングで分類された結果\n",
    "# 上の clusters_score = kmeans.fit_predict(scores_standardized) の clusters_scoreと同じ内容\n",
    "kmeans_score.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 各クラスターに所属するデータとクラスター中心の距離の二乗の総和\n",
    "np.round(kmeans_score.inertia_,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- シルエットスコアを計算する。\n",
    "  - クラスタリングの品質を評価するための指標の1つ\n",
    "  - データポイントがどれだけ自身のクラスタに適合し、他のクラスタとは分離されているかを測定\n",
    "  - スコアの範囲: -1から1の間の値を取る\n",
    "    - 1に近い: クラスタリングが良好\n",
    "    - 0に近い: クラスタ間の重なりがある\n",
    "    - -1に近い: データポイントが誤ったクラスタに割り当てられている可能性が高い\n",
    "- 計算方法: 各データポイントについて、(a)同じクラスタ内の他のポイントとの平均距離と、(b)最も近い他のクラスタのポイントとの平均距離を比較する\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# シルエットスコアを計算\n",
    "silhouette = silhouette_score(scores_standardized, clusters_score)\n",
    "\n",
    "print(f'シルエットスコア: {silhouette: .2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.4. 結果の可視化\n",
    "\n",
    "- 各クラスタを色分けし、クラスタ中心を X で示す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 結果の可視化\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(scores_standardized[:, 0], scores_standardized[:, 1], c=kmeans_score.labels_, cmap='winter')\n",
    "plt.scatter(kmeans_score.cluster_centers_[:, 0], kmeans_score.cluster_centers_[:, 1], \n",
    "            marker='x', s=200, linewidths=3, color='r', zorder=10)\n",
    "plt.title('K-means Clustering (3 clusters)')\n",
    "plt.xlabel('Standardized Exam 1')\n",
    "plt.ylabel('Standardized Exam 2')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.5. クラスター数の設定\n",
    "\n",
    "- 上の例ではクラスター数は 3 であることが自明だが、そうはうまくいかない場合もままある\n",
    "- その時に、クラスター数を1ずつ増やし、`kmeans.inertia_`を計算し、その推移を観察する\n",
    "- グラフはたいていの場合、急激に減少した後に緩やかになる\"肘 elbow\"の形状となる\n",
    "- この肘の位置が適切なクラスタ数である可能性があり、これをエルボー法という"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# エルボー法による適切なクラスター数の探索\n",
    "inertias = []\n",
    "k_range = range(1, 11)\n",
    "\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(n_init='auto', n_clusters=k, random_state=42)\n",
    "    kmeans.fit(scores_standardized)\n",
    "    inertias.append(kmeans.inertia_)\n",
    "\n",
    "plt.plot(k_range, inertias, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('Elbow Method For Optimal k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. 乳がんデータセットによるk-meansクラスタリング\n",
    "\n",
    "- 乳がんデータセットは良性・悪性がわかっている\n",
    "- 2つのクラスターにわけられるかトライする"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.1. パッケージのインポート\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.2. データの読み込みと前処理\n",
    "- 今日の前半で使用した breast_cancer_data.xlsx を読み込む\n",
    "- 正規化を行う"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# このセクションだけ実行しても大丈夫なようにファイルがない場合はダウンロードする\n",
    "![[ -f breast_cancer_data.xlsx ]] || wget https://raw.githubusercontent.com/kytk/AI-MAILs/main/data/breast_cancer_data.xlsx\n",
    "\n",
    "df = pd.read_excel('breast_cancer_data.xlsx')\n",
    "\n",
    "# target 列は削除\n",
    "df_data = df.drop('target', axis=1)\n",
    "\n",
    "# df_data の列名を 取り出したものを feature_names とする\n",
    "feature_names = df_data.columns\n",
    "\n",
    "# NumPy配列に変換\n",
    "data = df_data.to_numpy()\n",
    "\n",
    "# target は target としてnumpyに変換しておく\n",
    "target = df['target'].to_numpy()\n",
    "\n",
    "# データの正規化\n",
    "\n",
    "# MinMaxScaler() 関数を使用して標準化を行うオブジェクト scaler を生成\n",
    "scaler = MinMaxScaler()  \n",
    "# 正規化するためのパラメータを計算(fit)し、適用(transform)する\n",
    "data_normalized = scaler.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.3. k-means クラスタリングの適用\n",
    "- KMeans() 関数に、自分が分類したいクラスター数を指定し、kmeans という分類器を生成する\n",
    "- 今は、良性と悪性で2つのクラスターができるはずという思いから、2 とする\n",
    "- `kmeans.fit_predict()` で実際の分類を行う\n",
    "- シルエットスコアを計算する。シルエットスコアは -1から1 の範囲で、高いほどよいクラスタリングを示す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KMeans() を使って分類器のオブジェクトを生成\n",
    "kmeans_cancer = KMeans(n_init='auto', n_clusters=2, random_state=42)\n",
    "\n",
    "# 実際にデータを入れて分類\n",
    "clusters_cancer = kmeans_cancer.fit_predict(data_normalized)\n",
    "\n",
    "# シルエットスコアを計算\n",
    "silhouette = silhouette_score(data_normalized, clusters_cancer)\n",
    "\n",
    "print(f'シルエットスコア: {silhouette: .2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.4. 元の特徴量の寄与度を調べる\n",
    "\n",
    "##### 7.4.1. クラスターの中心と特徴量の関係を調べる\n",
    "- 以下の関数でクラスターの中心と特徴量の関係をグラフにできる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cluster_centers(kmeans, feature_names):\n",
    "    centers = kmeans.cluster_centers_\n",
    "    feature_importance = np.abs(centers[0] - centers[1])\n",
    "    sorted_idx = np.argsort(feature_importance)\n",
    "    pos = np.arange(sorted_idx.shape[0]) + .5\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.barh(pos, feature_importance[sorted_idx], align='center')\n",
    "    plt.yticks(pos, np.array(feature_names)[sorted_idx])\n",
    "    plt.xlabel('Absolute difference between cluster centers')\n",
    "    plt.title('Feature Importance in K-means Clustering')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_cluster_centers(kmeans_cancer, feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- kmeans.cluster_centers_ を改めて見てみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# この配列の形状を確認する\n",
    "kmeans_cancer.cluster_centers_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.round(kmeans_cancer.cluster_centers_,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- これは、特徴量ごとの、クラスター1とクラスター2の中心を示している\n",
    "- ある特徴量において、クラスター1とクラスター2の中心の距離が離れていたら、それは、その特徴量はクラスターを分けるのに大きく寄与しているということになる\n",
    "- 今の場合、\"concavity\" と \"perimeter\" あたりのパラメーターが大きく寄与している\n",
    "- \"mean concavity\" と \"mean perimeter\" のふたつでグラフを描いてみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'mean concavity' と 'mean perimeter' のインデックスを取得\n",
    "concavity_idx = np.where(feature_names == 'mean concavity')[0][0]\n",
    "perimeter_idx = np.where(feature_names == 'mean perimeter')[0][0]\n",
    "\n",
    "# 散布図の作成\n",
    "plt.figure(figsize=(8, 6))\n",
    "scatter = plt.scatter(data_normalized[:, concavity_idx], data_normalized[:, perimeter_idx], \n",
    "                      c=kmeans_cancer.labels_, cmap='winter', alpha=0.7)\n",
    "\n",
    "plt.xlabel('Mean Concavity (standardized)')\n",
    "plt.ylabel('Mean Perimeter (standardized)')\n",
    "plt.title('Scatter Plot: Mean Concavity vs Mean Perimeter')\n",
    "\n",
    "# クラスターの中心をプロット\n",
    "centers = kmeans_cancer.cluster_centers_\n",
    "plt.scatter(centers[:, concavity_idx], centers[:, perimeter_idx], \n",
    "            c='red', s=200, alpha=0.8, marker='X', label='Cluster Centers')\n",
    "\n",
    "plt.legend()\n",
    "plt.colorbar(scatter, label='Cluster')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# クラスターごとの統計情報を表示\n",
    "for i in range(2):\n",
    "    cluster_points = data_normalized[kmeans_cancer.labels_ == i]\n",
    "    print(f\"Cluster {i}:\")\n",
    "    print(f\"  Mean Concavity: {np.mean(cluster_points[:, concavity_idx]):.2f}\")\n",
    "    print(f\"  Mean Perimeter: {np.mean(cluster_points[:, perimeter_idx]):.2f}\")\n",
    "    print(f\"  Number of points: {len(cluster_points)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.5. 良性、悪性とクラスター分割の一致\n",
    "- kmeans クラスタリングの結果と良性、悪性の診断がどの程度一致するかを見てみる\n",
    "- kmeans クラスタリングはあくまでもクラスタリングであり、良性、悪性にわけるためのものではないことに注意\n",
    "- 調整ランド指数 (adjusted rand index; ARI) を使用する\n",
    "    - ARIは-1から1の範囲を取り、1に近いほどクラスタリング結果が実際のラベルと一致していることを示す\n",
    "    - 0は偶然の一致を意味し、負の値は偶然より悪い結果を示す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn.metrics から adjusted_rand_score をインポート\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "\n",
    "# 良性、悪性の値が入っている target と\n",
    "# 今回のクラスタリングの結果が入っている clusters を比較する\n",
    "ari = adjusted_rand_score(target, clusters_cancer)\n",
    "print(f\"\\n調整ランド指数（ARI）: {ari:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.6. 乳がんデータセットのクラスター数の探索\n",
    "\n",
    "- 乳がんデータセットは良性か悪性かの2つに分かれているが、実際はクラスターはもう少しあるかもしれない\n",
    "- エルボー法で探索してみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# エルボー法による適切なクラスター数の探索\n",
    "inertias = []\n",
    "k_range = range(1, 11)\n",
    "\n",
    "for k in k_range:\n",
    "    kmeans_cancer_explore = KMeans(n_init='auto', n_clusters=k, random_state=42)\n",
    "    kmeans_cancer_explore.fit(data_normalized)\n",
    "    inertias.append(kmeans_cancer_explore.inertia_)\n",
    "\n",
    "plt.plot(k_range, inertias, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('Elbow Method For Optimal k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 2 or 3 程度と考えられる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.7 K-meansクラスタリングのクロスバリデーション\n",
    "\n",
    "- クロスバリデーションは通常、教師あり学習で使用されるが、K-meansクラスタリングのような教師なし学習にも応用できる\n",
    "- クロスバリデーションの目的: クラスタリング結果の安定性と信頼性を評価する\n",
    "\n",
    "#### K-meansクラスタリングでのクロスバリデーションの手順：\n",
    "\n",
    "1. データセットを複数の部分集合（フォールド）に分割する。\n",
    "2. 各フォールドを順番にテストセットとして使用し、残りをトレーニングセットとする。\n",
    "3. トレーニングセットでK-meansを実行し、モデルを構築する。\n",
    "4. 構築したモデルをテストセットに適用し、クラスタリング結果を評価する。\n",
    "5. 全フォールドで手順3-4を繰り返し、結果の平均を取る。\n",
    "\n",
    "- 評価指標としては、シルエットスコアやクラスタ内分散などを使用できる。これにより、クラスタリング結果が特定のデータ分割に依存せず、一貫性があることを確認できる\n",
    "\n",
    "- 注意点：K-meansは初期中心点によって結果が変わる可能性があるため、各フォールドで複数回実行し、最良の結果を使用することも検討する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# クロスバリデーションを実行\n",
    "# 視覚化するために、まず、PCAを実行し、2次元に次元削減する\n",
    "# その後、データを5分割し、クロスバリデーションを行う\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def kmeans_cv_visualization(data, n_clusters=2, n_splits=5, random_state=42):\n",
    "    # KFoldでデータを分割するためのオブジェクト kf を生成する\n",
    "    #  n_splits: 分割する個数\n",
    "    #  shuffle=True: 分割する前にデータをシャッフルする\n",
    "    #  random_state: シャッフルする際の乱数表をどれを使うか\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "    fig, axs = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    fig.suptitle(f'K-means Clustering Cross-Validation and Whole data (n_clusters={n_clusters})', fontsize=16)\n",
    "\n",
    "    # PCAを適用してデータを2次元に削減\n",
    "    pca = PCA(n_components=2)\n",
    "    data_2d = pca.fit_transform(data)\n",
    "\n",
    "    # kf.split(data) で、データのインデックスが分割される\n",
    "    # enumerate() 関数は、分割の反復回数（0から始まる）と、\n",
    "    # 各分割で得られる（train_index, test_index）のタプルを返す\n",
    "    # 分割の反復回数が for の中の変数 i に代入される\n",
    "    # 今回は5分割されるので、5分の4がモデルの学習に使われ、\n",
    "    # 5分の1のテストデータに対し、モデルが適用される\n",
    "    for i, (train_index, test_index) in enumerate(kf.split(data)):\n",
    "        if i < 5:  # 最初の5つのサブプロットはクロスバリデーション用\n",
    "            # KMeansのオブジェクトを生成\n",
    "            kfold_kmeans = KMeans(n_init='auto', n_clusters=n_clusters, random_state=random_state)\n",
    "            # 訓練データに対して、KMeansでモデルを学習\n",
    "            kfold_kmeans.fit(data[train_index])\n",
    "            # テストデータに対して、モデルを適用し、クラスターを予測\n",
    "            test_labels = kfold_kmeans.predict(data[test_index])\n",
    "\n",
    "            # テストデータのプロット\n",
    "            ax = axs[i//3, i%3]\n",
    "            scatter = ax.scatter(data_2d[test_index, 0], data_2d[test_index, 1], c=test_labels, cmap='winter')\n",
    "            ax.set_title(f'Fold {i+1}')\n",
    "            ax.set_xlabel('First Principal Component')\n",
    "            ax.set_ylabel('Second Principal Component')\n",
    "            ax.set_xlim(-1.2,2.5)\n",
    "            ax.set_ylim(-1.2,1.5)\n",
    "\n",
    "            # クラスターの中心をプロット\n",
    "            centers_2d = pca.transform(kfold_kmeans.cluster_centers_)\n",
    "            ax.scatter(centers_2d[:, 0], centers_2d[:, 1], c='red', marker='x', s=200, linewidths=3)\n",
    "\n",
    "            # シルエットスコアを計算\n",
    "            score = silhouette_score(data[test_index], test_labels)\n",
    "            ax.text(0.05, 0.95, f'Silhouette Score: {score:.3f}', transform=ax.transAxes, verticalalignment='top')\n",
    "\n",
    "    # 6番目のサブプロットに全データを使用した時のクラスタリング結果を表示\n",
    "    ax = axs[1, 2]\n",
    "    whole_kmeans = KMeans(n_init='auto', n_clusters=n_clusters, random_state=random_state)\n",
    "    whole_labels = whole_kmeans.fit_predict(data)\n",
    "    scatter = ax.scatter(data_2d[:, 0], data_2d[:, 1], c=whole_labels, cmap='winter')\n",
    "    ax.set_title('Whole Data')\n",
    "    ax.set_xlabel('First Principal Component')\n",
    "    ax.set_ylabel('Second Principal Component')\n",
    "    ax.set_xlim(-1.2,2.5)\n",
    "    ax.set_ylim(-1.2,1.5)\n",
    "\n",
    "\n",
    "    # 全データを使った時のクラスターの中心をプロット\n",
    "    whole_centers_2d = pca.transform(whole_kmeans.cluster_centers_)\n",
    "    ax.scatter(whole_centers_2d[:, 0], whole_centers_2d[:, 1], c='red', marker='x', s=200, linewidths=3)\n",
    "\n",
    "\n",
    "    # 全データを使った時のシルエットスコアを計算\n",
    "    whole_score = silhouette_score(data, whole_labels)\n",
    "    ax.text(0.05, 0.95, f'Silhouette Score: {whole_score:.3f}', transform=ax.transAxes, verticalalignment='top')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return whole_score, whole_labels\n",
    "\n",
    "# n_clusters=2 でのクロスバリデーションと全データのクラスタリングの視覚化\n",
    "whole_score, whole_labels = kmeans_cv_visualization(data_normalized, n_clusters=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- クロスバリデーションと全データを使ったクラスタリングの結果の解釈:\n",
    "    - 最初の5つのサブプロットは、5分割交差検証の各フォールドでのテストデータのクラスタリング結果\n",
    "    - 6番目のサブプロットは、全データを使用したクラスタリング結果\n",
    "    - 点の色はクラスターの割り当てを、赤い×印はクラスターの中心\n",
    "    - 各プロットのシルエットスコアにより、分類の質を評価できる\n",
    "    - クロスバリデーションの結果と全データの結果を比較することで、モデルの安定性と一般化能力を評価できる\n",
    "    - 全データのシルエットスコア（0.385）は、モデル全体の性能を示している\n",
    "    - クラスターが明確に分離されているほど、そのクラスター数がデータに適していることを示唆している\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### クロスバリデーションを応用したクラスター数の推測\n",
    "- クロスバリデーションで得られるシルエットスコアから平均と標準偏差をとって、最適なクラスター数を推測する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "def kmeans_cross_validation(data, n_clusters, n_splits=5, n_init=10, random_state=42):\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "    silhouette_scores = []\n",
    "\n",
    "    for train_index, test_index in kf.split(data):\n",
    "        kmeans = KMeans(n_clusters=n_clusters, n_init=n_init, random_state=random_state)\n",
    "        kmeans.fit(data[train_index])\n",
    "        test_labels = kmeans.predict(data[test_index])\n",
    "        score = silhouette_score(data[test_index], test_labels)\n",
    "        silhouette_scores.append(score)\n",
    "\n",
    "    return np.mean(silhouette_scores), np.std(silhouette_scores)\n",
    "\n",
    "# クラスター数の範囲を設定\n",
    "cluster_range = range(2, 11)\n",
    "\n",
    "# 各クラスター数でクロスバリデーションを実行\n",
    "cv_scores = []\n",
    "for n_clusters in cluster_range:\n",
    "    mean_score, std_score = kmeans_cross_validation(data_normalized, n_clusters)\n",
    "    cv_scores.append((mean_score, std_score))\n",
    "    print(f\"Number of clusters {n_clusters}: Mean silhouette score = {mean_score:.3f} (±{std_score:.3f})\")\n",
    "\n",
    "# 結果をプロット\n",
    "means, stds = zip(*cv_scores)\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.errorbar(cluster_range, means, yerr=stds, fmt='o-', capsize=5)\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Mean Silhouette Score')\n",
    "plt.title('K-means Clustering Cross-Validation Results')\n",
    "plt.show()\n",
    "\n",
    "# 最適なクラスター数を特定\n",
    "# 平均シルエットスコアが最大のものを選択\n",
    "optimal_clusters = cluster_range[np.argmax(means)]\n",
    "print(f\"\\nOptimal number of clusters: {optimal_clusters}\")\n",
    "print(f\"Highest mean silhouette score: {max(means):.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 結果の解釈\n",
    "    - シルエットスコアが最も高いクラスター数が、データに最適なクラスター数と考えられる\n",
    "    - エラーバーは各クラスター数での結果の安定性を示している。エラーバーが小さいほど、結果が安定している\n",
    "    - クラスター数が増えるにつれてスコアが低下する場合、クラスター数は小さいことを示唆している\n",
    "    - 最適なクラスター数が2に近い場合、これは元々の良性/悪性の二分類と一致する可能性がある\n",
    "    - ただし、クラスター数の選択は常に問題の文脈や目的に応じて行う必要がある"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### おわりに\n",
    "- 以上、Pythonを使った医療データの解析の基本を一通りカバーしました\n",
    "  - Pythonの基礎\n",
    "  - Pandas, NumPy, Seaborn, Matplotlib\n",
    "  - 機械学習\n",
    "  - 深層学習\n",
    "- 私自身、学びながらのところもたくさんありましたが、基本をおさえることで、応用しやすくなると思います\n",
    "- 皆様が少しでもこのような領域に興味・関心を持っていただけたら、準備した甲斐があります\n",
    "- 様々なご意見もありがとうございました。最後の感想もぜひお聞かせください\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (参考) 主成分分析で使用したデータの生成方法\n",
    "\n",
    "- 150人の仮想の試験結果を生成\n",
    "- 理系科目が得意な75人と文系科目が得意な75人を想定\n",
    "- 5教科 (英語、数学、理科、国語、社会) を乱数で40-75点で生成\n",
    "- 全員に対して、英語は乱数で5-20点をかさ増し\n",
    "- 理系科目が得意な75人に対して、数学、理科を5-20点かさ増し\n",
    "- 文系科目が得意な75人に対して、国語、社会を5-20点かさ増し\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 主成分分析で使用したテストの成績の生成\n",
    "\n",
    "# 乱数のシードを設定して再現性を確保\n",
    "np.random.seed(42)\n",
    "\n",
    "# 初期化\n",
    "scores = []\n",
    "\n",
    "# 生徒の数\n",
    "n_students = 150\n",
    "\n",
    "# 理系群と文系群の生徒数（ほぼ半々に）\n",
    "# 理系群は生徒の数を2で割った時の商\n",
    "# 文系群は 全生徒の数 - 理系生徒の数\n",
    "n_science = n_students // 2\n",
    "n_humanities = n_students - n_science\n",
    "\n",
    "# 基本的な成績の生成（全科目）\n",
    "scores = np.random.randint(40, 76, size=(n_students, 5))\n",
    "\n",
    "# 英語の成績調整\n",
    "scores[:, 0] += np.random.randint(5, 20, n_students)\n",
    "\n",
    "# 理系群の成績調整\n",
    "scores[:n_science, 1] += np.random.randint(5, 20, n_science)  # 数学\n",
    "scores[:n_science, 2] += np.random.randint(5, 20, n_science)  # 理科\n",
    "\n",
    "# 文系群の成績調整\n",
    "scores[n_science:, 3] += np.random.randint(5, 20, n_humanities)  # 国語\n",
    "scores[n_science:, 4] += np.random.randint(5, 20, n_humanities)  # 社会\n",
    "\n",
    "# データフレームの作成\n",
    "df = pd.DataFrame(scores, columns=['English', 'Math', 'Science', 'Japanese', 'Social_studies'])\n",
    "\n",
    "# 実際の群（理系/文系）のラベルを追加\n",
    "df['group'] = ['science' if i < n_science else 'humanities' for i in range(n_students)]\n",
    "\n",
    "# データの保存\n",
    "df.to_csv('student_scores.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
