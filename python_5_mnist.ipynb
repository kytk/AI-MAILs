{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kytk/AI-MAILs/blob/main/python_5_mnist.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wq1rctd9cTZl"
      },
      "source": [
        "# 医療従事者のためのPython: 深層学習を使った手書き数字の分類\n",
        "\n",
        "根本清貴 (筑波大学医学医療系精神医学)\n",
        "\n",
        "Ver. 20240730"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FA01hVoUcTZo"
      },
      "source": [
        "## 本セクションの目標\n",
        "- MNISTデータセットを用いて手書き数字の分類を行うことで、どのように深層学習を実装するかを学ぶ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YoYcyBiBcTZp"
      },
      "source": [
        "## 目次\n",
        "1. MNISTデータセット\n",
        "2. Tensorflow/Keras を用いた深層学習の実装 (全結合層)\n",
        "3. 学習の可視化\n",
        "4. ハイパーパラメータ\n",
        "5. 畳み込みニューラルネットワーク"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCtPaGjTcTZp"
      },
      "source": [
        "## 1. MNISTデータセット\n",
        "- NIST (National Institute of Standards and Technology) が保有していたデータセットを再構成したデータベース\n",
        "- 60,000枚の訓練用画像と10,000枚の評価用画像が含まれている\n",
        "\n",
        "| <img src=\"https://www.nemotos.net/nb/img/MnistExamples.png\" width=\"500\"> |\n",
        "| --: |\n",
        "| Wikipediaより引用 |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SwiyJfcAcTZq"
      },
      "source": [
        "## 2. Tensorflow/Keras を用いた深層学習の実装 (全結合層)\n",
        "- 深層学習を実装する手順は以下となる\n",
        "\n",
        "| <img src=\"https://www.nemotos.net/nb/img/dl_flow.png\" width=\"500\"> |\n",
        "| --: |\n",
        "| 動かしながら学ぶPyTorchプログラミング入門より引用 |\n",
        "\n",
        "- この流れに従っていく\n",
        "\n",
        "- まずは、基本である「全結合層」のみを用いた識別を行い、その後、「畳みこみニューラルネットワーク」を用いた識別を行う"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3V7m5YWcTZq"
      },
      "source": [
        "### 2-1. 必要なパッケージのインポート\n",
        "- 今回必要なパッケージは以下\n",
        "    - numpy: 行列計算\n",
        "    - matplotlib: グラフの描画\n",
        "    - tensorflow: 深層学習のフレームワーク\n",
        "    - keras: tensorflowのフロントエンド（簡単なコマンドでTensorflowに仕事をさせるためのプログラム）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lj68M5PUcTZr"
      },
      "outputs": [],
      "source": [
        "# 必要なパッケージ、モジュールのインポート\n",
        "\n",
        "# NumPy\n",
        "import numpy as np\n",
        "\n",
        "# Matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# keras\n",
        "from tensorflow import keras\n",
        "\n",
        "# layers と Sequential (層　(layers) を定義し、層を順番 (sequential) につなげるために使用)\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "# ラベルを one-hotベクトルに変換する関数 to_categorical()\n",
        "from tensorflow.keras.utils import to_categorical"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0BWLynrcTZs"
      },
      "source": [
        "### 2-2. データの前処理\n",
        "- MNISTの画像データはひとつひとつのピクセルの値が0-255の値をとる\n",
        "- これを0-1の値をとるように変換する\n",
        "\n",
        "#### 2-2-1. データの読み込みと確認"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tbpCnqKscTZt"
      },
      "outputs": [],
      "source": [
        "# tensorflow パッケージの中に mnist データセットが既に入っているため、それを利用する\n",
        "from tensorflow.keras.datasets import mnist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8xoAHwMCaHx7",
        "outputId": "3ff63ec6-1b77-4216-f63d-45ce009d6c08"
      },
      "outputs": [],
      "source": [
        "# mnist.load_data() でデータセットを読み込むことができる\n",
        "# どのようなデータか確認してみる\n",
        "# 変数 dataset を作成して代入する\n",
        "\n",
        "dataset = mnist.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N6ZBD1Vvc6l_",
        "outputId": "c59dace2-f26a-4991-e3e0-62a3369cd59d"
      },
      "outputs": [],
      "source": [
        "# dataset のデータ型を確認する\n",
        "# type(dataset) で dataset のデータ型が確認できる\n",
        "# データ型はタプル\n",
        "type(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YsjUjgDqaHx7",
        "outputId": "b1b8188a-b614-4957-a5b9-8a7518e41837"
      },
      "outputs": [],
      "source": [
        "# dataset を実際に確認してみる\n",
        "# よくみると\n",
        "\n",
        "# ( (array画像, array数字), (array画像, array数字) )\n",
        "\n",
        "# となっている\n",
        "# (array画像, array数字) がかたまりとして1つで、2つの要素がある\n",
        "\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qXPPho84aHx7",
        "outputId": "a8f3dfad-878b-46a1-ba8b-3b4566ab2d0a"
      },
      "outputs": [],
      "source": [
        "#タプルの場合、len で中に入る要素数を確認できる\n",
        "# 今は2\n",
        "# (array画像, array数字) のかたまりが 2つあるという意味\n",
        "len(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dkQ4nai-c6Ig"
      },
      "outputs": [],
      "source": [
        "# 要素は 2 とわかったので、data0 と data1 にそれぞれ代入してみる\n",
        "# タプルもインデックスを使って要素を取り出せる\n",
        "# Python では、以下のように一行で2つの変数に代入できる\n",
        "# (data0, data1 = dataset でも実は大丈夫)\n",
        "data0, data1 = dataset[0], dataset[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQ4L3OqIdp3i",
        "outputId": "bcac3a38-be70-429a-df1f-91bccaa075a8"
      },
      "outputs": [],
      "source": [
        "# data0, data1 についてもそれぞれ、type と len を使ってみる\n",
        "print(f'data0: type {type(data0)} length {len(data0)}')\n",
        "print(f'data1: type {type(data1)} length {len(data1)}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RtLxKHbje5mv",
        "outputId": "ad92a0d3-27a3-4006-bdd4-3c24f0234160"
      },
      "outputs": [],
      "source": [
        "# data0 も data1 もタプルでさらに2つの要素があることがわかる\n",
        "# それぞれさらに data00, data01, data10, data11 に代入してみる\n",
        "data00, data01 = data0\n",
        "data10, data11 = data1\n",
        "\n",
        "print(f'data00: type {type(data00)} length {type(data00)}')\n",
        "print(f'data01: type {type(data01)} length {type(data01)}')\n",
        "print(f'data10: type {type(data10)} length {type(data10)}')\n",
        "print(f'data11: type {type(data11)} length {type(data11)}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTgEwKwKiWBH"
      },
      "source": [
        "- 今の作業でわかったこと\n",
        "- data は 2つのタプルで構成\n",
        "    - 各々のタプルはそれぞれ2つのNumpy配列で構成\n",
        "\n",
        "- mnist.load_data() のデータ構成\n",
        "    - 次のリンクに説明あり https://keras.io/ja/datasets/#mnist\n",
        "    - data00: 60000枚の手書き画像をNumpy配列に変換したもの → 学習のための画像\n",
        "    - data01: 60000枚の手書き画像の正解データ（数字）→ 学習のためのラベル\n",
        "    - data10: 10000枚の手書き画像をNumpy配列に変換したもの → テストのための画像\n",
        "    - data11: 10000枚の手書き画像の正解データ（数字） → テストのためのラベル\n",
        "\n",
        "- 情報は改ざんされたくないので、タプルに入れる(タプルに入った内容は変更できない)\n",
        "\n",
        "- これからデータ解析のために、改めて変数名をつけていく\n",
        "\n",
        "- data00 は train_images, data01 は train_labels という名前にする\n",
        "    - (train_images, train_labels) でひとつのタプルができる\n",
        "\n",
        "- data10 は test_images, data11 は test_labels という名前にする\n",
        "    - (test_images, test_labels) でもうひとつのタプルができる\n",
        "\n",
        "- これらをまとめて、data というタプルができる\n",
        "- **data = ((train_images, train_labels), (test_images, test_labels))**\n",
        "\n",
        "- 結論: もらったデータについて知りたかったら、type 関数 と len 関数を使うことからはじめるとよいでしょう"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JIb5aBq0cTZt"
      },
      "outputs": [],
      "source": [
        "# mnist.load_data() を訓練データとテストデータにわけて格納する\n",
        "# 様々な参考書はここからいきなり始まる(上はそれの説明だった)\n",
        "\n",
        "# mnistは、訓練データとテストデータがそれぞれタプルにわかれて入っている\n",
        "# 訓練データの画像を train_images, 正解ラベルを train_labels に格納する\n",
        "# テストデータも同じ\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6niJKGkScTZu",
        "outputId": "e624f0b8-d23d-40a8-a01b-e327a456545c"
      },
      "outputs": [],
      "source": [
        "# train_images について確認する\n",
        "# まず、型から確認\n",
        "# numpy.ndarray型\n",
        "type(train_images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FuwhQihBcTZu",
        "outputId": "36d20eb0-2e74-4be3-d438-5a6e6336f225"
      },
      "outputs": [],
      "source": [
        "# shape\n",
        "# 60000枚の画像、1枚の画像は 28 x 28 で構成\n",
        "train_images.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        },
        "id": "cNBe6385cTZu",
        "outputId": "a79827ad-1cf2-4180-acfe-67995fb8e09f"
      },
      "outputs": [],
      "source": [
        "# plt.imshow() を使って実際の画像を確認\n",
        "# for を使って、最初の3人分のデータを見る\n",
        "for i in range(3):\n",
        "    plt.figure(figsize=(0.5,0.5)) #グラフのサイズを決定 0.5,0.5 は 0.5インチ x 0.5インチ\n",
        "    plt.imshow(train_images[i], cmap='gray')\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OtJ5I-DwcTZv",
        "outputId": "9163301d-6c9d-41ac-cabb-12b424eecafb"
      },
      "outputs": [],
      "source": [
        "# train_labels の内容を確認\n",
        "# スライシングで train_labels の最初の3つのラベルを取り出す\n",
        "# 画像とラベルが一致していることを確認\n",
        "train_labels[0:3] # train_labels の0番目以上、3番目未満を抽出"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ahoD72qOcTZv",
        "outputId": "a46e45de-4c4e-433b-c5e9-8813ca090d39"
      },
      "outputs": [],
      "source": [
        "# train_labels の shape を確認\n",
        "# 60000 のデータがある1次元のデータ\n",
        "train_labels.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sGZs2lzxcTZv",
        "outputId": "db72c016-81f2-4777-bd65-e1e503460fc3"
      },
      "outputs": [],
      "source": [
        "# 同様にテストデータも確認\n",
        "# shape\n",
        "# 10,000枚の画像、1枚の画像は 28 x 28 で構成\n",
        "test_images.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        },
        "id": "D1QeVihlcTZw",
        "outputId": "f07c9f4a-4e5b-468c-e1d2-03ee7a6aa605",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# plt.imshow() を使って実際の画像を確認\n",
        "# for を使って、最初の3人分のデータを見る\n",
        "for i in range(3):\n",
        "    plt.figure(figsize=(0.5,0.5))\n",
        "    plt.imshow(test_images[i], cmap='gray')\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u_e2FUqtcTZw",
        "outputId": "54a2d2b7-1e40-438d-9ac1-286c3b9ebf85"
      },
      "outputs": [],
      "source": [
        "# test_labels の内容を確認\n",
        "# スライシングで test_labels の最初の3つのラベルを取り出す\n",
        "test_labels[0:3]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05kNQ_F7cTZw"
      },
      "source": [
        "#### 2-2-2. 正解ラベルの one-hotベクトル化\n",
        "- 正解ラベルを one-hotベクトルに変換する\n",
        "- `to_categorical()` 関数を使うことで変換できる"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hh54HJtxcTZx"
      },
      "outputs": [],
      "source": [
        "# train_labels を one-hotベクトルに変換\n",
        "train_labels = to_categorical(train_labels)\n",
        "\n",
        "# test_labels も同様に one-hotベクトルに変換\n",
        "test_labels = to_categorical(test_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tO9pRIMwcTZx",
        "outputId": "2bea5ee1-e1d0-40c4-8018-0136b08b8282"
      },
      "outputs": [],
      "source": [
        "# 新しい train_labels の shape を確認\n",
        "# 60000行10列の行列になっている\n",
        "train_labels.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQJ_v00hcTZx",
        "outputId": "1ee20a85-e10d-4c53-97f8-d6c40d58bc3d"
      },
      "outputs": [],
      "source": [
        "# train_labels の 最初の3行を見てみる\n",
        "# 正解 5, 0, 4 に相当するところが 1 になっていることに着目\n",
        "train_labels[0:3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fSUppKDNcTZx",
        "outputId": "91fd06b1-b413-4810-d8ce-4cd16020caa1"
      },
      "outputs": [],
      "source": [
        "# test_labels の 最初の3行も確認する\n",
        "# 正解 7, 2, 1 に相当するところが 1 になっていることに着目\n",
        "test_labels[0:3]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTfsnHAdcTZy"
      },
      "source": [
        "#### 2-2-3. データの正規化\n",
        "- 深層学習に限らず、データ解析においてデータの範囲をある決まった範囲に変換することを正規化という\n",
        "- 正規化を行うことで、異なる変数がモデルに与える影響を均等にできる\n",
        "    - 例: 年齢 (20-80) と 身長 (140-200)\n",
        "- 今、画像は 0-255 の整数をとるので、これを 0-1 になるように変換する"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x9_IsX8lcTZy",
        "outputId": "822a791b-5576-4f55-b2c5-20b225cf259c"
      },
      "outputs": [],
      "source": [
        "# dtype 属性で numpy配列内の数字のデータ型がわかる\n",
        "# uint8 は unsigned integer 8bit 符号なし 8bit 整数 (0-255)\n",
        "train_images.dtype"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ErcycXascTZy",
        "outputId": "af0c4fc3-9073-4809-ba9c-6666331a1054"
      },
      "outputs": [],
      "source": [
        "# train_images の最小値と最大値\n",
        "# min() メソッドとmax() メソッドを使えばよい\n",
        "print(f'min: {train_images.min()}')\n",
        "print(f'max: {train_images.max()}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xETMaTPkcTZy"
      },
      "outputs": [],
      "source": [
        "# 0-255で構成されるので、255で割れば、値は 0-1 の間となる\n",
        "# 255.0 と小数点をつけて割ることで、Pythonは出力を必ず float型としてくれる\n",
        "# 計算結果を同じ変数名にいれることで、変数を増やすことなく、正規化されたデータに変数の内容が置き換わる\n",
        "train_images = train_images / 255.0\n",
        "test_images = test_images / 255.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "abjPEhHjcTZz",
        "outputId": "0801e14b-fb56-4b04-c728-6a3428dd2bd4"
      },
      "outputs": [],
      "source": [
        "# train_images の値が本当に0-1になったか確認\n",
        "# min() と max() を使えばよい\n",
        "# 最小値\n",
        "print(f'min: {train_images.min()}')\n",
        "print(f'max: {train_images.max()}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RlPJlTofcTZz",
        "outputId": "c01575e3-7899-49c1-f107-3044e55170e2"
      },
      "outputs": [],
      "source": [
        "# dtypeも確認する\n",
        "# 今回は float64 倍精度浮動小数点数\n",
        "train_images.dtype"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSEAzdjLcTZz"
      },
      "source": [
        "### 2-3. 訓練データとテストデータの作成\n",
        "- MNISTデータセットは手書き数字6万枚の訓練データセットと手書き数字1万枚のテストデータセットから構成されている\n",
        "- 「**訓練データ**」「**検証データ**」「**テストデータ**」の3つを準備する\n",
        "    - 訓練データ: ニューラルネットワークのパラメータを決めるためのデータ\n",
        "    - 検証データ: 訓練データで得られたパラメータがどの程度の精度があるかを検証するためのデータ\n",
        "    - テストデータ: ニューラルネットワークの汎用性を評価するためのデータ\n",
        "        - テストデータは、必ず訓練で使ったものと別のセットを使わないといけない\n",
        "        - このため、訓練データの一部を検証データにする\n",
        "- Tensorflow には、`model.fit()` メソッドに、`validation_split` という引数が準備されており、ここで訓練データの何割を検証データとして使用するかを設定できる\n",
        "    - 今回は訓練データの2割を検証データとして使用することとする (`validation_split=0.2`)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3Db_WEzcTZz"
      },
      "source": [
        "### 2-4. ニューラルネットワークの定義\n",
        "\n",
        "- ここで、自分がイメージするニューラルネットワークモデルを定義する\n",
        "- 下図の赤線の部分, すなわち **順伝播 forwad propagation** のモデルを構築\n",
        "\n",
        "<img src=\"https://www.nemotos.net/nb/img/dl_overview_4.png\" width=\"500\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDdi5MPNcTZ0"
      },
      "source": [
        "- 今は以下のように定義する\n",
        "    - 層と層の結合は全結合とする\n",
        "    - 第1層は画像が 28 x 28 で構成されているので、そのピクセル数(784)がユニット数 (数字のピクセルを1列に並べたイメージ)\n",
        "    - 第2層のユニット数は 128 とする\n",
        "    - 第2層の活性化関数は **ReLU**関数 とする\n",
        "    - 過学習を防ぐため、全結合層の2割は drop とする (Dropout=0.2)\n",
        "    - このモデルとしては、最終の出力は 0-9 の10のクラスを分類したい\n",
        "    - そのため、第3層は 出力層に渡す準備として、ユニット数は 10 とする\n",
        "    - 第3層の出力はそのまま出力層の入力とする (Tensorflow ではそのように構築することが勧められている)\n",
        "    - 出力層の活性化関数は多クラス分類に適した **Softmax**関数 とする"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gIeUFwscTZ0"
      },
      "source": [
        "- 活性化関数の特徴\n",
        "\n",
        "| 関数名 | 特徴 |\n",
        "| :-- | :-- |\n",
        "| ReLU | 隠れ層に使うことで、非線形問題を解くことができるようになる <br> Sigmoid関数は0-1の値しかとらないので層が厚くなるほど誤差が小さくなっていき、<br>入力層まで誤差が伝搬する前に誤差が消失するという勾配消失問題が発生する |\n",
        "| Sigmoid | 0-1の間の確率で表現可能なため2クラス分類の出力層に用いる |\n",
        "| Softmax | 各クラスの確率の総和が1となるように正規化された関数であるため多クラス分類の出力層に用いる |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GK-C6vBccTZ0"
      },
      "source": [
        "<img src=\"https://www.nemotos.net/nb/img/nn_model.png\" width=\"500\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-3-9B5_cTZ0"
      },
      "source": [
        "- これらはTensorflow/Kerasでは以下のように定義できる\n",
        "    - プリセットで準備されている **Sequential**モデル を選択する(Sequential: 連続する)\n",
        "    - 入力画像は **layers.Flatten** を使うことでベクトルにできる\n",
        "    - 全結合層は **layers.Dense** で規定できる"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KWdEybUUcTZ0"
      },
      "outputs": [],
      "source": [
        "# モデルを定義\n",
        "# Sequentialモデルを使用すると、model.addとすればどんどん付け加えられる\n",
        "# ここはあくまでもモデルを作っているだけなので、データはまだ入力していない。（計算式を作るイメージ。データの代入はこれから）\n",
        "\n",
        "model = Sequential()                                # Sequential() から model というオブジェクトを生成\n",
        "model.add(layers.Flatten(input_shape=(28, 28)))     # 入力画像の 配列の大きさ(dimension) を指定 今の場合は(28,28)\n",
        "model.add(layers.Dense(128, activation='relu'))     # 第2層のユニット数を 128 にし、 活性化関数は ReLU とする\n",
        "model.add(layers.Dropout(0.2))                      # 過学習防止のため、結合層の20%を dropout\n",
        "model.add(layers.Dense(10))                         # 第3層のユニット数を 10 にする ここは特に活性化関数はなし\n",
        "model.add(layers.Softmax())                         # 出力層でSoftmax関数 で処理して結果を出力する\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yUrJLJF2cTZ2",
        "outputId": "cd895a3b-b315-4471-858b-a817eb97818b"
      },
      "outputs": [],
      "source": [
        "# どのようなモデルになったかを model.summary() で知ることができる\n",
        "model.summary()\n",
        "# パラメータ数\n",
        "#   Flatten: 入力層なのでなし\n",
        "#   Dense 100480 ← 入力層 784 * 第2層 128 + 第2層のそれぞれのユニットに対する定数項 128\n",
        "#   Dense 1290 ← 第2層 128 * 第3層 10 + 第3層のそれぞれのユニットに対する定数項 10\n",
        "# 合計 101,770 ものパラメータをこれから学習させることになる\n",
        "# パラメータが多いため、パラメータ推定のために必要なデータ数が膨大となる"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yS2Ctql9rDxm",
        "outputId": "440cb9d5-62b3-4666-e6fb-34858c1325c4"
      },
      "outputs": [],
      "source": [
        "# 訓練データを今作ったモデルに代入して予測値を計算 (順伝播; forward propagation)\n",
        "predictions = model(train_images)\n",
        "# predictions の データ型を確認\n",
        "print(type(predictions))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ggIVpUlcTZ1",
        "outputId": "175bd041-8939-46ee-8010-f07fae71f8ff"
      },
      "outputs": [],
      "source": [
        "# tensorflow.python.framework.ops.EagerTensor はそのままでは扱いにくいのでNumpy配列型に変換する\n",
        "# predictions の後に .numpy() をつけることで、NumPy配列に変換できる\n",
        "predictions = predictions.numpy()\n",
        "# 型を確認しておく\n",
        "print(type(predictions))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mRNEmNvucTZ1",
        "outputId": "0b0cea89-2a9b-4e77-e673-c9144898ca9f",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# predictions を表示\n",
        "# この時点では、モデルを決めただけで、重みはランダムに割り当てられている\n",
        "# そのため、各クラスの確率はおおよそ 1/10 あたりになるはず\n",
        "# ひとつの数字の画像が1行、列が 0 - 9 の数字である確率\n",
        "# 一切学習はしていないことに注意！train_labelsはまだ使われていない\n",
        "\n",
        "# np.round() は np.round(a, 2) で a を小数点2桁で丸めている\n",
        "\n",
        "print(np.round(predictions, 2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8iu2aV1qcTZ1"
      },
      "source": [
        "### 2-5. 損失関数と最適化関数の定義\n",
        "- 次に損失関数と最適化関数(オプティマイザ)を決定する\n",
        "- その後、model.compile() で損失関数と最適化関数をモデルに組み込む\n",
        "- 下図の赤線の部分、すなわち **逆伝播 back propagation** のモデルを構築\n",
        "\n",
        "<img src=\"https://www.nemotos.net/nb/img/dl_overview_5.png\" width=\"500\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIbpugr9cTZ1"
      },
      "source": [
        "- よく用いられる損失関数\n",
        "\n",
        "| 目的 | 関数名 | Function Name | 損失関数名<br>(Tensorflow) | 損失関数名(PyTorch) |\n",
        "| :-- | :-- | :-- | :-- | :-- |\n",
        "| 回帰 | 平均二乗誤差 | Mean Squared Error | mean_squared_error | nn.MSELoss |\n",
        "| 2クラス分類 | バイナリ交差エントロピー | Binary Cross Entropy | binary_crossentropy | nn.BCELoss |\n",
        "| 多クラス分類 | ソフトマックス交差エントロピー | Softmax Cross Entropy | categorical_crossentropy (one-hot vector用)<br> sparse_categorical_crossentropy | nn.CrossEntropyLoss |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nxq_oYV7cTZ1"
      },
      "outputs": [],
      "source": [
        "# 損失関数には、ソフトマックス交差エントロピー誤差を使用\n",
        "# 今回は正解ラベルは one-hotベクトル として準備していることから、\n",
        "# keras.losses.CategoricalCrossentropy() を使用する\n",
        "# one-hotベクトルでない場合は、\n",
        "# keras.losses.SparseCategoricalCrossentropy() を使用する\n",
        "loss_fn = keras.losses.CategoricalCrossentropy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SunEXnNDcTZ2",
        "outputId": "e470abaa-be26-4b5b-88fd-1162b042e182"
      },
      "outputs": [],
      "source": [
        "# 今の場合、予測値はいずれも 0.1 程度\n",
        "# 交差エントロピー誤差は、-log(予測値) で表現される（前セクションを参照）\n",
        "# 損失は、-log(0.1) ≒ 2.3 程度になるはず\n",
        "loss_fn(train_labels, predictions).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "im6ATAZIcTZ2",
        "outputId": "a598747e-d18a-4589-98ad-0d2859092fc9"
      },
      "outputs": [],
      "source": [
        "# 参考\n",
        "# -log(0.1) を計算\n",
        "-np.log(0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DzYUW_xUcTZ2"
      },
      "outputs": [],
      "source": [
        "# model.compile で損失関数、オプティマイザを指定する\n",
        "# 損失関数は先程定義した交差エントロピー誤差を使用する\n",
        "# 最適化関数(オプティマイザ)として、Adam, RMSpropなどがある。ここでは、RMSprop を選択する\n",
        "# モデルの評価は 後ほど、accuracy で行う\n",
        "\n",
        "model.compile(loss = loss_fn,\n",
        "              optimizer='rmsprop',\n",
        "              metrics = ['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ItiEo7N0cTZ2"
      },
      "source": [
        "### 2-6. 学習・評価\n",
        "- `model.fit()` で学習させる\n",
        "- この時、訓練データと訓練データの正解ラベルをモデルに与える\n",
        "- `validation_split` で訓練データのうち検証に使う割合を指定する\n",
        "- `batch_size` でバッチサイズを指定する\n",
        "- `epochs` で何回学習するかを指定する\n",
        "- 学習の結果を変数 history に代入してあとで可視化する"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AVx6U6CScTZ2",
        "outputId": "d8f16dc7-b957-4448-b378-2e4390c7061e"
      },
      "outputs": [],
      "source": [
        "# loss: 損失\n",
        "# accuracy: 正答率\n",
        "# 10回の繰り返しで、loss が少しずつ減少、accuracy は増加\n",
        "\n",
        "# 訓練データ, 訓練データの正解ラベル をまず入力 (train_images, train_labels)\n",
        "# 訓練データの2割を検証データとして使用 (validation_split=0.2)\n",
        "# ミニバッチ学習として、バッチサイズは128に設定 (batch_size=128)\n",
        "#   128枚ずつ学習していくということ\n",
        "# 繰り返し回数は10回 (epochs=10)\n",
        "\n",
        "# 走らせると、損失値 loss が少しずつ小さくなり、正確度 accuracy が改善していくことに着目\n",
        "history = model.fit(train_images,train_labels,\n",
        "                    validation_split=0.2,\n",
        "                    batch_size=128,\n",
        "                    epochs=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGm_RSAUcTZ3"
      },
      "source": [
        "- `model.evaluate(テストデータ,テストラベル)`を使うことで、modelの性能を表示できる"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3UKfzmTRcTZ3",
        "outputId": "549a03c2-ace0-4554-99ff-9eefaca96121"
      },
      "outputs": [],
      "source": [
        "# model.evaluateの引数に test_images, test_labels を指定\n",
        "# verbose =1 とすると、学習のときと同じような結果表示になる\n",
        "model.evaluate(test_images,test_labels, verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypRJJo5JcTZ3"
      },
      "source": [
        "## 3. 学習の視覚化\n",
        "- matplotlib を用いて学習の様子を視覚化する\n",
        "- 変数 history.history の中に loss と accuracy の10回の値が格納されている"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rfzM00sWcTZ3",
        "outputId": "15fef332-ed3c-4681-ae26-da7f97acf9e2"
      },
      "outputs": [],
      "source": [
        "# history.historyの中を見てみる\n",
        "# ディクショナリ型\n",
        "# キーが 'loss', 'accuracy', 'val_loss', 'val_accuracy'\n",
        "# val_ は 検証データでの結果\n",
        "# 値が 損失値と正答率の10回の学習での推移\n",
        "history.history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "nra-OFFtcTZ3",
        "outputId": "9a2c3509-eac3-46bc-d47f-ff469c4ff63e"
      },
      "outputs": [],
      "source": [
        "# 訓練データの損失値 loss と検証データの損失値 val_loss をグラフとして表示\n",
        "# 訓練データの loss の値を取り出して train_loss に代入\n",
        "# ディクショナリ型の値は 変数名['キー名'] で取り出せる\n",
        "train_loss = history.history['loss']\n",
        "# 同様に検証データの loss の値を取り出して val_loss に代入\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "# train_loss と val_loss をプロットする\n",
        "plt.plot(train_loss, label='training')\n",
        "plt.plot(val_loss, label='validation')\n",
        "# グラフのタイトル\n",
        "plt.title('loss over epochs')\n",
        "# x軸の名前\n",
        "plt.xlabel('epochs')\n",
        "# y軸の名前\n",
        "plt.ylabel('loss')\n",
        "# 凡例\n",
        "plt.legend()\n",
        "# これらをすべてまとめて表示\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "4uCX1MWKcTZ4",
        "outputId": "a541c1af-5cc1-4fb6-ad87-9bbbe76fc425"
      },
      "outputs": [],
      "source": [
        "# 訓練データの正答率 accuracy と検証データの正答率 val_accuracy をグラフとして表示\n",
        "# 訓練データの accuracy を取り出して train_accuracy に代入\n",
        "train_accuracy = history.history['accuracy']\n",
        "# 同様に検証データの accuracy を取り出して val_accuracy に代入\n",
        "val_accuracy = history.history['val_accuracy']\n",
        "\n",
        "# train_accuracy と val_accuracy をプロットする\n",
        "plt.plot(train_accuracy, label='training')\n",
        "plt.plot(val_accuracy, label='validation')\n",
        "# グラフのタイトル\n",
        "plt.title('accuracy over epochs')\n",
        "# x軸の名前\n",
        "plt.xlabel('epochs')\n",
        "# y軸の名前\n",
        "plt.ylabel('accuracy')\n",
        "# 凡例\n",
        "plt.legend()\n",
        "# これらをすべてまとめて表示\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fExC0TU-cTZ4"
      },
      "source": [
        "## 4. ハイパーパラメータ\n",
        "- 深層学習の実装の例を示したが、自身のデータで解析する際、人間が設定しなければならないパラメータがいくつかある\n",
        "- これらを**ハイパーパラメータ**という\n",
        "- 具体的には以下のようなものが挙げられる\n",
        "    - 中間層のユニット数\n",
        "    - Dropout率\n",
        "    - 中間層の活性化関数\n",
        "    - 損失関数\n",
        "    - 最適化関数\n",
        "    - バッチサイズ\n",
        "    - エポック数\n",
        "- より精度の高いモデルを構築するために、これらを吟味していくことが必要となる"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBAqxaLXcTZ5"
      },
      "source": [
        "### 演習問題\n",
        "\n",
        "- 以下のパラメータでモデルを構築し、学習させた時、テストデータの正答率がどう変わるかを見てみてください\n",
        "\n",
        "- 中間層のユニット数: 16\n",
        "- 中間層の活性化関数: 'relu' ではなく 'sigmoid'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sgg-X5RwcTZ5"
      },
      "outputs": [],
      "source": [
        "# 必要なパッケージのインポート\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "# データの準備\n",
        "from tensorflow.keras.datasets import mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "# データの正規化\n",
        "train_images = train_images / 255.0\n",
        "test_images = test_images / 255.0\n",
        "\n",
        "# 変数 model を初期化\n",
        "model = []\n",
        "\n",
        "# モデルの構築\n",
        "model = Sequential()\n",
        "model.add(layers.Flatten(input_shape=(28, 28)))\n",
        "model.add(layers.Dense(ここに代入, activation=ここに代入)) #中間層のユニット数(16)と活性化関数('sigmoid')\n",
        "model.add(layers.Dropout(0.2))\n",
        "model.add(layers.Dense(10))\n",
        "model.add(layers.Softmax())\n",
        "\n",
        "# モデルの要約\n",
        "model.summary()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-999bDGlcTZ5"
      },
      "outputs": [],
      "source": [
        "# モデルの最適化\n",
        "\n",
        "# 損失関数には、交差エントロピー誤差を使用\n",
        "# 正解ラベルを one-hot ベクトルに変換していないため、\n",
        "# SparseCategoricalCrossentropy()を使う\n",
        "loss_fn = keras.losses.SparseCategoricalCrossentropy()\n",
        "\n",
        "\n",
        "# 最適化関数には、RMSprop を使用\n",
        "model.compile(loss = loss_fn,\n",
        "              optimizer='rmsprop',\n",
        "              metrics = ['accuracy'])\n",
        "\n",
        "\n",
        "# モデルの学習\n",
        "history = model.fit(train_images,train_labels,\n",
        "                    validation_split=0.2,\n",
        "                    batch_size=128,\n",
        "                    epochs=10)\n",
        "\n",
        "# loss の推移\n",
        "train_loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "plt.plot(train_loss, label='training')\n",
        "plt.plot(val_loss, label='validation')\n",
        "plt.title('loss over epochs')\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# accuracy の推移\n",
        "train_accuracy = history.history['accuracy']\n",
        "val_accuracy = history.history['val_accuracy']\n",
        "plt.plot(train_accuracy, label='training')\n",
        "plt.plot(val_accuracy, label='validation')\n",
        "plt.title('accuracy over epochs')\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# テストデータでの評価\n",
        "model.evaluate(test_images,test_labels, verbose=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MH9vHgFnkZH"
      },
      "source": [
        "## 5. 畳み込みニューラルネットワーク Convolutional Neural Network (CNN)\n",
        "\n",
        "- 全結合層のニューラルネットワークよりも精度が向上\n",
        "- 畳み込みとプーリングを複数回施行していく\n",
        "    - イメージ: 1回目の畳み込みで「斜め」「横」「縦」などの形状を学習、プーリングで特徴量マップをダウンサンプリング、2回めの畳み込みで、「斜めと横の組み合わせ」など、もう少し大きな形状を学習、プーリングで特徴量マップをさらにダウンサンプリング、3回目の畳み込みで、全体の空間的な位置関係を学習\n",
        "- CNNの入力は(画像の縦のピクセル数, 画像の横のピクセル数, 画像のチャンネル数) で指定する\n",
        "    - MNISTの手書き数字の画像は、縦28, 横28, 白黒(チャンネルの次元数は1) なので、(28,28,1)となる\n",
        "- 畳み込みに使うフィルタの種類を filters で設定する。filters=32は、32種類のフィルタを使用するということである。kernel_size=3 は、フィルタの大きさが3x3の行列ということを示す\n",
        "    - 32種類のフィルタを使うということは、特徴量マップは32種類できるということである。\n",
        "- 最大プーリングに使う pool_size は特徴量マップを小さく分割する量を指定する。pool_size=2 は、特徴量マップを2x2ごとに分割し、各領域の最大値から新たな出力特徴量マップを生成する\n",
        "- 以下を実行し、出力されるモデルの要約と上の説明を比較すると理解が進む"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HSl3Q1sqnuPW",
        "outputId": "72d509dd-83f8-44c7-8dc0-49bf51ac7e51"
      },
      "outputs": [],
      "source": [
        "# 必要なモジュールは全結合層の時と同じ\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "# モデルを初期化する\n",
        "model = []\n",
        "\n",
        "# Sequential関数を使って、モデルに層を設定していく\n",
        "model = Sequential()\n",
        "\n",
        "# 32種類の 3x3 のフィルタを使って畳み込みを行う\n",
        "model.add(layers.Conv2D(filters=32, kernel_size=3, activation='relu',\n",
        "                        input_shape=(28, 28, 1)))\n",
        "# 2x2 で分割し最大値プーリング演算を行う\n",
        "# これにより、出力される特徴量マップは 13x13x32 になる\n",
        "model.add(layers.MaxPooling2D(pool_size=2))\n",
        "\n",
        "# 64種類の 3x3 のフィルタを使って畳み込みを行う\n",
        "model.add(layers.Conv2D(filters=64, kernel_size=3, activation='relu'))\n",
        "# 2x2 で分割し最大値プーリング演算を行う\n",
        "# これにより、出力される特徴量マップは 5x5x64 になる\n",
        "model.add(layers.MaxPooling2D(pool_size=2))\n",
        "\n",
        "# 128種類の 3x3 のフィルタを使って畳み込みを行う\n",
        "# 特徴量マップは 3x3x128 となり、縦と横は十分小さいので、プーリングは必要ない\n",
        "model.add(layers.Conv2D(filters=128, kernel_size=3, activation='relu'))\n",
        "\n",
        "# 全結合層に投入するために、3次元のデータを1次元に変換する\n",
        "model.add(layers.Flatten())\n",
        "\n",
        "# ソフトマックス関数を使って、10クラス分類を行う\n",
        "model.add(layers.Dense(10, activation='softmax'))\n",
        "\n",
        "model.summary()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Zj2btt224r3"
      },
      "source": [
        "- CNNの計算は時間がかかる\n",
        "- Google Colabでは、GPUを使用することができる\n",
        "    - メニューの「ランタイム」→「ランタイムのタイプを変更」\n",
        "    - ハードウェアアクセラレータを\"GPU\"に変更\n",
        "\n",
        "<img src=\"https://www.nemotos.net/nb/img/colab_gpu.png\" width=\"300\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "hmIuyV7HpMBd",
        "outputId": "e5bc677d-6436-4f3d-d13a-bbce61a8c820"
      },
      "outputs": [],
      "source": [
        "# データの準備\n",
        "# mnistデータセットを入手\n",
        "from tensorflow.keras.datasets import mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "# データの正規化\n",
        "# 訓練データを、3次元のテンソルが60000枚スタックしているという形状に変更する\n",
        "train_images = train_images.reshape((60000, 28, 28, 1))\n",
        "# 画素値を 0-255 から、 0-1 に正規化する\n",
        "train_images = train_images / 255.0\n",
        "\n",
        "# テストデータも同様\n",
        "test_images = test_images.reshape((10000, 28, 28, 1))\n",
        "test_images = test_images / 255.0\n",
        "\n",
        "# モデルの最適化\n",
        "# オプティマイザはRMSpropを使用する\n",
        "# 損失関数は sparse_categorical_crossentropy を使用する\n",
        "# モデルの評価には accuracy を使用する\n",
        "model.compile(loss='sparse_categorical_crossentropy',\n",
        "              optimizer='rmsprop',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# モデルの学習\n",
        "# 学習の際に訓練データの2割を検証データに使用する\n",
        "# ミニバッチ学習を行う。バッチサイズは128\n",
        "# エポック数は10とし、10回学習させる\n",
        "history = model.fit(train_images,train_labels,\n",
        "                    validation_split=0.2,\n",
        "                    batch_size=128,\n",
        "                    epochs=10)\n",
        "\n",
        "# loss の推移\n",
        "train_loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "plt.plot(train_loss, label='training')\n",
        "plt.plot(val_loss, label='validation')\n",
        "plt.title('loss over epochs')\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# accuracy の推移\n",
        "train_accuracy = history.history['accuracy']\n",
        "val_accuracy = history.history['val_accuracy']\n",
        "plt.plot(train_accuracy, label='training')\n",
        "plt.plot(val_accuracy, label='validation')\n",
        "plt.title('accuracy over epochs')\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Bl3mX8Npt0E",
        "outputId": "6cc81429-eaeb-4e27-ebdd-780268cf0c0b"
      },
      "outputs": [],
      "source": [
        "# テストデータでの評価\n",
        "# 全結合層のときよりも、結果が改善していることに着目\n",
        "model.evaluate(test_images,test_labels, verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLKthKZ43eeQ"
      },
      "source": [
        "## 復習\n",
        "\n",
        "- 授業では手書き文字の識別を行いました\n",
        "- fashion-mnist といって、洋服や小物などを集めた画像のデータベースがあります\n",
        "\n",
        "| <img src=\"https://www.nemotos.net/nb/img/fashion-mnist-sprite.png\" width=400> |\n",
        "| --: |\n",
        "| [https://github.com/zalandoresearch/fashion-mnist](https://github.com/zalandoresearch/fashion-mnist)より引用 |\n",
        "\n",
        "- MNISTの手書き文字と同じサイズで、訓練データとテストデータの数も同じです\n",
        "- 種類も10クラスです\n",
        "    0. T-シャツ/トップ (T-shirt/top)\n",
        "    1. ズボン (Trouser)\n",
        "    2. プルオーバー (Pullover)\n",
        "    3. ドレス (Dress)\n",
        "    4. コート (Coat)\n",
        "    5. サンダル (Sandal)\n",
        "    6. シャツ (Shirt)\n",
        "    7. スニーカー (Sneaker)\n",
        "    8. バッグ (Bag)\n",
        "    9. アンクルブーツ (Ankle boot)\n",
        "- 全結合層とCNNでどれだけ結果が違うか検証してみましょう\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ews38G8Q837d",
        "outputId": "a3383cea-f6e1-416c-9932-d1fb840a6e7e"
      },
      "outputs": [],
      "source": [
        "# 全結合層\n",
        "\n",
        "##### 必要なモジュールのインポート\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "\n",
        "##### データの準備\n",
        "# fashion-mnistデータセットを入手\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "\n",
        "# データの正規化\n",
        "# 訓練データを、3次元のテンソルが60000枚スタックしているという形状に変更する\n",
        "train_images = train_images.reshape((60000, 28, 28, 1))\n",
        "# 画素値を 0-255 から、 0-1 に正規化する\n",
        "train_images = train_images / 255.0\n",
        "\n",
        "# テストデータも同様\n",
        "test_images = test_images.reshape((10000, 28, 28, 1))\n",
        "test_images = test_images / 255.0\n",
        "\n",
        "##### 全結合層によるモデルの構築 (forward propagationの設定)\n",
        "# (全結合層とCNNの違いはここだけ！)\n",
        "# モデルを初期化する\n",
        "model = []\n",
        "\n",
        "# Sequential関数を使って、モデルに層を設定していく\n",
        "model = Sequential()\n",
        "\n",
        "# 28x28の画像を1次元に変換する\n",
        "model.add(layers.Flatten(input_shape=(28, 28)))\n",
        "\n",
        "# 隠れ層のユニット数は128ユニット、活性化関数はReLU\n",
        "model.add(layers.Dense(128, activation='relu'))\n",
        "\n",
        "# Dropout率を0.2で設定\n",
        "model.add(layers.Dropout(0.2))\n",
        "\n",
        "# 出力層は10クラス分類なので10、ソフトマックス関数で分類\n",
        "model.add(layers.Dense(10))\n",
        "model.add(layers.Softmax())\n",
        "\n",
        "# モデルのサマリを表示\n",
        "print('Model Summary')\n",
        "print(model.summary())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ucz3Cic7vlsT",
        "outputId": "309b78c8-8e44-4ed0-a5de-5637e4b83d12"
      },
      "outputs": [],
      "source": [
        "#### モデルの最適化 (backward propagationの設定)\n",
        "# オプティマイザはRMSpropを使用する\n",
        "# 損失関数は sparse_categorical_crossentropy を使用する\n",
        "# モデルの評価には accuracy を使用する\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "#### モデルの学習\n",
        "# history の初期化\n",
        "history = []\n",
        "\n",
        "# 学習の際に訓練データの2割を検証データに使用する\n",
        "# ミニバッチ学習を行う。バッチサイズは128\n",
        "# エポック数は10とし、10回学習させる\n",
        "history = model.fit(train_images,train_labels,\n",
        "                    validation_split=0.2,\n",
        "                    batch_size=128,\n",
        "                    epochs=10)\n",
        "\n",
        "\n",
        "##### 損失値のグラフ表示\n",
        "# 訓練データの損失値 loss と検証データの損失値 val_loss をグラフとして表示\n",
        "# 訓練データの loss の値を取り出して train_loss に代入\n",
        "# ディクショナリ型の値は 変数名['キー名']　で取り出せる\n",
        "train_loss = history.history['loss']\n",
        "# 同様に検証データの loss の値を取り出して val_loss に代入\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "# train_loss と val_loss をプロットする\n",
        "plt.plot(train_loss, label='training')\n",
        "plt.plot(val_loss, label='validation')\n",
        "# グラフのタイトル\n",
        "plt.title('loss over epochs')\n",
        "# x軸の名前\n",
        "plt.xlabel('epochs')\n",
        "# y軸の名前\n",
        "plt.ylabel('loss')\n",
        "# 凡例\n",
        "plt.legend()\n",
        "# これらをすべてまとめて表示\n",
        "plt.show()\n",
        "\n",
        "\n",
        "##### 正答率 のグラフ表示\n",
        "# 訓練データの accuracy を取り出して train_accuracy に代入\n",
        "train_accuracy = history.history['accuracy']\n",
        "# 同様に検証データの accuracy を取り出して val_accuracy に代入\n",
        "val_accuracy = history.history['val_accuracy']\n",
        "\n",
        "# train_accuracy と val_accuracy をプロットする\n",
        "plt.plot(train_accuracy, label='training')\n",
        "plt.plot(val_accuracy, label='validation')\n",
        "# グラフのタイトル\n",
        "plt.title('accuracy over epochs')\n",
        "# x軸の名前\n",
        "plt.xlabel('epochs')\n",
        "# y軸の名前\n",
        "plt.ylabel('accuracy')\n",
        "# 凡例\n",
        "plt.legend()\n",
        "# これらをすべてまとめて表示\n",
        "plt.show()\n",
        "\n",
        "#### テストデータでの評価\n",
        "print('Evaluate with test data')\n",
        "model.evaluate(test_images,test_labels, verbose=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kwrY_w596Axu",
        "outputId": "07c556c0-79cd-4f7d-e564-8928fde12d32"
      },
      "outputs": [],
      "source": [
        "# CNN\n",
        "\n",
        "##### 必要なモジュールのインポート\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "\n",
        "##### データの準備\n",
        "# fashion-mnistデータセットを入手\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "\n",
        "# データの正規化\n",
        "# 訓練データを、3次元のテンソルが60000枚スタックしているという形状に変更する\n",
        "train_images = train_images.reshape((60000, 28, 28, 1))\n",
        "# 画素値を 0-255 から、 0-1 に正規化する\n",
        "train_images = train_images / 255.0\n",
        "\n",
        "# テストデータも同様\n",
        "test_images = test_images.reshape((10000, 28, 28, 1))\n",
        "test_images = test_images / 255.0\n",
        "\n",
        "\n",
        "##### CNNによるモデルの構築 (forward propagationの設定)\n",
        "# 全結合層とCNNの違いはここだけ！\n",
        "# モデルを初期化する\n",
        "model = []\n",
        "\n",
        "# Sequential関数を使って、モデルに層を設定していく\n",
        "model = Sequential()\n",
        "\n",
        "# 32種類の 3x3 のフィルタを使って畳み込みを行う\n",
        "model.add(layers.Conv2D(filters=32, kernel_size=3, activation='relu',\n",
        "                        input_shape=(28, 28, 1)))\n",
        "# 2x2 で分割し最大値プーリング演算を行う\n",
        "# これにより、出力される特徴量マップは 13x13x32 になる\n",
        "model.add(layers.MaxPooling2D(pool_size=2))\n",
        "\n",
        "# 64種類の 3x3 のフィルタを使って畳み込みを行う\n",
        "model.add(layers.Conv2D(filters=64, kernel_size=3, activation='relu'))\n",
        "# 2x2 で分割し最大値プーリング演算を行う\n",
        "# これにより、出力される特徴量マップは 5x5x64 になる\n",
        "model.add(layers.MaxPooling2D(pool_size=2))\n",
        "\n",
        "# 128種類の 3x3 のフィルタを使って畳み込みを行う\n",
        "# 特徴量マップは 3x3x128 となり、縦と横は十分小さいので、プーリングは必要ない\n",
        "model.add(layers.Conv2D(filters=128, kernel_size=3, activation='relu'))\n",
        "\n",
        "# 全結合層に投入するために、3次元のデータを1次元に変換する\n",
        "model.add(layers.Flatten())\n",
        "\n",
        "# ユニット数128の隠れ層を使用する。活性化関数は ReLU\n",
        "model.add(layers.Dense(128,activation='relu'))\n",
        "\n",
        "# Dropout率を0.2で設定\n",
        "model.add(layers.Dropout(0.2))\n",
        "\n",
        "# ソフトマックス関数を使って、10クラス分類を行う\n",
        "model.add(layers.Dense(10, activation='softmax'))\n",
        "\n",
        "# モデルのサマリを表示\n",
        "print('Model Summary')\n",
        "print(model.summary())\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ucKJ_1HwwwCE",
        "outputId": "172bb4e6-0311-4fd0-8e42-db24e9cc1c8a"
      },
      "outputs": [],
      "source": [
        "#### モデルの最適化 (backward propagationの設定)\n",
        "# オプティマイザはRMSpropを使用する\n",
        "# 損失関数は sparse_categorical_crossentropy を使用する\n",
        "# モデルの評価には accuracy を使用する\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "#### モデルの学習\n",
        "# history.history の初期化\n",
        "# hisotry.historyは dict型\n",
        "history=[]\n",
        "\n",
        "# 学習の際に訓練データの2割を検証データに使用する\n",
        "# ミニバッチ学習を行う。バッチサイズは128\n",
        "# エポック数は10とし、10回学習させる\n",
        "history = model.fit(train_images,train_labels,\n",
        "                    validation_split=0.2,\n",
        "                    batch_size=128,\n",
        "                    epochs=10)\n",
        "\n",
        "\n",
        "##### 損失値のグラフ表示\n",
        "# 訓練データの損失値 loss と検証データの損失値 val_loss をグラフとして表示\n",
        "# 訓練データの loss の値を取り出して train_loss に代入\n",
        "# ディクショナリ型の値は 変数名['キー名']　で取り出せる\n",
        "train_loss = history.history['loss']\n",
        "# 同様に検証データの loss の値を取り出して val_loss に代入\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "# train_loss と val_loss をプロットする\n",
        "plt.plot(train_loss, label='training')\n",
        "plt.plot(val_loss, label='validation')\n",
        "# グラフのタイトル\n",
        "plt.title('loss over epochs')\n",
        "# x軸の名前\n",
        "plt.xlabel('epochs')\n",
        "# y軸の名前\n",
        "plt.ylabel('loss')\n",
        "# 凡例\n",
        "plt.legend()\n",
        "# これらをすべてまとめて表示\n",
        "plt.show()\n",
        "\n",
        "\n",
        "##### 正答率 のグラフ表示\n",
        "# 訓練データの accuracy を取り出して train_accuracy に代入\n",
        "train_accuracy = history.history['accuracy']\n",
        "# 同様に検証データの accuracy を取り出して val_accuracy に代入\n",
        "val_accuracy = history.history['val_accuracy']\n",
        "\n",
        "# train_accuracy と val_accuracy をプロットする\n",
        "plt.plot(train_accuracy, label='training')\n",
        "plt.plot(val_accuracy, label='validation')\n",
        "# グラフのタイトル\n",
        "plt.title('accuracy over epochs')\n",
        "# x軸の名前\n",
        "plt.xlabel('epochs')\n",
        "# y軸の名前\n",
        "plt.ylabel('accuracy')\n",
        "# 凡例\n",
        "plt.legend()\n",
        "# これらをすべてまとめて表示\n",
        "plt.show()\n",
        "\n",
        "#### テストデータでの評価\n",
        "print('Evaluate with test data')\n",
        "model.evaluate(test_images,test_labels, verbose=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ztYBfZgIifBx"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "include_colab_link": true,
      "name": "python_5_mnist.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
