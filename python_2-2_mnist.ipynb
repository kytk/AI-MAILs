{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI-MAILs\n",
    "## 深層学習入門: 手書き数字の分類\n",
    "\n",
    "Ver. 20220715"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 本セクションの目標\n",
    "- MNISTデータセットを用いて手書き数字の分類を行うことで、どのように深層学習を実装するかを学ぶ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 目次\n",
    "- A. MNISTデータセット\n",
    "- B. Tensorflow/Keras を用いた深層学習の実装\n",
    "- C. 学習の可視化 \n",
    "- D. ハイパーパラメータ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. MNISTデータセット\n",
    "- NIST (National Institute of Standards and Technology) が保有していたデータセットを再構成したデータベース\n",
    "- 60,000枚の訓練用画像と10,000枚の評価用画像が含まれている\n",
    "\n",
    "| <img src=\"https://www.nemotos.net/nb/img/MnistExamples.png\" width=\"300\"> |\n",
    "| --: |\n",
    "| Wikipediaより引用 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. 深層学習の実装\n",
    "- 深層学習を実装する手順は以下となる\n",
    "\n",
    "| <img src=\"https://www.nemotos.net/nb/img/dl_flow.png\" width=\"300\"> |\n",
    "| --: |\n",
    "| 動かしながら学ぶPyTorchプログラミング入門より引用 |\n",
    "\n",
    "- この流れに従っていく"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 必要なパッケージのインポート\n",
    "- 今回必要なパッケージは以下\n",
    "    - numpy\n",
    "    - matplotlib\n",
    "    - tensorflow\n",
    "        - keras は tensorflow 2.0 から tensorflow の中に取り込まれた"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 必要なパッケージのインポート\n",
    "\n",
    "# NumPy\n",
    "import numpy as np\n",
    "\n",
    "# Matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Tensorflow\n",
    "import tensorflow as tf\n",
    "\n",
    "# ラベルを one-hotベクトルに変換する関数 to_categorical()\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. データの前処理\n",
    "- MNISTの画像データはひとつひとつのピクセルの値が0-255の値をとる\n",
    "- これを0-1の値をとるように変換する\n",
    "- 脳画像の前処理に関しては、午前中の下地先生のセクションがここに該当\n",
    "\n",
    "#### 2.1. データの読み込みと確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorflow の中に mnist データセットが既に入っている\n",
    "mnist = tf.keras.datasets.mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mnist.load_data() で訓練データとテストデータにわけて格納する\n",
    "# mnistは、訓練データとテストデータがそれぞれタプルにわかれて入っている\n",
    "# 訓練データの画像を train_images, 正解ラベルを train_labels に格納する\n",
    "# テストデータも同じ\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_images について確認する\n",
    "# まず、型から確認\n",
    "# numpy.ndarray型\n",
    "type(train_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape\n",
    "# 60000枚の画像、1枚の画像は 28 x 28 で構成\n",
    "train_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow() を使って実際の画像を確認\n",
    "# for を使って、最初の3人分のデータを見る\n",
    "for i in range(3):\n",
    "    plt.figure(figsize=(1,1))\n",
    "    plt.imshow(train_images[i], cmap='gray')\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_labels の内容を確認\n",
    "# スライシングで train_labels の最初の3つのラベルを取り出す\n",
    "# 画像とラベルが一致していることを確認\n",
    "train_labels[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_labels の shape を確認\n",
    "# 60000 のデータがある1次元のデータ\n",
    "train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 同様にテストデータも確認\n",
    "# shape\n",
    "# 10,000枚の画像、1枚の画像は 28 x 28 で構成\n",
    "test_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plt.imshow() を使って実際の画像を確認\n",
    "# for を使って、最初の3人分のデータを見る\n",
    "for i in range(3):\n",
    "    plt.figure(figsize=(1,1))\n",
    "    plt.imshow(test_images[i], cmap='gray')\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_labels の内容を確認\n",
    "# スライシングで test_labels の最初の3つのラベルを取り出す\n",
    "test_labels[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 正解ラベルの one-hotベクトル化\n",
    "- 第3部(1)でみたように、正解ラベルを one-hotベクトルに変換する\n",
    "- `to_categorical()` 関数を使うことで変換できる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_labels を one-hotベクトルに変換\n",
    "train_labels = to_categorical(train_labels)\n",
    "\n",
    "# test_labels も同様に one-hotベクトルに変換\n",
    "test_labels = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 新しい train_labels の shape を確認\n",
    "# 60000行10列の行列になっている\n",
    "train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_labels の 最初の3つを見てみる\n",
    "# 正解 5, 0, 4 に相当するところが 1 になっていることに着目\n",
    "train_labels[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_labels の 最初の3つも確認する\n",
    "# 正解 7, 2, 1 に相当するところが 1 になっていることに着目\n",
    "test_labels[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 データの正規化\n",
    "- 深層学習に限らず、データ解析においてデータの範囲をある決まった範囲に変換することを正規化という\n",
    "- 正規化を行うことで、異なる変数がモデルに与える影響を均等にできる\n",
    "    - 例: 年齢 (20-80) と 身長 (140-200)\n",
    "- 今、画像は 0-255 の整数をとるので、これを 0-1 になるように変換する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dtype 属性で numpy配列内の数字のデータ型がわかる\n",
    "# uint8 は unsigned integer 8bit 符号なし 8bit 整数 (0-255)\n",
    "train_images.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_images の最小値\n",
    "# min() メソッドを使えばよい\n",
    "train_images.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_images の最大値\n",
    "# max() メソッドを使えばよい\n",
    "train_images.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0-255で構成されるので、255で割れば、値は 0-1 の間となる\n",
    "# 255.0 と小数点をつけて割ることで、Pythonは出力を必ず float型としてくれる\n",
    "# 計算結果を同じ変数名にいれることで、変数を増やすことなく、\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_images の値が本当に0-1になったか確認\n",
    "# min() と max() を使えばよい\n",
    "# 最小値\n",
    "train_images.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最大値\n",
    "train_images.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dtypeも確認する\n",
    "# 今回は float64 倍精度浮動小数点数\n",
    "train_images.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 訓練データとテストデータの作成\n",
    "- MNISTデータセットは手書き数字6万枚の訓練データセットと手書き数字1万枚のテストデータセットから構成されている\n",
    "- 「**訓練データ**」「**検証データ**」「**テストデータ**」の3つを準備する\n",
    "    - 訓練データ: ニューラルネットワークのパラメータを決めるためのデータ\n",
    "    - 検証データ: 訓練データで得られたパラメータがどの程度の精度があるかを検証するためのデータ\n",
    "    - テストデータ: ニューラルネットワークの汎用性を評価するためのデータ\n",
    "        - 訓練で使ったものと別のセットを使わないといけない\n",
    "- Tensorflow には、`model.fit()` メソッドに、`validation_split` という引数が準備されており、ここで訓練データの何割を検証データとして使用するかを設定できる\n",
    "    - 今回は訓練データの2割を検証データとして使用することとする (`validation_split=0.2`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. ニューラルネットワークの定義\n",
    "\n",
    "- ここで、自分がイメージするニューラルネットワークモデルを定義する\n",
    "- 下図の赤線の部分, すなわち **順伝播 forwad propagation** のモデルを構築\n",
    "\n",
    "<img src=\"https://www.nemotos.net/nb/img/dl_overview_4.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 今は以下のように定義する\n",
    "    - 層と層の結合は全結合とする\n",
    "    - 第1層は画像が 28 x 28 で構成されているので、そのピクセル数(784)がユニット数\n",
    "    - 第2層のユニット数は 128 とする\n",
    "    - 第2層の活性化関数は **ReLU**関数 とする\n",
    "    - 過学習を防ぐため、全結合層の2割は drop とする (Dropout=0.2, 明日説明)\n",
    "    - このモデルとしては、最終の出力は 0-9 の10のクラスを分類したい\n",
    "    - そのため、第3層は 出力層に渡す準備として、ユニット数は 10 とする\n",
    "    - 第3層の出力はそのまま出力層の入力とする (Tensorflow ではそのように構築することが勧められている)\n",
    "    - 出力層の活性化関数は多クラス分類に適した **Softmax**関数 とする"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 活性化関数の特徴\n",
    "\n",
    "| 関数名 | 特徴 | \n",
    "| :-- | :-- |\n",
    "| ReLU | 隠れ層に使うことで、非線形問題を解くことができるようになる <br> Sigmoid関数は0-1の値しかとらないので層が厚くなるほど誤差が小さくなっていき、<br>入力層まで誤差が伝搬する前に誤差が消失するという勾配消失問題が発生する |\n",
    "| Sigmoid | 0-1の間の確率で表現可能なため2クラス分類の出力層に用いる |\n",
    "| Softmax | 各クラスの確率の総和が1となるように正規化された関数であるため多クラス分類の出力層に用いる |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.nemotos.net/nb/img/nn_model.png\" width=\"400\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- これらはTensorflow/Kerasでは以下のように定義できる\n",
    "    - プリセットで準備されている **tf.keras.Sequential**モデル を選択する(Sequential: 連続する)\n",
    "    - 入力画像は **Flatten** を使うことでベクトルにできる\n",
    "    - 全結合層は **Dense** で規定できる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルを定義\n",
    "# tf.keras.modls.Sequential([第1層,第2層,第3層,出力層]) と順に記載していく\n",
    "# リストなので、項目と項目の区切りに , を忘れない(忘れるとエラーになる)\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    # 入力画像の dimension を指定\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)), \n",
    "    # 第2層のユニット数を 128 にし、 活性化関数は ReLU とする\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    # 過学習防止のため、結合層の20%を dropout\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    # 第3層のユニット数を 10 にする\n",
    "    tf.keras.layers.Dense(10),\n",
    "    # 出力層でSoftmax関数 で処理して結果を出力する\n",
    "    tf.keras.layers.Softmax()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練データを使って予測値を計算 (forward propagation)\n",
    "# model() の後に .numpy() をつけることで、NumPy配列に変換する\n",
    "predictions = model(train_images).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# predictions を表示\n",
    "# この時点では、モデルを決めただけで、重みはランダムに割り当てられている\n",
    "# そのため、各クラスの確率はおおよそ 1/10 あたりになるはず\n",
    "# ひとつの数字の画像が1行、列が 0 - 9 の数字である確率\n",
    "# 一切学習はしていないことに注意！train_labelsはまだ使われていない\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 損失関数と最適化関数の定義\n",
    "- 次に損失関数と最適化関数(オプティマイザ)を決定する\n",
    "- その後、model.compile() で損失関数と最適化関数をモデルに組み込む\n",
    "- 下図の赤線の部分、すなわち **逆伝播 back propagation** のモデルを構築\n",
    "\n",
    "<img src=\"https://www.nemotos.net/nb/img/dl_overview_5.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- よく用いられる損失関数\n",
    "\n",
    "| 目的 | 関数名 | Function Name | 損失関数名<br>(Tensorflow) | 損失関数名(PyTorch) |\n",
    "| :-- | :-- | :-- | :-- | :-- |\n",
    "| 回帰 | 平均二乗誤差 | Mean Squared Error | mean_squared_error | nn.MSELoss |\n",
    "| 2クラス分類 | バイナリ交差エントロピー | Binary Cross Entropy | binary_crossentropy | nn.BCELoss |\n",
    "| 多クラス分類 | ソフトマックス交差エントロピー | Softmax Cross Entropy | categorical_crossentropy (one-hot vector用)<br> sparse_categorical_crossentropy | nn.CrossEntropyLoss |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 損失関数には、ソフトマックス交差エントロピー誤差を使用\n",
    "# 今回は正解ラベルは one-hotベクトル として準備していることから、\n",
    "# tf.keras.losses.CategoricalCrossentropy() を使用する\n",
    "# one-hotベクトルでない場合は、\n",
    "# tf.keras.losses.SparseCategoricalCrossentropy() を使用する\n",
    "loss_fn = tf.keras.losses.CategoricalCrossentropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 今の場合、予測値はいずれも 0.1 程度\n",
    "# 損失は、-log(0.1) ≒ 2.3 程度になるはず\n",
    "loss_fn(train_labels, predictions).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 参考\n",
    "# -log(0.1) を計算\n",
    "-np.log(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最適化関数として、Adam を選択する\n",
    "# 損失関数は先程定義した交差エントロピー誤差を使用する\n",
    "# モデルの評価は accuracy で行う\n",
    "model.compile(optimizer='adam',\n",
    "             loss = loss_fn,\n",
    "             metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# どのようなモデルになったかを model.summary() で知ることができる\n",
    "model.summary()\n",
    "# パラメータ数\n",
    "# Flatten: 入力層なのでなし\n",
    "# Dense 100480\n",
    "# 入力層 784 * 第2層 128 + 第2層のそれぞれのユニットに対する定数項 128\n",
    "# Dense 1290\n",
    "# 第2層 128 * 第3層 10 + 第3層のそれぞれのユニットに対する定数項 10\n",
    "# 合計 101,770 ものパラメータをこれから学習させることになる\n",
    "# パラメータが多いため、パラメータ推定のために必要なデータ数が膨大となる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. 学習・評価\n",
    "- `model.fit()` で学習させる\n",
    "- この時、訓練データと訓練データの正解ラベルをモデルに与える\n",
    "- `validation_split` で訓練データのうち検証に使う割合を指定する\n",
    "- `batch_size` でバッチサイズを指定する\n",
    "- `epochs` で何回学習するかを指定する\n",
    "- 学習の結果を変数 history に代入してあとで可視化する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss: 損失\n",
    "# accuracy: 正答率\n",
    "# 10回の繰り返しで、loss が少しずつ減少、accuracy は増加\n",
    "#\n",
    "# 訓練データ,訓練データの正解ラベルをまず入力 (train_images, train_labels)\n",
    "# 訓練データの2割を検証データとして使用 (validation_split=0.2)\n",
    "# ミニバッチ学習として、バッチサイズは128に設定 (batch_size=128)\n",
    "# 繰り返し回数は10回 (epochs=10)                            \n",
    "history = model.fit(train_images,train_labels,\n",
    "                    validation_split=0.2,\n",
    "                    batch_size=128,\n",
    "                    epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `model.evaluate(テストデータ,テストラベル)`を使うことで、modelの性能を表示できる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.evaluateの引数に test_images, test_labels を指定\n",
    "# verbose =1 とすると、学習のときと同じような結果表示になる\n",
    "model.evaluate(test_images,test_labels, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. 学習の視覚化\n",
    "- matplotlib を用いて学習の様子を視覚化する\n",
    "- 変数 history.history の中に loss と accuracy の10回の値が格納されている"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history.historyの中を見てみる\n",
    "# ディクショナリ型\n",
    "# キーが 'loss', 'accuracy', 'val_loss', 'val_accuracy'\n",
    "# val_ は 検証データでの結果\n",
    "# 値が 損失値と正答率の推移\n",
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練データの損失値 loss と検証データの損失値 val_loss をグラフとして表示\n",
    "# 訓練データの loss の値を取り出して train_loss に代入\n",
    "# ディクショナリ型の値は 変数名['キー名']　で取り出せる\n",
    "train_loss = history.history['loss']\n",
    "# 検証データの loss を取り出して val_loss に代入\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "# train_loss と val_loss をプロットする\n",
    "plt.plot(train_loss, label='training')\n",
    "plt.plot(val_loss, label='validation')\n",
    "# グラフのタイトル\n",
    "plt.title('loss over epochs')\n",
    "# x軸の名前\n",
    "plt.xlabel('epochs')\n",
    "# y軸の名前\n",
    "plt.ylabel('loss')\n",
    "# 凡例\n",
    "plt.legend()\n",
    "# これらをすべてまとめて表示\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練データの正答率 accuracy と検証データの正答率 val_accuracy をグラフとして表示\n",
    "# 訓練データの accuracy を取り出して train_accuracy に代入\n",
    "train_accuracy = history.history['accuracy']\n",
    "# 検証データの accuracy を取り出して val_accuracy に代入\n",
    "val_accuracy = history.history['val_accuracy']\n",
    "\n",
    "# train_accuracy と val_accuracy をプロットする\n",
    "plt.plot(train_accuracy, label='training')\n",
    "plt.plot(val_accuracy, label='validation')\n",
    "# グラフのタイトル\n",
    "plt.title('accuracy over epochs')\n",
    "# x軸の名前\n",
    "plt.xlabel('epochs')\n",
    "# y軸の名前\n",
    "plt.ylabel('accuracy')\n",
    "# 凡例\n",
    "plt.legend()\n",
    "# これらをすべてまとめて表示\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D. ハイパーパラメータ\n",
    "- 深層学習の実装の例を示したが、自身のデータで解析する際、人間が設定しなければならないパラメータがいくつかある\n",
    "- これらをハイパーパラメータという\n",
    "- 具体的には以下のようなものが挙げられる\n",
    "    - 中間層のユニット数\n",
    "    - Dropout率\n",
    "    - 中間層の活性化関数\n",
    "    - 損失関数\n",
    "    - 最適化関数\n",
    "    - バッチサイズ\n",
    "    - エポック数\n",
    "- より精度の高いモデルを構築するために、これらを吟味していくことが必要となる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 練習問題\n",
    "\n",
    "- 以下のパラメータでモデルを構築し、学習させた時、テストデータの正答率がどう変わるかを見てみてください\n",
    "\n",
    "- 中間層のユニット数: 64\n",
    "- バッチサイズ: 16\n",
    "- エポック数: 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 必要なパッケージのインポート\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "# データの準備\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "# データの正規化\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0\n",
    "\n",
    "# 変数 model を初期化\n",
    "model = []\n",
    "\n",
    "# モデルの構築\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)), \n",
    "    tf.keras.layers.Dense(ここに代入, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(10),\n",
    "    tf.keras.layers.Softmax()\n",
    "])\n",
    "\n",
    "# 損失関数には、交差エントロピー誤差を使用\n",
    "# 正解ラベルを one-hot ベクトルに変換していないため、\n",
    "# SparseCategoricalCrossentropy()を使う\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "# 最適化関数には、adam を使用\n",
    "model.compile(optimizer='adam',\n",
    "             loss = loss_fn,\n",
    "             metrics = ['accuracy'])\n",
    "\n",
    "# モデルの要約\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルの学習\n",
    "history = model.fit(train_images,train_labels,\n",
    "                    validation_split=0.2,\n",
    "                    batch_size=ここに代入,\n",
    "                    epochs=ここに代入)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss の推移\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "plt.plot(train_loss, label='training')\n",
    "plt.plot(val_loss, label='validation')\n",
    "plt.title('loss over epochs')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy の推移\n",
    "train_accuracy = history.history['accuracy']\n",
    "val_accuracy = history.history['val_accuracy']\n",
    "plt.plot(train_accuracy, label='training')\n",
    "plt.plot(val_accuracy, label='validation')\n",
    "plt.title('accuracy over epochs')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# テストデータでの評価\n",
    "model.evaluate(test_images,test_labels, verbose=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
